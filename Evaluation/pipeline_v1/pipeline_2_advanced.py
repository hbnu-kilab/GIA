# pip install openai>=1.0.0 chromadb tiktoken langchain langchain-community
import os
import chromadb
import tiktoken
import pandas as pd 
from openai import OpenAI
from datetime import datetime
from multiprocessing import Process, Queue
from typing import List, Dict, Optional, Tuple
import time
import torch
from langchain_core.tools import Tool
from langchain_google_community import GoogleSearchAPIWrapper
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers import Html2TextTransformer
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_huggingface import HuggingFaceEmbeddings


# API Keys Configuration
# wlsruddms@gmail.com
os.environ["GOOGLE_CSE_ID"] = "API_key"  # Your Google CSE ID
os.environ["GOOGLE_API_KEY"] = "API_key"  # Your Google API Key
os.environ["OPENAI_API_KEY"] = "API_key"

# Constants
OPENAI_EMBED_MODEL = "text-embedding-3-large"
EMBED_DIMS = None

# System prompt for network engineering assistant
chatgpt_system_prompt = """You are an expert network engineering assistant with deep knowledge of 
network configurations, troubleshooting, and security best practices. You have access to various 
network device configurations, XML schemas, and technical documentation."""

# Initialize OpenAI client
openai_client = OpenAI()  # í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”

class OpenAIEmbedder:
    """OpenAI ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤"""
    def __init__(self, model, dims: int | None = EMBED_DIMS):
        self.client = OpenAI()
        self.model = model
        self.dims = dims

    def embed(self, texts: list[str] | str) -> list[list[float]]:
        if isinstance(texts, str):
            texts = [texts]
        resp = self.client.embeddings.create(
            model=self.model,
            input=texts,
            **({"dimensions": self.dims} if self.dims else {})
        )
        return [d.embedding for d in resp.data]
    
class HuggingFaceEmbedder:
    def __init__(self, model_name, device, batch_size):
        self.model_name = model_name
        self.device = device
        self.batch_size = batch_size
        
        self.embedder = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={"device": device},
            encode_kwargs={"batch_size": batch_size}
        )

        self.model_name = model_name
        self.device = device
        self.batch_size = batch_size

    def embed(self, texts: list[str] | str) -> list[list[float]]:
        if isinstance(texts, str):
            texts = [texts]
        return self.embedder.embed_documents(texts)

class ChromaDB:
    """ì‚¬ì „ ì„ë² ë”©ëœ XML íŒŒì¼ë“¤ì„ ìœ„í•œ ChromaDB ì¸í„°í˜ì´ìŠ¤"""
    def __init__(self, db_path: str, collection_name: str, embedder: object):
        self.db_path = db_path
        self.client = chromadb.PersistentClient(path=db_path)
        self.embedder = embedder or OpenAIEmbedder()
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ (ì‚¬ì „ ì„ë² ë”©ëœ XML ë°ì´í„°)
            self.collection = self.client.get_collection(name=collection_name)
            print(f"[INFO] Loaded existing collection: {collection_name}")
            print(f"[INFO] Total documents in collection: {self.collection.count()}")
        except:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            self.collection = self.client.create_collection(name=collection_name)
            print(f"[INFO] Created new collection: {collection_name}")

    def add_docs(self, ids: list[str], docs: list[str], metadatas: list[dict] | None = None):
        """ìƒˆ ë¬¸ì„œ ì¶”ê°€ (í•„ìš”ì‹œ)"""
        embeddings = self.embedder.embed(docs)
        self.collection.add(ids=ids, documents=docs, embeddings=embeddings, metadatas=metadatas)

    def query(self, text: str, n_results: int = 5) -> Dict:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        q_emb = self.embedder.embed(text)
        return self.collection.query(query_embeddings=q_emb, n_results=n_results)

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    encoding = tiktoken.get_encoding(encoding_name)
    return len(encoding.encode(string))

def chunk_texts(text: str, chunk_size: int = 1500) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ í† í° í¬ê¸°ì— ë§ê²Œ ë¶„í• """
    tokens = num_tokens_from_string(text)
    if tokens <= chunk_size:
        return [text]
    
    texts = []
    n = (tokens // chunk_size) + 1
    part_length = len(text) // n
    extra = len(text) % n
    
    parts = []
    start = 0
    for i in range(n):
        end = start + part_length + (1 if i < extra else 0)
        parts.append(text[start:end].replace('\n', " "))
        start = end
    
    return parts

def get_classification_result(question: str) -> str:
    """ì‚¬ìš©ì ì…ë ¥ì„ 2ê°€ì§€ ì‘ì—… ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜"""
    classification_prompt = '''
    You are an excellent network engineering assistant. Classify the question into ONE of these categories:

    1. **Simple Lookup Tasks** - Tasks that can be solved by referring to or retrieving information from network configuration XML files
    2. **Other Tasks** - All other cases that do not rely on network configuration XML files

    IMPORTANT:
    - Return ONLY the exact category name, nothing else.
    ex) Simple Lookup Tasks
    ex) Other Tasks
    '''
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nInstruction: {classification_prompt}"}
        ],
        temperature=0.05
    )
    
    task_type = response.choices[0].message.content.strip()
    print(f"[INFO] Task classified as: {task_type}")
    return task_type

def get_draft(question: str, task_type: str) -> str:
    """ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ë§ì¶¤í˜• ì´ˆì•ˆ ìƒì„±"""
    task_prompts = {
        "Simple Lookup Tasks": '''
        IMPORTANT:
        - Emphasize writing concise, factual, and brief content when drafting.
        ''',
        
        "Other Tasks" : '''
        IMPORTANT:
        - Generate a draft answer that provides step-by-step reasoning and recommended actions
        - Focus on configuration changes, troubleshooting, optimization, or security/audit responses
        - Include specific commands or configuration snippets with proper syntax when relevant
        - Use numbered or bulleted lists to structure solutions clearly
        - Provide a brief explanation for each step or command
        - Maintain a professional and technical tone suitable for network engineering
        - Respond directly without unnecessary disclaimers unless explicitly requested
        '''
    }
    
    # ë¶„ë¥˜ ê²°ê³¼ ì—†ìœ¼ë©´ Simple Lookup Tasksë¡œ
    prompt = task_prompts.get(task_type, task_prompts["Simple Lookup Tasks"])
    
    # ì´ˆì•ˆì€ Temperature ë†’ê²Œ.
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\n{prompt}"}
        ],
        temperature=0.3
    )
    
    return response.choices[0].message.content

def get_xml_query(question: str, answer: str) -> str:
    """ChromaDBì˜ XML íŒŒì¼ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    query_prompt = '''
        Create a search query to find relevant XML configuration files and network documentation in ChromaDB.
            Focus on:
            - Network device types (router, switch, firewall)
            - Configuration elements (VLAN, BGP, OSPF, ACL, etc.)
            - Vendor-specific terms (Cisco IOS, Juniper, etc.)
            - Technical keywords from the question

        Output ONLY the query, no explanations.
        '''
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nContent: {answer}\n\nInstruction: {query_prompt}"}
        ],
        temperature=0.05
    )
    
    return response.choices[0].message.content.strip()

def get_internet_query(question: str, answer: str) -> str:
    """ì¸í„°ë„· ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    query_prompt = '''
        Create a Google search query to verify and enhance the technical accuracy of the network engineering answer.
            Focus on:
            - Recent best practices and standards
            - Vendor documentation and official guides
            - Common issues and solutions
            - Latest security advisories if relevant

        Make the query specific and technical.
        Output ONLY the query, no explanations.
        '''
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nContent: {answer}\n\nInstruction: {query_prompt}"}
        ],
        temperature=0.1
    )
    
    return response.choices[0].message.content.strip()


def get_google_search(query: str = "", k: int = 3) -> Optional[List[Dict]]:
    """ìˆ˜ì •ëœ Google Search API í•¨ìˆ˜"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print(f"[DEBUG] Google search query: '{query}' (k={k})")
        
        # API í‚¤ í™•ì¸
        api_key = os.environ.get("GOOGLE_API_KEY")
        cse_id = os.environ.get("GOOGLE_CSE_ID")
        
        if not api_key:
            print("[ERROR] GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
            
        if not cse_id:
            print("[ERROR] GOOGLE_CSE_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # GoogleSearchAPIWrapper ìƒì„±
        search = GoogleSearchAPIWrapper(
            k=k,
            google_api_key=api_key,
            google_cse_id=cse_id
        )
        
        # ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
        def search_results(query):
            try:
                results = search.results(query, k)
                print(f"[DEBUG] Search results count: {len(results) if results else 0}")
                return results
            except Exception as e:
                print(f"[ERROR] search.results() í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                return None
        
        # Tool ìƒì„±
        tool = Tool(
            name="Google Search Snippets",
            description="Search Google for recent results.",
            func=search_results,
        )
        
        # ê²€ìƒ‰ ì‹¤í–‰
        ref_text = tool.run(query)
        
        if len(ref_text) > 0:
            # ê²°ê³¼ ê²€ì¦ - ì²« ë²ˆì§¸ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            first_item = ref_text[0]
            if isinstance(first_item, dict):
                # 'Result' í‚¤ê°€ ì—†ìœ¼ë©´ ìœ íš¨í•œ ê²°ê³¼ë¡œ íŒë‹¨ (ì›ë˜ ì¡°ê±´ ìˆ˜ì •)
                if 'Result' not in first_item:
                    print(f"[SUCCESS] Valid search results found: {len(ref_text)} items")
                    return ref_text
                else:
                    print("[WARNING] Results contain 'Result' key - treating as invalid")
                    return None
            else:
                # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆì–´ë„ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë°˜í™˜
                print(f"[INFO] Non-dict results found: {len(ref_text)} items")
                return ref_text
        else:
            print("[INFO] No search results returned")
            return None
            
    except ImportError as e:
        print(f"[ERROR] í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install langchain-google-community")
        return None
    except Exception as e:
        print(f"[ERROR] Google search ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def get_page_content(link: str) -> Optional[str]:
    """ì›¹ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ"""
    try:
        loader = AsyncHtmlLoader([link])
        docs = loader.load()
        html2text = Html2TextTransformer()
        docs_transformed = html2text.transform_documents(docs)
        if len(docs_transformed) > 0:
            return docs_transformed[0].page_content
        else:
            return None
    except Exception as e:
        print(f"[ERROR] Failed to fetch page content: {e}")
        return None
    
def get_internet_content(query: str) -> Optional[List[str]]:
    """Google ê²€ìƒ‰ì„ í†µí•´ ì¸í„°ë„· ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
    search_results = get_google_search(query, k=3)
    
    if not search_results:
        print("[INFO] No Google search results found")
        return None
    
    all_content = []
    for result in search_results[:2]:  # ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ ì²˜ë¦¬
        link = result.get('link')
        if link:
            print(f"[INFO] Fetching content from: {link}")
            page_content = get_page_content(link)
            if page_content:
                # ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = chunk_texts(page_content, 1500)
                all_content.extend(chunks[:2])  # ê° í˜ì´ì§€ì—ì„œ ìµœëŒ€ 2ê°œ ì²­í¬
    
    return all_content if all_content else None

def get_revise_answer(question: str, answer: str, content: str, task_type: str) -> str:
    """ì°¸ì¡° ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìˆ˜ì •"""
    task_prompts = {
            "Simple Lookup Tasks": '''
            You are a network lookup expert.
            Your goal is to provide lookup results.
            IMPORTANT :
            Just output the revised answer directly. DO NOT add additional explanations or announcements.
            ex) sample5, sample6, sample7
            ex) 174.123.123.123
            ''',
            
            "Other Tasks" : '''
            I want to revise the answer according to retrieved related content.
            Task Type: {task_type}

            Guidelines:
            - Verify technical accuracy against the reference
            - Add missing critical details from the reference
            - Correct any errors or outdated information
            - If the answer is already accurate and complete, keep it as is

            IMPORTANT:
            Just output the revised answer directly. DO NOT add additional explanations or announcements.
        '''
        }
        
    # # ë¶„ë¥˜ ê²°ê³¼ ì—†ìœ¼ë©´ Simple Lookup Tasksë¡œ
    prompt = task_prompts.get(task_type, task_prompts["Simple Lookup Tasks"])
    

    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", 
             "content": f"Reference Content: {content}\n\nQuestion: {question}\n\nOriginal Answer: {answer}\n\nInstruction: {prompt}"}
        ],
        temperature=0.1
    )
    
    return response.choices[0].message.content

def determine_reference_source(task_type: str, iteration: int) -> str:
    """ì‘ì—… ìœ í˜•ê³¼ ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¼ ì°¸ì¡° ì†ŒìŠ¤ ê²°ì •"""
    if task_type == "Simple Lookup Tasks":
        # Simple Lookupì€ ì£¼ë¡œ ë‚´ë¶€ XML DB ì‚¬ìš©
        return "chromadb"
    else:
        return "chromadb" if iteration % 2 == 0 else "internet"

def run_with_timeout(func, timeout, *args, **kwargs):
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” í•¨ìˆ˜ ì‹¤í–‰"""
    q = Queue()
    
    def wrapper(q, *args, **kwargs):
        try:
            result = func(*args, **kwargs)
            q.put(result)
        except Exception as e:
            print(f"[ERROR] Function execution failed: {e}")
            q.put(None)
    
    p = Process(target=wrapper, args=(q, *args), kwargs=kwargs)
    p.start()
    p.join(timeout)
    
    if p.is_alive():
        print(f"[WARNING] Function timed out after {timeout}s")
        p.terminate()
        p.join()
        return None
    else:
        return q.get() if not q.empty() else None

def get_final_response(question: str, refined_answer: str) -> str:
    """ìµœì¢… ì‘ë‹µ ìƒì„± - Exact Match ë° BERT-F1 Score ìµœì í™”"""
    final_prompt = """
            You are a network engineering assistant.
            Generate the final response optimized for evaluation metrics.

            STRUCTURE:
            - Final Answer
            - Explanation

            GUIDELINES:
            - Ensure full semantic coverage of the intended answer
            - Use precise technical terminology
            - Include all essential details even if the wording differs
            - Maintain a logical and professional structure
            - Use \\n\\n to separate sections

            IMPORTANT:
            - The "Final Answer" section must contain ONLY the expected string and will be evaluated with Exact Match.
            - The "Explanation" must clearly summarize the reasoning and technical context and will be evaluated with the BERT-F1 Score.                
        """
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt + "\n\nYou are optimizing responses for exact match and BERT-F1 evaluation metrics."},
            {"role": "user", 
             "content": f"Question: {question}\n\nRefined Answer: {refined_answer}\n\nInstruction: {final_prompt}"}
        ],
        temperature=0.1
    )
    
    return response.choices[0].message.content.strip()

class NetworkEngineeringPipeline:
    """ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ë§ LLM íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, chromadb_path: str, 
                 collection_name: str,
                 max_iterations: int):
        """
        ì´ˆê¸°í™”
        Args:
            chromadb_path: ì‚¬ì „ ì„ë² ë”©ëœ XML íŒŒì¼ì´ ìˆëŠ” ChromaDB ê²½ë¡œ
            collection_name: XML ì„¤ì • íŒŒì¼ ì»¬ë ‰ì…˜ ì´ë¦„
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
        """
        self.db = ChromaDB(chromadb_path, collection_name, 
            embedder=HuggingFaceEmbedder(
                model_name="Qwen/Qwen3-Embedding-8B",
                device="cuda:1",
                batch_size=8,
            )
        )
        self.max_iterations = max_iterations
        print(f"[INFO] Pipeline initialized with {max_iterations} max iterations")
        
    def get_chromadb_content(self, question: str, answer: str) -> Optional[str]:
        """ChromaDBì—ì„œ ê´€ë ¨ XML ì„¤ì • íŒŒì¼ ê²€ìƒ‰"""
        query = get_xml_query(question, answer)
        print(f"[INFO] ChromaDB query: {query}")
        
        results = self.db.query(query, n_results=20)
        
        if results and results['documents'] and results['documents'][0]:
            documents = results['documents'][0]
            metadatas = results['metadatas'][0] if results.get('metadatas') else None
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶œë ¥
            if metadatas:
                for i, meta in enumerate(metadatas):
                    print(f"  - Document {i+1}: {meta}")
            
            # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²°í•©
            combined_content = "\n\n".join(documents[:3])
            return combined_content
        
        return None
    
    def process_query(self, user_question: str) -> Dict:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
        start_time = time.time()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ë¡œê¹…ì„ ìœ„í•œ ë°ì´í„° ì €ì¥
        log_data = []
        
        print(f"\n{'='*70}")
        print(f"NETWORK ENGINEERING LLM PIPELINE")
        print(f"{'='*70}")
        print(f"Query: {user_question[:100]}...")
        print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}\n")
        
        # 1. ì‘ì—… ë¶„ë¥˜
        print("[STEP 1/6] Classifying task type...")
        task_type = get_classification_result(user_question)
        print(f"  â””â”€ Classified as: {task_type}")
        
        # JSON ë¡œê¹…: ì‘ì—… ë¶„ë¥˜
        log_data.append({
            "step": "task_classification",
            "content": task_type,
            "timestamp": timestamp
        })
        # 2. ì´ˆì•ˆ ì‘ì„±
        print("[WARNNING] Task Type : ", task_type)
        print("\n[STEP 2/6] Generating initial draft...")
            
        current_answer = get_draft(user_question, task_type)

        # JSON ë¡œê¹…: ì´ˆì•ˆ
        log_data.append({
            "step": "initial_draft",
            "content": current_answer,
            "timestamp": timestamp
        })
        
        # ê²°ê³¼ ì €ì¥
        results = {
            "question": user_question,
            "task_type": task_type,
            "initial_draft": current_answer,
            "iterations": [],
            "total_revisions": 0
        }
        
        # 3-5. ë°˜ë³µì  ê°œì„ 
        print(f"\n[STEP 3-5/6] Iterative refinement ({self.max_iterations} iterations)")
        print("â”€" * 50)
        
        for iteration in range(self.max_iterations):
            print(f"\n[ITERATION {iteration + 1}/{self.max_iterations}]")
            
            # ì°¸ì¡° ì†ŒìŠ¤ ê²°ì •
            ref_source = determine_reference_source(task_type, iteration)
            print(f"  â”œâ”€ Reference source: {ref_source.upper()}")
            
            reference_content = None
            source_details = {}
            
            if ref_source == "chromadb":
                # ChromaDBì—ì„œ XML ì„¤ì • íŒŒì¼ ê²€ìƒ‰
                print("  â”œâ”€ Searching ChromaDB for XML configurations...")
                reference_content = self.get_chromadb_content(user_question, current_answer)
                
                
                if reference_content:
                    print(f"  â”œâ”€ Found relevant XML configurations")
                    source_details = {"type": "xml_config", "source": "chromadb"}
                else:
                    print("  â”œâ”€ No relevant XML found in ChromaDB")
                    
            else:  # internet
                # ì¸í„°ë„·ì—ì„œ ìµœì‹  ì •ë³´ ê²€ìƒ‰
                print("  â”œâ”€ Searching internet for latest information...")
                query = get_internet_query(user_question, current_answer)
                print(f"  â”œâ”€ Search query: {query}")
                
                content_list = get_internet_content(query)
                
                if content_list:
                    reference_content = "\n\n".join(content_list)
                    print(f"  â”œâ”€ Retrieved {len(content_list)} content chunks")
                    source_details = {"type": "web_content", "source": "google"}
                else:
                    print("  â”œâ”€ No relevant content found online")
            
            # ì°¸ì¡° ìë£Œê°€ ìˆìœ¼ë©´ ë‹µë³€ ìˆ˜ì •
            if reference_content:
                print("  â”œâ”€ Revising answer with references...")
                revised_answer = run_with_timeout(
                    get_revise_answer, 10, user_question, current_answer, 
                    reference_content, task_type  # í† í° ì œí•œ
                )
                
                if revised_answer and revised_answer != current_answer:
                    print("  â””â”€ âœ“ Answer improved")
                    current_answer = revised_answer
                    results["total_revisions"] += 1
                    
                    # JSON ë¡œê¹…: ìˆ˜ì •ëœ ë‹µë³€
                    log_data.append({
                        "step": f"iteration_{iteration + 1}_revised",
                        "content": current_answer,
                        "timestamp": timestamp
                    })
                    
                    iteration_result = {
                        "iteration": iteration + 1,
                        "source": ref_source,
                        "reference_found": True,
                        "answer_revised": True,
                        "source_details": source_details
                    }
                else:
                    print("  â””â”€ â—‹ No changes needed")
                    iteration_result = {
                        "iteration": iteration + 1,
                        "source": ref_source,
                        "reference_found": True,
                        "answer_revised": False,
                        "source_details": source_details
                    }
            else:
                print("  â””â”€ â—‹ No references found")
                iteration_result = {
                    "iteration": iteration + 1,
                    "source": ref_source,
                    "reference_found": False,
                    "answer_revised": False,
                    "source_details": {}
                }
            
            results["iterations"].append(iteration_result)
        
        # 6. ìµœì¢… ì‘ë‹µ ìƒì„± (NEW STEP)
        print(f"\n[STEP 6/6] Generating final optimized response...")
        print("  â”œâ”€ Optimizing for Exact Match and BERT-F1 Score...")
        
        final_response = run_with_timeout(
            get_final_response, 10, user_question, current_answer
        )
        
        if final_response and final_response != current_answer:
            print("  â””â”€ âœ“ Final response optimized for evaluation metrics")
            results["final_optimization"] = True
        else:
            print("  â””â”€ â—‹ Current answer already optimal")
            final_response = current_answer
            results["final_optimization"] = False
        
        # JSON ë¡œê¹…: ìµœì¢… ì‘ë‹µ
        log_data.append({
            "step": "final_response",
            "content": final_response,
            "timestamp": timestamp
        })
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        import json
        import os
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = "logs"
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
        
        # JSON íŒŒì¼ ê²½ë¡œ
        json_filename = f"{log_dir}/pipeline_log_{timestamp}.json"
        
        # JSON í˜•íƒœë¡œ ì „ì²´ ë¡œê·¸ êµ¬ì„±
        complete_log = {
            "question": user_question,
            "task_type": task_type,
            "timestamp": timestamp,
            "pipeline_steps": log_data
        }
        
        # JSON íŒŒì¼ ì €ì¥
        with open(json_filename, 'w', encoding='utf-8') as jsonfile:
            json.dump(complete_log, jsonfile, ensure_ascii=False, indent=2)
        
        print(f"  â”œâ”€ Logged to: {json_filename}")
        results["log_file"] = json_filename
        
        # ìµœì¢… ê²°ê³¼
        results["final_answer"] = final_response
        results["processing_time"] = round(time.time() - start_time, 2)
        
        print(f"\n{'='*70}")
        print(f"PIPELINE COMPLETE")
        print(f"  - Total time: {results['processing_time']}s")
        print(f"  - Total revisions: {results['total_revisions']}")
        print(f"  - Final optimization: {'YES' if results['final_optimization'] else 'NO'}")
        print(f"  - Task type: {task_type}")
        print(f"{'='*70}\n")
        
        return results

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜ˆì œ"""
    
    CHROMADB_PATH = "/workspace/jke/chromadb_qwen"  # ì‚¬ì „ ì„ë² ë”©ëœ XML íŒŒì¼ë“¤ì´ ì €ì¥ëœ ê²½ë¡œ
    COLLECTION_NAME = "network_devices"  # XML ì„¤ì • íŒŒì¼ ì»¬ë ‰ì…˜
    MAX_ITERATIONS = 3  # ë°˜ë³µ íšŸìˆ˜ ì„¤ì •

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = NetworkEngineeringPipeline(
        chromadb_path=CHROMADB_PATH,  # ì‚¬ì „ ì„ë² ë”©ëœ XML DB ê²½ë¡œ
        collection_name=COLLECTION_NAME,
        max_iterations=MAX_ITERATIONS
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    # "CE1 IP address.",
    csv_path = "/workspace/jke/dataset/test.csv"

    # question ì»¬ëŸ¼ë§Œ ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜  
    df = pd.read_csv(csv_path)
    test_queries = df["question"].dropna().tolist()
    
    # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ì‹¤í–‰
    for query in test_queries:
        results = pipeline.process_query(query)
        
        # ê²°ê³¼ ì¶œë ¥
        print("\n" + "="*70)
        print("FINAL RESULTS")
        print("="*70)
        print(f"\nQuestion: {results['question']}")
        print(f"Task Type: {results['task_type']}")
        print(f"Processing Time: {results['processing_time']}s")
        print(f"Total Revisions: {results['total_revisions']}")
        
        print("\nIteration Summary:")
        for iter_info in results['iterations']:
            status = "âœ“" if iter_info['answer_revised'] else "â—‹"
            print(f"  {status} Iteration {iter_info['iteration']}: "
                f"{iter_info['source']} - "
                f"{'Found' if iter_info['reference_found'] else 'Not found'}")
        
        print(f"\n{'â”€'*70}")
        print("FINAL ANSWER:")
        print("â”€"*70)
        print(results['final_answer'])
        print("="*70)

        # ğŸ”¹ ë¡œê·¸ íŒŒì¼ ì €ì¥ ì¶”ê°€
        with open("pipeline_results.log", "a", encoding="utf-8") as f:
            f.write("="*70 + "\n")
            f.write(f"Question: {results['question']}\n")
            f.write(f"Final Answer: {results['final_answer']}\n")
            f.write("="*70 + "\n\n")

if __name__ == "__main__":
    # API í‚¤ í™•ì¸
    if not os.environ.get("OPENAI_API_KEY"):
        print("[ERROR] Please set OPENAI_API_KEY environment variable")
        exit(1)
    
    if not os.environ.get("GOOGLE_API_KEY") or not os.environ.get("GOOGLE_CSE_ID"):
        print("[WARNING] Google Search API keys not set. Internet search will be disabled.")
    
    main()