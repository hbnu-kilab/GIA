#!/usr/bin/env python3
"""ì‹¤í—˜ ê²°ê³¼ í†µí•© ë¶„ì„ (pipeline_v2)

Non-RAG ë° RAG JSON ê²°ê³¼ë¥¼ ì…ë ¥ë°›ì•„ ë¦¬ì¹˜ ë¦¬í¬íŠ¸(Markdown)ì™€ LaTeX í‘œë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

from __future__ import annotations

import argparse
import json
import re
from pathlib import Path
from typing import Dict, List, Tuple

# Additional helpers
from config import CSV_PATH
from common.data_utils import extract_and_preprocess
import pandas as pd
import numpy as np


def _load_json(p: str | Path):
    with open(p, "r", encoding="utf-8") as f:
        return json.load(f)


def _preprocess_text(text: str) -> str:
    """í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬: ë§ˆí¬ë‹¤ìš´ ìš”ì†Œ ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±°"""
    if not isinstance(text, str):
        return str(text)
    
    # ë§ˆí¬ë‹¤ìš´ ë³¼ë“œ í‘œì‹œ ì œê±° (**text**)
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)
    
    # ë§ˆí¬ë‹¤ìš´ ì´íƒ¤ë¦­ í‘œì‹œ ì œê±° (*text*)
    text = re.sub(r'\*(.*?)\*', r'\1', text)
    
    # ê¸°íƒ€ ë§ˆí¬ë‹¤ìš´ ìš”ì†Œë“¤ ì œê±°
    text = re.sub(r'`(.*?)`', r'\1', text)  # ë°±í‹±
    text = re.sub(r'_(.*?)_', r'\1', text)  # ì–¸ë”ìŠ¤ì½”ì–´
    
    # ì—¬ëŸ¬ ê³µë°±ì„ í•˜ë‚˜ë¡œ í†µí•©
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()


def _fmt(x) -> str:
    try:
        # ì „ì²˜ë¦¬ í›„ ìˆ«ì ë³€í™˜
        if isinstance(x, str):
            x = _preprocess_text(x)
        return f"{float(x):.4f}"
    except Exception:
        return _preprocess_text(str(x))


def _pick_non_rag(non_rag_path: Path) -> Dict:
    if non_rag_path.is_file():
        return _load_json(non_rag_path)
    cand = sorted(non_rag_path.rglob("*.json"), reverse=True)
    for p in cand:
        try:
            d = _load_json(p)
            if "evaluation" in d and "results" in d:
                return d
        except Exception:
            continue
    raise RuntimeError("Non-RAG JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def _pick_rag(rag_path: Path) -> Dict:
    if rag_path.is_file():
        return _load_json(rag_path)
    # prefer rag_all_results.json
    f = rag_path / "rag_all_results.json"
    if f.exists():
        return _load_json(f)
    cand = sorted(rag_path.rglob("*.json"), reverse=True)
    for p in cand:
        try:
            d = _load_json(p)
            if "experiments" in d:
                return d
        except Exception:
            continue
    raise RuntimeError("RAG JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def _collect_rows(non_eval: Dict, rag_exps: Dict) -> Tuple[List[Dict], Dict, Dict]:
    """í‘œ ìƒì„±ìš© row ìˆ˜ì§‘ ë° 'ìµœê³  ì„±ëŠ¥' ê³„ì‚°ì— í•„ìš”í•œ ì¸ë±ìŠ¤ ë°˜í™˜"""
    rows: List[Dict] = []
    # Non-RAG baseline
    if non_eval:
        overall = non_eval.get("overall", {})
        overall_relaxed = non_eval.get("overall_relaxed", overall)  # fallback to strict if no relaxed
        rows.append(
            {
                "method": "Non-RAG",
                "setting": "-",
                "overall_em": _preprocess_text(str(overall.get("exact_match", 0))),
                "overall_f1": _preprocess_text(str(overall.get("f1_score", 0))),
                "overall_em_relaxed": _preprocess_text(str(overall_relaxed.get("exact_match", 0))),
                "overall_f1_relaxed": _preprocess_text(str(overall_relaxed.get("f1_score", 0))),
                "rule_em": _preprocess_text(str(non_eval["rule_based"]["exact_match"])),
                "enh_gt_em": _preprocess_text(str(non_eval["enhanced_llm"]["ground_truth"]["exact_match"])),
            }
        )
    # RAG variants
    for k, obj in rag_exps.items():
        ev = obj.get("evaluation") or {}
        if not ev:
            continue
        overall = ev.get("overall", {})
        overall_relaxed = ev.get("overall_relaxed", overall)  # fallback to strict if no relaxed
        rows.append(
            {
                "method": "RAG",
                "setting": k.replace("top_k_", "k="),
                "overall_em": _preprocess_text(str(overall.get("exact_match", 0))),
                "overall_f1": _preprocess_text(str(overall.get("f1_score", 0))),
                "overall_em_relaxed": _preprocess_text(str(overall_relaxed.get("exact_match", 0))),
                "overall_f1_relaxed": _preprocess_text(str(overall_relaxed.get("f1_score", 0))),
                "rule_em": _preprocess_text(str(ev["rule_based"]["exact_match"])),
                "enh_gt_em": _preprocess_text(str(ev["enhanced_llm"]["ground_truth"]["exact_match"])),
            }
        )

    # best by EM and F1 (both strict and relaxed)
    best_em = max(rows, key=lambda r: (float(r["overall_em"]), float(r["overall_f1"]))) if rows else None
    best_f1 = max(rows, key=lambda r: (float(r["overall_f1"]), float(r["overall_em"]))) if rows else None
    best_em_relaxed = max(rows, key=lambda r: (float(r["overall_em_relaxed"]), float(r["overall_f1_relaxed"]))) if rows else None
    best_f1_relaxed = max(rows, key=lambda r: (float(r["overall_f1_relaxed"]), float(r["overall_em_relaxed"]))) if rows else None
    baseline = next((r for r in rows if r["method"] == "Non-RAG"), None)

    return rows, {
        "best_em": best_em, 
        "best_f1": best_f1,
        "best_em_relaxed": best_em_relaxed,
        "best_f1_relaxed": best_f1_relaxed
    }, {"baseline": baseline}


def _markdown_report(rows: List[Dict], best: Dict, baseline_info: Dict) -> str:
    lines: List[str] = []
    lines.append("# Experiment Comparison Report (Enhanced)")
    lines.append("")
    # Highlights
    be = best.get("best_em")
    bf = best.get("best_f1")
    ber = best.get("best_em_relaxed")
    bfr = best.get("best_f1_relaxed")
    base = baseline_info.get("baseline")
    lines.append("## ğŸ” Highlights")
    if be:
        lines.append(
            f"- Best Overall EM (Strict): {_fmt(be['overall_em'])} (*{be['method']} {be['setting']}*)"
        )
    if bf:
        lines.append(
            f"- Best Overall F1 (Strict): {_fmt(bf['overall_f1'])} (*{bf['method']} {bf['setting']}*)"
        )
    if ber:
        lines.append(
            f"- Best Overall EM (Relaxed): {_fmt(ber['overall_em_relaxed'])} (*{ber['method']} {ber['setting']}*)"
        )
    if bfr:
        lines.append(
            f"- Best Overall F1 (Relaxed): {_fmt(bfr['overall_f1_relaxed'])} (*{bfr['method']} {bfr['setting']}*)"
        )
    if base and be:
        delta_em = float(be["overall_em"]) - float(base["overall_em"]) if base else 0.0
        delta_f1 = float(be["overall_f1"]) - float(base["overall_f1"]) if base else 0.0
        lines.append(
            f"- Gain vs Non-RAG baseline (Strict): EM +{_fmt(delta_em)}, F1 +{_fmt(delta_f1)}"
        )
    if base and ber:
        delta_em_r = float(ber["overall_em_relaxed"]) - float(base["overall_em_relaxed"]) if base else 0.0
        delta_f1_r = float(bfr["overall_f1_relaxed"]) - float(base["overall_f1_relaxed"]) if base else 0.0
        lines.append(
            f"- Gain vs Non-RAG baseline (Relaxed): EM +{_fmt(delta_em_r)}, F1 +{_fmt(delta_f1_r)}"
        )
    lines.append("")

    # Summary table (Markdown) - strict and relaxed
    lines.append("## ğŸ“Š Results Table (Strict Evaluation)")
    lines.append("| Method | Setting | Overall EM | Overall F1 | Rule-based EM | Enhanced LLM GT EM |")
    lines.append("|--------|---------|-----------:|-----------:|--------------:|--------------------:|")
    for r in rows:
        m = r["method"]
        s = r["setting"]
        em = _fmt(r["overall_em"]) 
        f1 = _fmt(r["overall_f1"]) 
        rem = _fmt(r["rule_em"]) 
        eh = _fmt(r["enh_gt_em"]) 
        # Bold best cells
        if best.get("best_em") is r:
            em = f"**{em}**"
        if best.get("best_f1") is r:
            f1 = f"**{f1}**"
        lines.append(f"| {m} | {s} | {em} | {f1} | {rem} | {eh} |")

    lines.append("")
    lines.append("## ğŸ“Š Results Table (Relaxed Evaluation - Order-insensitive)")
    lines.append("| Method | Setting | Overall EM | Overall F1 | Rule-based EM | Enhanced LLM GT EM |")
    lines.append("|--------|---------|-----------:|-----------:|--------------:|--------------------:|")
    for r in rows:
        m = r["method"]
        s = r["setting"]
        em_r = _fmt(r["overall_em_relaxed"]) 
        f1_r = _fmt(r["overall_f1_relaxed"]) 
        rem = _fmt(r["rule_em"]) 
        eh = _fmt(r["enh_gt_em"]) 
        # Bold best cells for relaxed
        if best.get("best_em_relaxed") is r:
            em_r = f"**{em_r}**"
        if best.get("best_f1_relaxed") is r:
            f1_r = f"**{f1_r}**"
        lines.append(f"| {m} | {s} | {em_r} | {f1_r} | {rem} | {eh} |")

    lines.append("")
    # LaTeX block preview
    lines.append("## ğŸ§ª LaTeX Table (copy-paste ready)")
    latex = _latex_table(rows, best)
    lines.append("```")
    lines.append(latex)
    lines.append("```")
    lines.append("")
    return "\n".join(lines)


# -----------------------------
# New tables per user spec
# -----------------------------
def _avg_processing_time(results: List[Dict]) -> float:
    vals = [r.get("processing_time") for r in results if isinstance(r, dict) and r.get("processing_time") is not None]
    return sum(vals) / len(vals) if vals else 0.0


def _table1_basic(non_rag: Dict, rag_exps: Dict) -> str:
    lines = ["## í‘œ 1: RAG vs Non-RAG ê¸°ë³¸ ë¹„êµ",
             "| Method | Setting | Overall EM | Overall F1 | Processing Time |",
             "|--------|---------|-----------:|-----------:|----------------:|"]
    # Non-RAG
    if non_rag:
        ov = non_rag.get("evaluation", {}).get("overall", {})
        pt = _avg_processing_time(non_rag.get("results", []))
        lines.append(f"| Non-RAG | - | {_fmt(ov.get('exact_match', 0))} | {_fmt(ov.get('f1_score', 0))} | {_fmt(pt)} |")
    # RAG k's
    for k, obj in rag_exps.items():
        ev = obj.get("evaluation", {})
        ov = ev.get("overall", {})
        pt = _avg_processing_time(obj.get("results", []))
        lines.append(f"| RAG | {k.replace('top_k_', 'k=')} | {_fmt(ov.get('exact_match', 0))} | {_fmt(ov.get('f1_score', 0))} | {_fmt(pt)} |")
    return "\n".join(lines)


def _table2_rag_k_detail(rag_exps: Dict) -> str:
    lines = ["## í‘œ 2: RAG kê°’ë³„ ìƒì„¸ ë¶„ì„",
             "| Setting | Overall EM | Overall F1 | BERTScore F1 | ROUGE-L F1 |",
             "|---------|-----------:|-----------:|-------------:|-----------:|"]
    for k, obj in rag_exps.items():
        ev = obj.get("evaluation", {})
        ov = ev.get("overall", {})
        expl = ev.get("enhanced_llm", {}).get("explanation", {})
        lines.append(
            f"| {k.replace('top_k_', 'k=')} | {_fmt(ov.get('exact_match', 0))} | {_fmt(ov.get('f1_score', 0))} | {_fmt(expl.get('bert_f1', 0))} | {_fmt(expl.get('rouge_l_f1', 0))} |"
        )
    return "\n".join(lines)


def _table3_best_vs_baseline(non_rag: Dict, rag_exps: Dict) -> str:
    # pick best k by EM (strict)
    best_key = None
    best_em = -1.0
    for k, obj in rag_exps.items():
        ev = obj.get("evaluation", {})
        em = float(ev.get("overall", {}).get("exact_match", 0) or 0)
        if em > best_em:
            best_em = em
            best_key = k
    lines = ["## í‘œ 3: RAG vs Non-RAG ìƒì„¸ ë¶„ì„ ë¹„êµ (RAG ìµœê³ ì„±ëŠ¥ K ê¸°ì¤€)",
             "Setting | BERTScore F1 | Exact Match | ROUGE-1 F1 | ROUGE-2 F1 | ROUGE-L F1 |",
             "---|---:|---:|---:|---:|---:|"]
    # Non-RAG row
    if non_rag:
        ev = non_rag.get("evaluation", {})
        expl = ev.get("enhanced_llm", {}).get("explanation", {})
        lines.append(
            f"Non-RAG | {_fmt(expl.get('bert_f1', 0))} | {_fmt(ev.get('overall', {}).get('exact_match', 0))} | {_fmt(expl.get('rouge_1_f1', 0))} | {_fmt(expl.get('rouge_2_f1', 0))} | {_fmt(expl.get('rouge_l_f1', 0))} |"
        )
    # Best RAG row
    if best_key:
        ev = rag_exps[best_key].get("evaluation", {})
        expl = ev.get("enhanced_llm", {}).get("explanation", {})
        lines.append(
            f"RAG {best_key.replace('top_k_', 'k=')} | {_fmt(expl.get('bert_f1', 0))} | {_fmt(ev.get('overall', {}).get('exact_match', 0))} | {_fmt(expl.get('rouge_1_f1', 0))} | {_fmt(expl.get('rouge_2_f1', 0))} | {_fmt(expl.get('rouge_l_f1', 0))} |"
        )
    return "\n".join(lines)


# -----------------------------
# Grouped A/B/C helpers and tables
# -----------------------------
def _split_groups(rag_exps: Dict) -> Dict[str, Dict]:
    groups = {"A": {}, "B": {}, "C": {}}
    for k, v in rag_exps.items():
        if not isinstance(k, str) or len(k) < 2:
            continue
        g = k[0]
        if g in groups:
            rest = k[2:] if k.startswith(g + "_") else k
            groups[g][rest] = v
    return groups


# -----------------------------
# New tables per user's requested format
# -----------------------------
def _mean_support_at_k(results: List[Dict]) -> float:
    vals = [1.0 if (isinstance(r, dict) and r.get("support_at_k")) else 0.0 for r in results]
    return float(sum(vals) / len(vals)) if vals else 0.0


def _table1_groups_k10_main(groups: Dict[str, Dict]) -> str:
    label = {"A": "RAG-Direct (1-pass)",
             "B": "RAG + Self-Rationale (2-pass)",
             "C": "RAG + RAT (Planâ†’Actâ†’Synthesize)"}
    lines = ["1. í‘œ 1. Main Results (Top-K = 10, ê³ ì •)",
             "| Group | Method                          | Top-K | EM(%) | F1(%) | Support@K(%) |",
             "| ----: | ------------------------------- | ----: | ----: | ----: | ------------: |"]
    for g in ["A", "B", "C"]:
        exps = groups.get(g, {})
        obj = exps.get("top_k_10")
        if not obj:
            continue
        ev = obj.get("evaluation", {})
        ov = ev.get("overall", {})
        em = float(ov.get("exact_match", 0)) * 100.0
        f1 = float(ov.get("f1_score", 0)) * 100.0
        sup = _mean_support_at_k(obj.get("results", [])) * 100.0
        lines.append(f"| {g:>5} | {label[g]:<31} | {10:>5} | {em:>5.1f} | {f1:>5.1f} | {sup:>12.1f} |")
    return "\n".join(lines)


def _table2_sensitivity_support(groups: Dict[str, Dict]) -> str:
    ks = [5, 10, 15]
    lines = ["2. í‘œ2 Sensitivity to Top-K (ê°„ë‹¨ ë¯¼ê°ë„/ì„¸ë¶€ ê²°ê³¼)",
             "| Group | Metric        | K=5 | K=10 | K=15 |",
             "| ----: | ------------- | :-: | :--: | :--: |"]
    for g in ["A", "B", "C"]:
        exps = groups.get(g, {})
        # EM row
        row_em = [g, "EM(%)"]
        for k in ks:
            obj = exps.get(f"top_k_{k}")
            if obj and obj.get("evaluation"):
                em = float(obj["evaluation"].get("overall", {}).get("exact_match", 0)) * 100.0
                row_em.append(f"{em:.1f}")
            else:
                row_em.append("â€”")
        lines.append(f"| {row_em[0]:>5} | {row_em[1]:<13} | {row_em[2]:^3} | {row_em[3]:^4} | {row_em[4]:^4} |")
        # Support@K row
        row_sup = [g, "Support@K(%)"]
        for k in ks:
            obj = exps.get(f"top_k_{k}")
            if obj:
                sup = _mean_support_at_k(obj.get("results", [])) * 100.0
                row_sup.append(f"{sup:.1f}")
            else:
                row_sup.append("â€”")
        lines.append(f"| {row_sup[0]:>5} | {row_sup[1]:<13} | {row_sup[2]:^3} | {row_sup[3]:^4} | {row_sup[4]:^4} |")
    return "\n".join(lines)


def _compute_per_sample_bert_from_results(obj: Dict, df: pd.DataFrame) -> List[float]:
    """Compute per-sample BERTScore F1 for explanations (ko) for the given experiment object.
    - Filters rows with non-empty explanation; if df has 'origin', filters origin=='enhanced_llm_with_agent'.
    - Returns list aligned to those filtered rows.
    """
    try:
        from bert_score import score as bert_score
    except Exception:
        return []
    # predictions explanations
    results = obj.get("results", [])
    preds = []
    for r in results:
        ans = r.get("final_answer", "")
        _, ex_raw, _, _ = extract_and_preprocess(ans)
        preds.append(str(ex_raw))
    # ground-truth explanations and filter
    expl_series = df.get("explanation", pd.Series([""] * len(df))).astype(str)
    if "origin" in df.columns:
        mask = (df["origin"] == "enhanced_llm_with_agent") & (expl_series.str.strip() != "")
    else:
        mask = expl_series.str.strip() != ""
    idx = [i for i, ok in enumerate(mask.tolist()) if ok]
    v_pred = [preds[i] for i in idx if i < len(preds)]
    v_true = [expl_series.iloc[i] for i in idx]
    if not v_pred or not v_true or len(v_pred) != len(v_true):
        return []
    P, R, F1 = bert_score(v_pred, v_true, lang="ko", verbose=False)
    try:
        return F1.tolist()
    except Exception:
        return []


def _table3_expl_quality_terciles(groups: Dict[str, Dict], rag_exps: Dict) -> str:
    df = pd.read_csv(CSV_PATH)
    def _pick_obj(g: str) -> Dict:
        if g == "B" and "Bexp_top_k_10" in rag_exps:
            return rag_exps["Bexp_top_k_10"]
        return groups.get(g, {}).get("top_k_10", {})
    lines = ["3. í‘œ 3. Explanation Quality by Group (BERTScore, Top-K = 10) â€” 33%ì”© ë‚˜ëˆ”",
             "| Group | BERTScore ìƒ | BERTScore ì¤‘ | BERTScore í•˜ |",
             "| ----: | -----------: | -----------: | -----------: |"]
    for g in ["A", "B", "C"]:
        obj = _pick_obj(g)
        if not obj:
            continue
        f1s = _compute_per_sample_bert_from_results(obj, df)
        if not f1s:
            lines.append(f"| {g:>5} | {'â€”':>11} | {'â€”':>11} | {'â€”':>11} |")
            continue
        vals = sorted([float(x) for x in f1s])
        n = len(vals)
        t = max(1, n // 3)
        lower = np.mean(vals[:t]) if vals[:t] else 0.0
        mid = np.mean(vals[t:2*t]) if vals[t:2*t] else 0.0
        upper = np.mean(vals[2*t:]) if vals[2*t:] else 0.0
        lines.append(f"| {g:>5} | {upper*100:>11.1f} | {mid*100:>11.1f} | {lower*100:>11.1f} |")
    return "\n".join(lines)


def _table4_qualitative_examples(groups: Dict[str, Dict], n: int = 3) -> str:
    """Top/Mid/Bottom examples from best group @ K=10 with evidence flags."""
    df = pd.read_csv(CSV_PATH)
    # pick best group by overall EM @ K=10
    best_g, best_em, best_obj = None, -1.0, None
    for g in ["A", "B", "C"]:
        obj = groups.get(g, {}).get("top_k_10")
        if not obj:
            continue
        em = float(obj.get("evaluation", {}).get("overall", {}).get("exact_match", 0) or 0)
        if em > best_em:
            best_em, best_g, best_obj = em, g, obj
    if not best_obj:
        return "4. í‘œ 4. ì •ì„± ë¶„ì„ (Qualitative Examples)\në°ì´í„° ì—†ìŒ"
    # compute per-sample f1s
    f1s = _compute_per_sample_bert_from_results(best_obj, df)
    results = best_obj.get("results", [])
    if not f1s:
        return "4. í‘œ 4. ì •ì„± ë¶„ì„ (Qualitative Examples)\nBERTScore ì‚°ì¶œ ë¶ˆê°€(íŒ¨í‚¤ì§€ ë¯¸ì„¤ì¹˜ ë˜ëŠ” ë°ì´í„° ë¶ˆì¼ì¹˜)"
    # align idx to explanation rows
    expl_series = df.get("explanation", pd.Series([""] * len(df))).astype(str)
    if "origin" in df.columns:
        mask = (df["origin"] == "enhanced_llm_with_agent") & (expl_series.str.strip() != "")
    else:
        mask = expl_series.str.strip() != ""
    valid_idx = [i for i, ok in enumerate(mask.tolist()) if ok]
    if len(valid_idx) != len(f1s):
        return "4. í‘œ 4. ì •ì„± ë¶„ì„ (Qualitative Examples)\nìƒ˜í”Œ ì ìˆ˜ ë¶ˆì¼ì¹˜ë¡œ ìƒëµ"
    pairs = list(zip(valid_idx, f1s))
    pairs_sorted = sorted(pairs, key=lambda x: x[1])
    pick = []
    if pairs_sorted:
        pick += pairs_sorted[-1:]
        pick += pairs_sorted[len(pairs_sorted)//2:len(pairs_sorted)//2+1]
        pick += pairs_sorted[:1]
    def _short(s: str, lim: int = 48) -> str:
        s = _preprocess_text(s)
        return (s[:lim] + 'â€¦') if len(s) > lim else s
    lines = ["4. í‘œ 4. ì •ì„± ë¶„ì„ (Qualitative Examples)",
             "",
             "|  # | Group | Question (ìš”ì•½) | Prediction (ìš”ì•½) | Ground Truth (ìš”ì•½) | Explanation (ìš”ì•½) | Evidence OK? | ë¹„ê³  |",
             "| -: | ----: | --------------- | ----------------- | ------------------- | ------------------ | :----------: | ---- |"]
    for rank, (idx, score) in enumerate(pick, 1):
        q = str(df.iloc[idx]["question"]) if idx < len(df) else ""
        gt = str(df.iloc[idx]["ground_truth"]) if idx < len(df) else ""
        pred = ""; expl = ""; ok = "â€”"
        if idx < len(results):
            ans = results[idx].get("final_answer", "")
            gt_raw, ex_raw, pre_gt, pre_ex = extract_and_preprocess(ans)
            pred = pre_gt
            expl = _preprocess_text(ex_raw)
            ok = "âœ”" if results[idx].get("support_at_k") else "âœ˜"
        lines.append(f"| {rank:>2} | {best_g:>5} | {_short(q)} | {_short(pred)} | {_short(gt)} | {_short(expl)} | {ok:^12} | {'':4} |")
    return "\n".join(lines)


def _groups_table_main(groups: Dict[str, Dict]) -> str:
    lines = ["## í‘œ 1 (ê·¸ë£¹): Main Results @ Top-K=10",
             "| Group | Overall EM | Overall F1 | BERTScore F1 | ROUGE-L F1 |",
             "|-------|-----------:|-----------:|-------------:|-----------:|"]
    for g in ["A", "B", "C"]:
        exps = groups.get(g, {})
        key = "top_k_10"
        if key not in exps:
            continue
        ev = exps[key].get("evaluation", {})
        ov = ev.get("overall", {})
        expl = ev.get("enhanced_llm", {}).get("explanation", {})
        lines.append(
            f"| {g} | {_fmt(ov.get('exact_match', 0))} | {_fmt(ov.get('f1_score', 0))} | {_fmt(expl.get('bert_f1', 0))} | {_fmt(expl.get('rouge_l_f1', 0))} |"
        )
    return "\n".join(lines)


def _groups_table_sensitivity(groups: Dict[str, Dict]) -> str:
    lines = ["## í‘œ 2 (ê·¸ë£¹): Sensitivity to Top-K"]
    for g in ["A", "B", "C"]:
        exps = groups.get(g, {})
        if not exps:
            continue
        lines.append(f"\n### Group {g}")
        lines.append(_table2_rag_k_detail(exps))
    return "\n".join(lines)


def _groups_table_expl_quality(groups: Dict[str, Dict], rag_exps: Dict) -> str:
    lines = ["## í‘œ 3 (ê·¸ë£¹): Explanation Quality @ Top-K=10",
             "| Group | BERTScore F1 | ROUGE-1 F1 | ROUGE-2 F1 | ROUGE-L F1 |",
             "|-------|-------------:|----------:|----------:|-----------:|"]
    for g in ["A", "B", "C"]:
        exps = groups.get(g, {})
        key = "top_k_10"
        if key not in exps:
            continue
        # For B, prefer Bexp_top_k_10 when present to reflect Pass-1 explanation quality
        label = g
        if g == "B" and "Bexp_top_k_10" in rag_exps:
            ev = rag_exps["Bexp_top_k_10"].get("evaluation", {})
            label = "B (Pass-1 explanation only)"
        else:
            ev = exps[key].get("evaluation", {})
        expl = ev.get("enhanced_llm", {}).get("explanation", {})
        lines.append(
            f"| {label} | {_fmt(expl.get('bert_f1', 0))} | {_fmt(expl.get('rouge_1_f1', 0))} | {_fmt(expl.get('rouge_2_f1', 0))} | {_fmt(expl.get('rouge_l_f1', 0))} |"
        )
    return "\n".join(lines)


def _groups_table_qualitative(groups: Dict[str, Dict], n: int = 3) -> str:
    # pick best group at top_k_10 by overall EM
    best_g = None
    best_em = -1.0
    for g in ["A", "B", "C"]:
        exps = groups.get(g, {})
        obj = exps.get("top_k_10")
        if not obj:
            continue
        em = float(obj.get("evaluation", {}).get("overall", {}).get("exact_match", 0) or 0)
        if em > best_em:
            best_em = em
            best_g = g
    if not best_g:
        return "## í‘œ 4 (ê·¸ë£¹): ì •ì„± ë¶„ì„ â€” ë°ì´í„° ì—†ìŒ"

    obj = groups[best_g]["top_k_10"]
    ev = obj.get("evaluation", {})
    per = ev.get("enhanced_llm", {}).get("explanation", {}).get("per_sample_bert_f1", [])
    results = obj.get("results", [])

    # Load dataset for GT/explanation presence
    df = pd.read_csv(CSV_PATH)
    valid_idx = [i for i, x in enumerate(df.get("explanation", [""] * len(df))) if str(x).strip()]
    # Align per-sample scores to dataset indices
    if len(per) != len(valid_idx):
        return "## í‘œ 4 (ê·¸ë£¹): ì •ì„± ë¶„ì„ â€” ìƒ˜í”Œ ì ìˆ˜ ë¶ˆì¼ì¹˜ë¡œ ìƒëµ"

    pairs = list(zip(valid_idx, per))
    pairs_sorted = sorted(pairs, key=lambda x: x[1])
    bottom = pairs_sorted[:n]
    top = pairs_sorted[-n:][::-1]
    mid_start = max(0, len(pairs_sorted) // 2 - n // 2)
    mid = pairs_sorted[mid_start: mid_start + n]

    def _block(title: str, items: List[Tuple[int, float]]) -> List[str]:
        lines = [f"### {title}"]
        lines.append("| # | BERT F1 | Question | GT | Predicted GT |")
        lines.append("|---:|--------:|---------|----|-------------|")
        for idx, score in items:
            q = str(df.iloc[idx]["question"]) if idx < len(df) else ""
            gt = str(df.iloc[idx]["ground_truth"]) if idx < len(df) else ""
            pred = ""
            if idx < len(results):
                ans = results[idx].get("final_answer", "")
                _, _, pre_gt, _ = extract_and_preprocess(ans)
                pred = pre_gt
            lines.append(f"| {idx} | {_fmt(score)} | {_preprocess_text(q)} | {_preprocess_text(gt)} | {_preprocess_text(pred)} |")
        return lines

    out = [f"## í‘œ 4 (ê·¸ë£¹): ì •ì„± ë¶„ì„ (Top-K=10, Best Group={best_g})"]
    out += _block("Top Examples", top)
    out += [""] + _block("Middle Examples", mid)
    out += [""] + _block("Bottom Examples", bottom)
    return "\n".join(out)


def _table5_taskwise(non_rag: Dict, rag_exps: Dict) -> str:
    # Load dataset to fetch GTs
    df = pd.read_csv(CSV_PATH)
    gt = [str(x) for x in df["ground_truth"].tolist()]

    def _compute_for_results(results: List[Dict]) -> Dict[str, Dict[str, float]]:
        # Build predictions aligned by index
        preds = []
        types = []
        for r in results:
            ans = r.get("final_answer", "")
            _, _, pre_gt, _ = extract_and_preprocess(ans)
            preds.append(pre_gt)
            t = r.get("task_type", "Other Tasks")
            types.append("Simple Lookup" if "Simple" in t else "Other Tasks")
        # split by type
        from common.evaluation import calculate_exact_match, calculate_f1_score
        out = {}
        for tlabel in ["Simple Lookup", "Other Tasks"]:
            idx = [i for i, t in enumerate(types) if t == tlabel]
            if not idx:
                out[tlabel] = {"em": 0.0, "f1": 0.0}
                continue
            p = [preds[i] for i in idx]
            g = [gt[i] for i in idx]
            out[tlabel] = {
                "em": calculate_exact_match(p, g),
                "f1": calculate_f1_score(p, g),
            }
        return out

    lines = ["## í‘œ 5: Task-wise Analysis",
             "| Method | Task Type | Overall EM | Overall F1 |",
             "|--------|-----------|-----------:|-----------:|"]
    # Non-RAG
    if non_rag:
        by = _compute_for_results(non_rag.get("results", []))
        for t in ["Simple Lookup", "Other Tasks"]:
            lines.append(f"| Non-RAG | {t} | {_fmt(by[t]['em'])} | {_fmt(by[t]['f1'])} |")
    # RAG (all k)
    for k, obj in rag_exps.items():
        by = _compute_for_results(obj.get("results", []))
        for t in ["Simple Lookup", "Other Tasks"]:
            lines.append(f"| RAG {k.replace('top_k_', 'k=')} | {t} | {_fmt(by[t]['em'])} | {_fmt(by[t]['f1'])} |")
    return "\n".join(lines)


def _latex_table(rows: List[Dict], best: Dict) -> str:
    """Non-RAG / RAG ì„±ëŠ¥ í…Œì´ë¸” LaTeX ì½”ë“œ ìƒì„±"""
    def b(s: str, cond: bool) -> str:
        return f"\\textbf{{{s}}}" if cond else s

    header = (
        "\\begin{table}[t]\n"
        "\\centering\n"
        "\\caption{Non-RAG vs. RAG Performance (Overall EM/F1).}\n"
        "\\label{tab:rag_nonrag_results}\n"
        "\\begin{tabular}{llcccc}\n"
        "\\hline\n"
        "Method & Setting & Overall EM & Overall F1 & Rule-based EM & Enhanced LLM GT EM \\\\ \n"
        "\\hline\n"
    )
    body_lines: List[str] = []
    be = best.get("best_em")
    bf = best.get("best_f1")
    for r in rows:
        em = _fmt(r["overall_em"]) 
        f1 = _fmt(r["overall_f1"]) 
        rem = _fmt(r["rule_em"]) 
        eh = _fmt(r["enh_gt_em"]) 
        em = b(em, r is be)
        f1 = b(f1, r is bf)
        body_lines.append(
            f"{r['method']} & {r['setting']} & {em} & {f1} & {rem} & {eh} \\\\"  # noqa: E501
        )
    footer = "\\hline\n\\end{tabular}\n\\end{table}"
    return header + "\n".join(body_lines) + "\n" + footer


def main():
    parser = argparse.ArgumentParser(description="Compare experiment results (rich report + LaTeX)")
    parser.add_argument("--non-rag", required=False, default=None, help="Non-RAG ê²°ê³¼ JSON íŒŒì¼ (ë˜ëŠ” ë””ë ‰í† ë¦¬, ì„ íƒ)")
    parser.add_argument("--rag", required=True, help="RAG ê²°ê³¼ JSON íŒŒì¼ (ë˜ëŠ” ë””ë ‰í† ë¦¬)")
    parser.add_argument("--output", default="comparison_report.md")
    parser.add_argument("--latex-output", default=None, help="LaTeX í‘œ ì €ì¥ ê²½ë¡œ (ë¯¸ì§€ì • ì‹œ outputê³¼ ë™ì¼ basename .tex)")
    parser.add_argument("--retrieval", default=None, help="Retrieval ì„±ëŠ¥ JSON(retrieval_performance.json) ê²½ë¡œ(ì˜µì…˜)")
    args = parser.parse_args()

    rag_path = Path(args.rag)
    rag_data = _pick_rag(rag_path)
    if args.non_rag:
        non_rag_path = Path(args.non_rag)
        non_rag_data = _pick_non_rag(non_rag_path)
    else:
        non_rag_data = {}

    non_eval = non_rag_data.get("evaluation", {})
    rag_exps = rag_data.get("experiments", {})

    rows, best, base = _collect_rows(non_eval, rag_exps)
    sections: List[str] = []
    # Existing summary/relaxed + LaTeX preview
    sections.append(_markdown_report(rows, best, base))
    # Table 1
    sections.append(_table1_basic(non_rag_data, rag_exps))
    # Table 2
    sections.append(_table2_rag_k_detail(rag_exps))
    # Table 3
    sections.append(_table3_best_vs_baseline(non_rag_data, rag_exps))
    # Grouped A/B/C tables if keys are present
    groups = _split_groups(rag_exps)
    if any(groups[g] for g in ["A", "B", "C"]):
        # ê¸°ì¡´ ê·¸ë£¹ í…Œì´ë¸” ìœ ì§€
        sections.append(_groups_table_main(groups))
        sections.append(_groups_table_sensitivity(groups))
        sections.append(_groups_table_expl_quality(groups, rag_exps))
        sections.append(_groups_table_qualitative(groups, n=3))
        # ì‚¬ìš©ì ì§€ì • í˜•ì‹ì˜ 4ê°œ í‘œ ì¶”ê°€
        sections.append(_table1_groups_k10_main(groups))
        sections.append(_table2_sensitivity_support(groups))
        sections.append(_table3_expl_quality_terciles(groups, rag_exps))
        sections.append(_table4_qualitative_examples(groups, n=3))
    # Table 4 (optional if file provided)
    if args.retrieval:
        try:
            rj = _load_json(Path(args.retrieval))
            m = rj.get("methods", {})
            lines = ["## í‘œ 4: Retrieval Performance (RAGë§Œ)",
                     "| Method | Recall@1 | Recall@5 | Recall@10 | MRR |",
                     "|--------|---------:|---------:|----------:|----:|"]
            h = m.get("heuristic+gpt_rerank", {})
            l = m.get("llm+gpt_rerank", {})
            lines.append(f"| Heuristic + GPT ReRank | {_fmt(h.get('recall@1', 0))} | {_fmt(h.get('recall@5', 0))} | {_fmt(h.get('recall@10', 0))} | {_fmt(h.get('mrr', 0))} |")
            lines.append(f"| LLM Query + GPT ReRank | {_fmt(l.get('recall@1', 0))} | {_fmt(l.get('recall@5', 0))} | {_fmt(l.get('recall@10', 0))} | {_fmt(l.get('mrr', 0))} |")
            sections.append("\n".join(lines))
        except Exception:
            pass
    # Table 5
    sections.append(_table5_taskwise(non_rag_data, rag_exps))
    md = "\n\n".join(sections)

    out_md = Path(args.output)
    out_md.write_text(md, encoding="utf-8")

    # Save LaTeX table separately as file, too
    latex_path = Path(args.latex_output) if args.latex_output else out_md.with_suffix(".tex")
    latex_path.write_text(_latex_table(rows, best), encoding="utf-8")

    print(f"âœ… ë¹„êµ ë¦¬í¬íŠ¸ ì €ì¥: {out_md}")
    print(f"âœ… LaTeX í‘œ ì €ì¥: {latex_path}")


if __name__ == "__main__":
    main()
