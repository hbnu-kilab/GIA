import json
import csv
import sys
from pathlib import Path

sys.path.append(str(Path(__file__).resolve().parents[1] / "src"))

from integrated_pipeline import DatasetSample, assign_task_category

def convert_to_csv(json_paths: list[str], csv_path: str):
    """
    ì—¬ëŸ¬ ë°ì´í„°ì…‹ JSON íŒŒì¼ì„ ì½ì–´ í•˜ë‚˜ì˜ í‰ê°€ìš© CSV íŒŒì¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
    """
    print(f"ğŸš€ {', '.join(json_paths)} íŒŒì¼ì„ CSVë¡œ ë³€í™˜ ì‹œì‘...")
    
    # [ìˆ˜ì •] í—¤ë”ì— 'origin' ì¶”ê°€ (rule-based vs llm-generated êµ¬ë¶„ìš©)
    header = [
        "id", "origin", "level", "task_category", "answer_type", 
        "complexity", "persona", "question", "ground_truth", 
        "explanation", "context", "source_files"
    ]
    all_samples = []

    # [ìˆ˜ì •] ì—¬ëŸ¬ JSON íŒŒì¼ì„ ìˆœíšŒí•˜ë©° ë°ì´í„° ë¡œë“œ
    for json_path in json_paths:
        path = Path(json_path)
        if not path.exists():
            print(f"âš ï¸ ê²½ê³ : {json_path} íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        with open(path, 'r', encoding='utf-8') as f:
            dataset = json.load(f)

        # JSON ë°ì´í„° êµ¬ì¡°ì— ë”°ë¼ ìƒ˜í”Œ ì¶”ì¶œ
        if isinstance(dataset, dict):
            for split in ["train", "validation", "test"]:
                if split in dataset and isinstance(dataset[split], list):
                    all_samples.extend(dataset[split])
        elif isinstance(dataset, list):
            all_samples.extend(dataset)

    rows = []
    for sample in all_samples:
        ds = DatasetSample(
            id=sample.get("id", ""),
            question=sample.get("question", ""),
            context=sample.get("context", ""),
            ground_truth=sample.get("ground_truth"),
            explanation=sample.get("explanation", ""),
            answer_type=sample.get("answer_type", ""),
            category=sample.get("category", ""),
            complexity=sample.get("complexity", ""),
            level=sample.get("level", 1),
            persona=sample.get("persona"),
            scenario=sample.get("scenario"),
            source_files=sample.get("source_files"),
            metadata=sample.get("metadata", {}),
        )

        task_category = ds.metadata.get("task_category") or assign_task_category(ds)
        origin = ds.metadata.get("origin", "unknown")
        
        # ground_truthê°€ ë¦¬ìŠ¤íŠ¸ë‚˜ ë”•ì…”ë„ˆë¦¬ì¼ ê²½ìš° JSON ë¬¸ìì—´ë¡œ ë³€í™˜
        gt = ds.ground_truth
        if isinstance(gt, (dict, list)):
            gt_str = json.dumps(gt, ensure_ascii=False)
        else:
            gt_str = str(gt)

        rows.append({
            "id": ds.id,
            "origin": origin,
            "level": ds.level,
            "task_category": task_category,
            "answer_type": ds.answer_type,
            "complexity": ds.complexity,
            "persona": ds.persona,
            "question": ds.question,
            "ground_truth": gt_str,
            "explanation": ds.explanation,
            "context": ds.context,
            "source_files": ", ".join(ds.source_files or []),
        })

    output_path = Path(csv_path)
    output_path.parent.mkdir(exist_ok=True, parents=True)
    
    with open(output_path, 'w', newline='', encoding='utf-8-sig') as f:
        writer = csv.DictWriter(f, fieldnames=header)
        writer.writeheader()
        writer.writerows(rows)
    
    print(f"âœ… CSV ë³€í™˜ ì™„ë£Œ! {len(rows)}ê°œ í–‰ì´ {csv_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == '__main__':
    # [ìˆ˜ì •] basicê³¼ enhanced JSON íŒŒì¼ì„ ëª¨ë‘ ì…ë ¥ìœ¼ë¡œ ì „ë‹¬
    convert_to_csv(
        json_paths=[
            "/workspace/Yujin/GIA/basic_dataset.json",
            
        ],
        csv_path="output/basic_dataset_many.csv"
    )