"""
í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ê¸°ì¡´ ë°ì´í„°ì…‹ì„ ì—ì´ì „íŠ¸ì™€ ë¡œì§ìœ¼ë¡œ ë™ì‹œì— ê²€ì¦
"""

import json
import argparse
from pathlib import Path
from datetime import datetime
import matplotlib.pyplot as plt

from agents.hybrid_validation_system import HybridValidationSystem, ValidationMode
from agents.hybrid_feedback_loop import HybridFeedbackLoop
from parsers.universal_parser import UniversalParser

def visualize_validation_results(validation_history: List[Dict[str, Any]]):
    """ê²€ì¦ ê²°ê³¼ë¥¼ ì‹œê°í™”"""
    
    if not validation_history:
        return
    
    iterations = range(1, len(validation_history) + 1)
    agent_acc = [h['agent_performance']['accuracy'] for h in validation_history]
    gt_acc = [h['ground_truth_quality']['accuracy'] for h in validation_history]
    
    plt.figure(figsize=(10, 6))
    plt.plot(iterations, agent_acc, 'b-o', label='Agent Accuracy')
    plt.plot(iterations, gt_acc, 'g-o', label='Ground Truth Accuracy')
    plt.xlabel('Iteration')
    plt.ylabel('Accuracy')
    plt.title('Hybrid Validation Progress')
    plt.legend()
    plt.grid(True)
    plt.savefig('validation_progress.png')
    print("\nğŸ“Š ê·¸ë˜í”„ ì €ì¥: validation_progress.png")

def main():
    parser = argparse.ArgumentParser(description='í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ')
    parser.add_argument('--dataset', required=True, help='ê²€ì¦í•  ë°ì´í„°ì…‹ ê²½ë¡œ')
    parser.add_argument('--xml-dir', required=True, help='XML íŒŒì¼ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', default='output/hybrid_validation', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')
    parser.add_argument('--mode', choices=['agent', 'logic', 'hybrid'], 
                       default='hybrid', help='ê²€ì¦ ëª¨ë“œ')
    parser.add_argument('--sample-size', type=int, help='ìƒ˜í”Œ í¬ê¸°')
    parser.add_argument('--max-iter', type=int, default=3, help='ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜')
    
    args = parser.parse_args()
    
    print("="*70)
    print("ğŸš€ í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ ì‹œì‘")
    print("="*70)
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n1. ë°ì´í„° ë¡œë”©...")
    with open(args.dataset, 'r', encoding='utf-8') as f:
        dataset = json.load(f)
    print(f"   âœ“ ë°ì´í„°ì…‹: {len(dataset)}ê°œ í•­ëª©")
    
    # 2. Network Facts íŒŒì‹±
    print("\n2. ë„¤íŠ¸ì›Œí¬ ì„¤ì • íŒŒì‹±...")
    parser = UniversalParser()
    network_facts = parser.parse_dir(args.xml_dir)
    print(f"   âœ“ ì¥ë¹„: {len(network_facts.get('devices', []))}ê°œ")
    
    # 3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    print(f"\n3. ê²€ì¦ ì‹œìŠ¤í…œ ì´ˆê¸°í™” (ëª¨ë“œ: {args.mode})...")
    mode_map = {
        'agent': ValidationMode.AGENT_ONLY,
        'logic': ValidationMode.LOGIC_ONLY,
        'hybrid': ValidationMode.HYBRID
    }
    
    validator = HybridValidationSystem(
        network_facts=network_facts,
        mode=mode_map[args.mode]
    )
    feedback_loop = HybridFeedbackLoop(network_facts)
    
    # 4. ë°˜ë³µ ê²€ì¦ ë£¨í”„
    print("\n4. ê²€ì¦ ë£¨í”„ ì‹œì‘")
    print("-"*50)
    
    iteration = 0
    validation_history = []
    
    while iteration < args.max_iter:
        print(f"\n### ë°˜ë³µ {iteration + 1}/{args.max_iter} ###")
        
        # ê²€ì¦ ìˆ˜í–‰
        validation_results, stats = validator.validate_dataset(
            dataset, 
            sample_size=args.sample_size
        )
        validation_history.append(stats)
        
        # ëª©í‘œ ë‹¬ì„± í™•ì¸
        gt_accuracy = stats['ground_truth_quality']['accuracy']
        if gt_accuracy >= 0.95:
            print("\nğŸ‰ ëª©í‘œ ë‹¬ì„±! Ground Truth ì •í™•ë„ 95% ì´ìƒ")
            break
        
        # í”¼ë“œë°± ë£¨í”„
        print("\nê°œì„  ì‘ì—… ì¤‘...")
        improved_dataset, improvement_report = feedback_loop.improve_dataset(
            validation_results,
            dataset
        )
        
        if improvement_report['total_improvements'] == 0:
            print("ë” ì´ìƒ ê°œì„ í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
            break
        
        dataset = improved_dataset
        print(f"âœ“ {improvement_report['total_improvements']}ê°œ í•­ëª© ê°œì„ ë¨")
        
        iteration += 1
    
    # 5. ê²°ê³¼ ì €ì¥
    print("\n5. ê²°ê³¼ ì €ì¥...")
    output_path = Path(args.output)
    output_path.mkdir(parents=True, exist_ok=True)
    
    # ê°œì„ ëœ ë°ì´í„°ì…‹ ì €ì¥
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    dataset_file = output_path / f"validated_dataset_{timestamp}.json"
    with open(dataset_file, 'w', encoding='utf-8') as f:
        json.dump(dataset, f, ensure_ascii=False, indent=2)
    print(f"   âœ“ ë°ì´í„°ì…‹: {dataset_file}")
    
    # ê²€ì¦ ë¦¬í¬íŠ¸ ì €ì¥
    report = {
        "mode": args.mode,
        "iterations": iteration + 1,
        "validation_history": validation_history,
        "timestamp": timestamp
    }
    
    report_file = output_path / f"validation_report_{timestamp}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    print(f"   âœ“ ë¦¬í¬íŠ¸: {report_file}")
    
    # ì‹œê°í™”
    visualize_validation_results(validation_history)
    
    # ìµœì¢… ìš”ì•½
    print("\n" + "="*70)
    print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì™„ë£Œ!")
    print("="*70)
    
    if validation_history:
        final_stats = validation_history[-1]
        print(f"\nìµœì¢… ê²°ê³¼:")
        print(f"  â€¢ ì—ì´ì „íŠ¸ ì •í™•ë„: {final_stats['agent_performance']['accuracy']:.1%}")
        print(f"  â€¢ Ground Truth ì •í™•ë„: {final_stats['ground_truth_quality']['accuracy']:.1%}")
        print(f"  â€¢ ì´ ë°˜ë³µ: {iteration + 1}íšŒ")
    
    print(f"\nê²°ê³¼ ì €ì¥ ìœ„ì¹˜: {output_path}")
    print("="*70)

if __name__ == "__main__":
    main()