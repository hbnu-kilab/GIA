"""
ë„¤íŠ¸ì›Œí¬ Q&A ë°ì´í„°ì…‹ ìƒì„± (LLM ë¯¸ì‚¬ìš©, ìˆœìˆ˜ ê·œì¹™ ê¸°ë°˜)
"""

import argparse
import json
import random
from pathlib import Path
from datetime import datetime
from typing import Dict, Any, List

from parsers.universal_parser import UniversalParser
from generators.rule_based_generator import RuleBasedGenerator, RuleBasedGeneratorConfig
from utils.builder_core import BuilderCore


def _get_all_categories(policies_path: str) -> List[str]:
    """policies.jsonì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
    with open(policies_path, 'r', encoding='utf-8') as f:
        policies_data = json.load(f)

    categories = set()
    for policy in policies_data.get("policies", []):
        category = policy.get("category")
        if category:
            categories.add(category)

    return sorted(list(categories))


def _normalize_to_text(value: Any) -> str:
    """ê°„ë‹¨í•œ í‰ë¬¸í™”: dict/list/ê¸°íƒ€ë¥¼ ì‚¬ëŒì´ ì½ì„ ìˆ˜ ìˆëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜"""
    if value is None:
        return ""
    if isinstance(value, list):
        try:
            items = sorted(list({str(x) for x in value}))
        except Exception:
            items = [str(x) for x in value]
        return ", ".join(items)
    if isinstance(value, dict):
        try:
            pairs = sorted((str(k), str(v)) for k, v in value.items())
        except Exception:
            pairs = [(str(k), str(v)) for k, v in value.items()]
        return ", ".join([f"{k}: {v}" for k, v in pairs])
    if isinstance(value, bool):
        return "true" if value else "false"
    return str(value)


def _split_dataset(items: List[Dict[str, Any]], seed: int = 42) -> Dict[str, List[Dict[str, Any]]]:
    """ê°„ë‹¨í•œ 8:1:1 ë¶„í• """
    rnd = random.Random(seed)
    items_copy = list(items)
    rnd.shuffle(items_copy)
    n = len(items_copy)
    n_train = int(n * 0.8)
    n_val = int(n * 0.1)
    train = items_copy[:n_train]
    val = items_copy[n_train:n_train + n_val]
    test = items_copy[n_train + n_val:]
    return {"train": train, "validation": val, "test": test}


def main():
    parser = argparse.ArgumentParser(
        description='ë„¤íŠ¸ì›Œí¬ Q&A ë°ì´í„°ì…‹ ìƒì„± (ê·œì¹™ ê¸°ë°˜, LLM ë¹„ì‚¬ìš©)'
    )

    # ê¸°ë³¸ ì¸ì
    parser.add_argument('--xml-dir', default='data/raw/XML_Data', help='ë„¤íŠ¸ì›Œí¬ ì„¤ì • XML íŒŒì¼ ë””ë ‰í† ë¦¬')
    # ìŠ¤í¬ë¦½íŠ¸ ìœ„ì¹˜ ê¸°ì¤€ ê¸°ë³¸ ê²½ë¡œ ì„¤ì •
    default_policies = str((Path(__file__).resolve().parents[1] / 'policies.json'))
    parser.add_argument('--policies', default=default_policies, help='ì •ì±… íŒŒì¼ ê²½ë¡œ (JSON)')
    parser.add_argument('--categories', nargs='+', help='ìƒì„±í•  ì¹´í…Œê³ ë¦¬ ëª©ë¡ (ë¯¸ì§€ì • ì‹œ policies.json ì „ì²´)')
    parser.add_argument('--output-dir', default='output/logic_only', help='ì¶œë ¥ ë””ë ‰í† ë¦¬')

    # ìƒì„± ì˜µì…˜ (ì°¸ê³ : í–¥ìƒ ìˆ˜ëŠ” í˜„ì¬ ë¬´ì˜ë¯¸. ì „ë¶€ ê·œì¹™ ê¸°ë°˜ ë™ì¼ ë¡œì§ìœ¼ë¡œ ìƒì„±)
    parser.add_argument('--basic-per-category', type=int, default=0, help='ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ ì§ˆë¬¸ ìˆ˜ ì œí•œ(0=ë¬´ì œí•œ)')
    parser.add_argument('--enhanced-per-category', type=int, default=0, help='[í˜¸í™˜ìš©] ë¯¸ì‚¬ìš©, 0ìœ¼ë¡œ ë‘ì„¸ìš”')
    parser.add_argument('--verbose', action='store_true', help='ìƒì„¸ ì¶œë ¥')

    args = parser.parse_args()

    # ì¹´í…Œê³ ë¦¬ ê²°ì •
    all_categories = _get_all_categories(args.policies)
    target_categories = args.categories or all_categories

    print("=" * 70)
    print("ğŸš€ ë„¤íŠ¸ì›Œí¬ Q&A ë°ì´í„°ì…‹ ìƒì„± (ê·œì¹™ ê¸°ë°˜)")
    print("=" * 70)
    print(f"  â€¢ XML ë””ë ‰í† ë¦¬: {args.xml_dir}")
    print(f"  â€¢ ì¹´í…Œê³ ë¦¬: {', '.join(target_categories)}")
    print(f"  â€¢ ì¶œë ¥ ë””ë ‰í† ë¦¬: {args.output_dir}")
    print("-" * 70)

    try:
        # 1) XML â†’ Facts ë¡œë“œ
        parser_u = UniversalParser()
        facts = parser_u.parse_dir(args.xml_dir)
        if args.verbose:
            print(f"[DEBUG] Loaded devices: {len(facts.get('devices', []))}")

        # 2) ì •ì±… â†’ DSL ì»´íŒŒì¼ (LLM ë¯¸ì‚¬ìš©)
        rb_cfg = RuleBasedGeneratorConfig(policies_path=args.policies)
        rb = RuleBasedGenerator(rb_cfg)
        dsl = rb.compile(capabilities=facts, categories=target_categories)
        if args.verbose:
            print(f"[DEBUG] DSL items: {len(dsl)}")

        # 3) DSL â†’ ì§ˆë¬¸/ì •ë‹µ í™•ì¥ (BuilderCore)
        core = BuilderCore(facts.get("devices", []))
        by_cat = core.expand_from_dsl(dsl)

        # 4) í›„ì²˜ë¦¬: expected_answer.value â†’ ground_truth ë¡œ í‰ë¬¸í™”, id ë¶€ì—¬
        per_cat: Dict[str, List[Dict[str, Any]]] = {}
        for cat, arr in by_cat.items():
            keep: List[Dict[str, Any]] = []
            for t in arr:
                qa = dict(t)
                qa["id"] = qa.get("test_id") or qa.get("id")
                exp = (qa.get("expected_answer") or {}).get("value")
                qa["ground_truth"] = _normalize_to_text(exp)
                qa.setdefault("explanation", f"Derived from metric {((qa.get('evidence_hint') or {}).get('metric') or '')}.")
                # ê¸°ì¡´ expected_answer í•„ë“œëŠ” ì„ íƒ ì‚¬í•­ì´ë¯€ë¡œ ìœ ì§€í•˜ì§€ ì•ŠìŒ (ê°„ê²°í™”)
                qa.pop("expected_answer", None)
                keep.append(qa)
            # ì¹´í…Œê³ ë¦¬ë³„ ìµœëŒ€ ê°œìˆ˜ ì œí•œ (0=ë¬´ì œí•œ)
            if args.basic_per_category and args.basic_per_category > 0:
                keep = keep[: args.basic_per_category]
            per_cat[cat] = keep

        # 5) ì „ì²´ í”Œë«ë¦¬ìŠ¤íŠ¸ë¡œ í†µí•© í›„ ë¶„í• 
        all_items: List[Dict[str, Any]] = []
        for cat, arr in per_cat.items():
            all_items.extend(arr)

        final_dataset = _split_dataset(all_items)

        # ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        out_dir = Path(args.output_dir)
        out_dir.mkdir(parents=True, exist_ok=True)
        dataset_file = out_dir / f"dataset_logic_only_{timestamp}.json"
        with open(dataset_file, 'w', encoding='utf-8') as f:
            json.dump(final_dataset, f, ensure_ascii=False, indent=2)

        # ìš”ì•½ ì¶œë ¥
        total_samples = sum(len(final_dataset.get(split, [])) for split in ("train", "validation", "test"))
        print("\n" + "=" * 70)
        print("âœ… ì™„ë£Œ!")
        print("=" * 70)
        print(f"  â€¢ ì´ ì§ˆë¬¸ ìˆ˜: {total_samples}ê°œ")
        print(f"    - í›ˆë ¨ìš©: {len(final_dataset.get('train', []))}ê°œ")
        print(f"    - ê²€ì¦ìš©: {len(final_dataset.get('validation', []))}ê°œ")
        print(f"    - í…ŒìŠ¤íŠ¸ìš©: {len(final_dataset.get('test', []))}ê°œ")
        print(f"\nğŸ“ ê²°ê³¼ íŒŒì¼: {dataset_file}")

        return 0

    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
