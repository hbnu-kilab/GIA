[
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-001",
    "question": "For a planned new data center, what BGP topology reconfiguration (iBGP full-mesh, route reflectors, new eBGP peering or AS reassignments) will be required to keep convergence and scale acceptable long-term, and what are the concrete peer counts and missing adjacency risks to address?",
    "context": "BGP 설정 현황:\nCE1: AS65001, 1개 피어\nCE2: AS65003, 1개 피어\nsample10: AS65000, 3개 피어\nsample7: AS65000, 3개 피어\nsample8: AS65000, 3개 피어",
    "ground_truth": {
      "required_reconfiguration": [
        "Assign a proper non‑zero BGP local AS on the new DC routers.",
        "Establish at least two eBGP peerings to independent upstream ASes for external redundancy.",
        "Adopt an iBGP route‑reflector (RR) topology for scale: deploy two RRs (in separate failure domains) and make all other routers RR clients.",
        "Do not rely on an iBGP full‑mesh once the DC has more than ~4 routers; use RRs to keep peer counts and convergence manageable."
      ],
      "concrete_peer_counts": {
        "recommended_eBGP_sessions": 2,
        "recommended_route_reflectors": 2,
        "iBGP_sessions_per_leaf_router": 2,
        "iBGP_sessions_between_RRs": 1,
        "formula_total_iBGP_sessions_with_RRs": "M*2 + R*(R-1)/2  (where M = number of leaf routers, R = number of RRs; with R=2 => M*2 + 1)",
        "example_counts": {
          "small_full_mesh_example_N=5": 10,
          "RR_design_example_M=20_R=2_total_iBGP_sessions": 41
        }
      },
      "missing_adjacency_risks_to_address": [
        "No eBGP peers configured (ebgp_remote_as_map is empty) — risk of no external reachability.",
        "Local AS is unset (bgp_local_as_numeric = 0) — BGP sessions will not form until a proper AS is configured.",
        "If only one RR is deployed, that RR becomes a single point of failure for iBGP path distribution.",
        "If a full‑mesh is used beyond a small router count, exponential growth in iBGP sessions will harm CPU, memory and convergence times."
      ]
    },
    "explanation": "The provided evidence shows zero BGP neighbors, no configured eBGP remote ASes, and a local AS value of 0, which means BGP is not currently configured for the planned data center. To provide stable external reachability and avoid a single point of failure, you must first assign a valid non‑zero local AS and create eBGP sessions to at least two independent upstream ASes. For internal routing scale, a route‑reflector design is preferred once you have more than a few routers; two RRs in separate failure domains give redundancy while keeping each leaf router to only two iBGP sessions (one to each RR). RRs should peer with each other (one session for two RRs) to ensure route propagation. The concrete peer counts above follow these choices: two external eBGP sessions for redundancy, two RRs, and each leaf maintaining two iBGP sessions. The formulas and examples show how iBGP session counts scale: a full mesh requires N*(N-1)/2 sessions and becomes impractical quickly, while the RR design yields M*2 + R*(R-1)/2 total iBGP sessions (with R=2 this is M*2+1). The listed risks directly map to the empty/zero evidence fields: absence of eBGP peers and a zero local AS prevent external BGP adjacency, a single RR risks service continuity, and excessive full‑mesh iBGP creates performance and convergence problems at scale.",
    "answer_type": "long",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "Measure current BGP neighbor totals per device to estimate peer growth when adding DC.",
          "required_metric": "bgp_neighbor_count",
          "metric_params": {
            "host": "device1"
          }
        },
        {
          "step": 2,
          "purpose": "Confirm current iBGP completeness and quantify missing pairs that may require rework.",
          "required_metric": "ibgp_missing_pairs_count",
          "metric_params": {
            "host": "device2"
          }
        },
        {
          "step": 3,
          "purpose": "Map existing eBGP peerings and remote AS assignments to understand inter-AS topology and multihoming options.",
          "required_metric": "ebgp_remote_as_map",
          "metric_params": {
            "host": "device3"
          }
        },
        {
          "step": 4,
          "purpose": "Inspect actual neighbor/state details on candidate routers to validate adjacencies and peer health before design changes.",
          "intent": "show_bgp_neighbors",
          "params": {
            "host": "device3"
          }
        },
        {
          "step": 5,
          "purpose": "Determine local AS values to decide whether to change AS assignment strategy for the new DC (e.g., same AS vs different AS for multihoming).",
          "required_metric": "bgp_local_as_numeric",
          "metric_params": {
            "host": "device3"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_1_bgp_neighbor_count": 0,
        "step_2_ibgp_missing_pairs_count": 0,
        "step_3_ebgp_remote_as_map": {},
        "step_5_bgp_local_as_numeric": 0
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-002",
    "question": "Will adding the new DC necessitate an OSPF area redesign (area splits, additional ABRs, or cost tuning) to maintain route database size and convergence SLAs, and which routers/interfaces are the primary candidates for ABR placement?",
    "context": "총 장비 수: 6 | 사용 기술: BGP, OSPF, L2VPN, L3VPN | AS 번호: 65000, 65001, 65003",
    "ground_truth": {
      "redesign_required": false,
      "abr_candidates": []
    },
    "explanation": "제공된 증거에 따르면 OSPF area 설정이 없고(ospf_area_set: 없음), area0에 속한 인터페이스 수가 0이며(ospf_area0_if_count: 0) 전체 인터페이스 수에도 문제점이 보고되지 않았습니다(interface_count: 0). 따라서 현재 상태에서는 새로운 DC 추가로 인해 즉시 OSPF 영역 재설계(영역 분할, 추가 ABR, 비용 튜닝)가 필요하다고 판단되지 않습니다. 다만 실제로 DC를 연결하면서 새로운 영역을 만들거나 대규모 접두사/경로가 유입되어 라우트 DB가 급증하거나 수렴 SLA가 악화되는 경우에는 재설계가 필요할 수 있습니다. 그런 상황이 발생하면 일반적으로 ABR은 DC의 집계/분배 계층과 백본(area 0)을 연결하는 장비, 즉 DC 업링크를 담당하는 집계 또는 코어 라우터, 그리고 WAN/경계로 동작하는 코어(엣지) 라우터들이 우선 후보가 됩니다. 권장 조치는 접두사/라우트 수와 수렴 지표를 모니터링하고, 필요 시 DC 집계·코어 경로의 라우터들을 ABR 후보로 검토하여 영역 분할 또는 비용 조정 등을 계획하는 것입니다.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "Identify which OSPF areas are currently used in the network.",
          "required_metric": "ospf_area_set",
          "metric_params": {
            "host": "device1"
          }
        },
        {
          "step": 2,
          "purpose": "Quantify Area 0 interface count to understand current spine/core load in backbone area.",
          "required_metric": "ospf_area0_if_count",
          "metric_params": {
            "host": "device2"
          }
        },
        {
          "step": 3,
          "purpose": "View OSPF routes on a core device to see current LSDB-derived path usage and potential hotspots.",
          "intent": "show_ip_route_ospf",
          "params": {
            "host": "device2"
          }
        },
        {
          "step": 4,
          "purpose": "Inventory interface counts to estimate growth in OSPF adjacencies and control-plane load after DC addition.",
          "required_metric": "interface_count",
          "metric_params": {
            "host": "device1"
          }
        },
        {
          "step": 5,
          "purpose": "If cost-based tuning is needed for traffic steering, plan candidate cost changes.",
          "intent": "set_ospf_cost",
          "params": {
            "host": "device2",
            "interface": "TenGigE0/1",
            "cost": 10
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_1_ospf_area_set": [],
        "step_2_ospf_area0_if_count": 0,
        "step_4_interface_count": 0
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-003",
    "question": "How will new DC trunk circuits and additional L2VPN endpoints affect existing L2VPN stability and what pre-emptive VC/pseudowire changes are required to avoid unidirectional or mismatch outages during expansion?",
    "context": "총 장비 수: 6 | 사용 기술: BGP, OSPF, L2VPN, L3VPN | AS 번호: 65000, 65001, 65003",
    "ground_truth": [
      "Impact: New DC trunk circuits and additional L2VPN endpoints will risk traffic re-homing, path asymmetry, and VC/pseudowire parameter mismatches that can produce unidirectional or service-mismatch outages unless proactively managed.",
      "Required pre-emptive changes:",
      "1) Pre-provision symmetric bi-directional VCs/pseudowires on all new and affected endpoints (create both directions before carrying live traffic).",
      "2) Synchronize VC/pseudowire IDs, PW parameters (encapsulation, control word usage, MTU, fragmenting behavior), and label allocations across all endpoints prior to cutover.",
      "3) Ensure consistent VLAN/tag rewrite, split-horizon and bridge-domain settings, and MAC-aging timers across sites to avoid forwarding asymmetries.",
      "4) Pre-warm and verify the control plane and forwarding state (BGP-EVPN/LDP/RSVP, LSPs, TE tunnels) so traffic uses expected symmetric paths immediately after bringing circuits up.",
      "5) Coordinate routing and TE changes to preserve symmetric return paths or explicitly steer return traffic to match forward paths.",
      "6) Stage the expansion (drain/maintenance windows, traffic mirroring), validate with end-to-end tests, and have an immediate rollback plan.",
      "7) Monitor actively for one-way packet counters, PW/VC state mismatches, MAC learning anomalies and be prepared to adjust timers or disable auto-provisioning features that could create asymmetric PWs."
    ],
    "explanation": "The provided evidence shows an existing symmetric L2VPN pairing (sample7<->sample9 and sample9<->sample7) and no current unidirectional pairs, which indicates the current deployment has bidirectional pseudowires and matching parameters. Adding new DC trunk circuits and additional L2VPN endpoints can change how traffic is routed and where pseudowires are terminated, which can introduce asymmetry or parameter mismatches that lead to one-way traffic or service-level mismatches. Unidirectional outages commonly occur when one direction of a pseudowire is not instantiated, when labels/VC IDs or encapsulation/MTU settings differ between ends, or when control-plane convergence causes one side to switch paths or LSPs while the other side does not. To prevent these failures, operators must pre-provision both directions of any new pseudowires and ensure consistency of all PW and bridge-domain parameters so there is no implicit negotiation or auto-creation that could result in asymmetric state. Synchronizing label and VC allocations prevents label/ID collisions or mismatches. Ensuring MTU, tag-rewrite, split-horizon and MAC-aging alignment avoids blackholing or asymmetric learning. Pre-warming the control plane and LSP/TE tunnels reduces transient blackholes caused by route/LSP reconvergence. Coordinating routing and TE ensures forward and return paths remain symmetric or are explicitly engineered to match, which prevents return-path asymmetry that manifests as apparent unidirectional failures. Finally, performing staged cutovers with traffic mirroring and active monitoring for one-way counters and PW state lets you validate behavior and roll back quickly if anomalies appear. These steps collectively preserve the stable, bidirectional L2VPN behavior reflected in the existing sample7<->sample9 state while enabling safe expansion.",
    "answer_type": "long",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "List all current L2VPN pseudowire pairs to understand topology and endpoints.",
          "required_metric": "l2vpn_pairs",
          "metric_params": {
            "host": "device4"
          }
        },
        {
          "step": 2,
          "purpose": "Identify existing unidirectional pseudowire issues that could be aggravated by added sites.",
          "required_metric": "l2vpn_unidirectional_pairs",
          "metric_params": {
            "host": "device4"
          }
        },
        {
          "step": 3,
          "purpose": "Check status/details of VCs on a core device to validate existing operational issues before adding new VCs.",
          "intent": "show_l2vpn_vc",
          "params": {
            "host": "device4"
          }
        },
        {
          "step": 4,
          "purpose": "Review VC status broadly to plan capacity and failure domains to avoid single points of failure.",
          "intent": "show_l2vpn_status",
          "params": {
            "host": "device4"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_1_l2vpn_pairs": [
          "sample7<->sample9",
          "sample9<->sample7"
        ],
        "step_2_l2vpn_unidirectional_pairs": []
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-004",
    "question": "What VRF design adjustments (RD/RT normalization, VRF placement, and interface binding strategy) are needed to safely onboard the new DC while preventing route leaks and preserving VRF scale management?",
    "context": "VRF 설정 현황:\nsample10: 1개 VRF (exam-l3vpn)\nsample7: 1개 VRF (exam-l3vpn)\nsample8: 1개 VRF (exam-l3vpn)\nsample9: 1개 VRF (exam-l3vpn)",
    "ground_truth": {
      "RD_RT_normalization": [
        "Adopt a single, documented RD format that guarantees global uniqueness (for example ASN:VRF-ID or 8-byte with site-id:VRF-ID) and assign an RD per VRF instance.",
        "Assign a unique RT per VRF for export/import and keep RTs consistent across devices; use distinct RTs for shared-services versus tenant-specific VRFs.",
        "Limit RT export to only the RTs that remote sites actually need and implement explicit RT import lists so no RTs are implicitly imported.",
        "Maintain a central RT/RD registry (allocation table) and use automation/templates to enforce normalization."
      ],
      "VRF_placement": [
        "Terminate tenant VRFs at the DC edge (leaf/aggregation) and keep the core as VRF-agnostic or a minimal transport-only plane to avoid core-level VRF scale and leak surface.",
        "Use a centralized shared-services VRF for cross-tenant services that truly must be shared instead of exporting each tenant’s routes everywhere.",
        "Where multi-DC tenant presence is required, use explicit site-to-site VRF export/import policies (or controlled route leaking) rather than broad RT advertisement."
      ],
      "interface_binding_strategy": [
        "Bind access interfaces and SVIs to VRFs at the leaf/access layer so VRF membership is localized and predictable.",
        "Avoid binding a very large number of distinct VRFs to a single device; consolidate low-risk tenants into shared VRFs or use NAT/shared-services to reduce per-device VRF counts when platform limits matter.",
        "Use templates and automation for interface-to-VRF bindings, employ anycast-gateway patterns if needed, and ensure all interface bindings are accompanied by strict route-target/BGP import filters to prevent accidental leaks.",
        "Require any intentional route leaking to be done via explicit RT import/export or well-audited route-policy, never via default or permissive imports."
      ]
    },
    "explanation": "Because there are currently no existing VRFs, no per-device RT lists, and no interface bindings, you should establish a normalized RD/RT scheme up front so identifiers are unique and predictable; this prevents accidental collisions and makes automated enforcement possible. Limiting RT export and using explicit RT import lists ensure that only intended routes are shared between sites and prevent broad, uncontrolled route leaks. Placing VRF termination at the DC edge (leaf/aggregation) and keeping the core transport-focused reduces the number of devices that must host VRFs, limits the blast radius of misconfiguration, and preserves core scale. Using a centralized shared-services VRF reduces duplication of common services and avoids exporting many tenant routes just to reach shared infrastructure. For interface binding, localizing VRF bindings to leaf devices (SVIs or routed access) keeps VRF membership straightforward and reduces per-device state changes. Where device VRF scale is a concern, consolidate low-risk tenants or use shared-services/NAT to reduce the number of VRFs on constrained platforms. Finally, require all route leaking to be explicit—implemented with route-target export/import controls and BGP route policies—so that every cross-site import is intentional and auditable, thereby preserving both safety from leaks and long-term VRF scale manageability.",
    "answer_type": "long",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "Count VRFs to understand current scale baseline and forecast growth impact.",
          "required_metric": "vrf_count",
          "metric_params": {
            "host": "device5"
          }
        },
        {
          "step": 2,
          "purpose": "Enumerate route-target assignments per device to detect inconsistent RTs that could cause leaks.",
          "required_metric": "vrf_rt_list_per_device",
          "metric_params": {
            "host": "device5"
          }
        },
        {
          "step": 3,
          "purpose": "Inspect VRF binding to interfaces to gauge configuration complexity and needed automation for new DC interfaces.",
          "required_metric": "vrf_interface_bind_count",
          "metric_params": {
            "host": "device5"
          }
        },
        {
          "step": 4,
          "purpose": "Show VRF configuration on a candidate device to validate syntax and RT/RD usage before scaling.",
          "intent": "show_vrf",
          "params": {
            "host": "device5"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_1_vrf_count": 0,
        "step_2_vrf_rt_list_per_device": [],
        "step_3_vrf_interface_bind_count": 0
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-005",
    "question": "Which BGP policy and peering changes (local-pref, AS-path prepends, additional eBGP links) should be implemented to ensure predictable failover and traffic steering for the new DC while minimizing global routing churn?",
    "context": "BGP 설정 현황:\nCE1: AS65001, 1개 피어\nCE2: AS65003, 1개 피어\nsample10: AS65000, 3개 피어\nsample7: AS65000, 3개 피어\nsample8: AS65000, 3개 피어",
    "ground_truth": [
      "Bring up at least two eBGP sessions to diverse upstream ASes/ISPs (different providers and ideally different PoPs).",
      "Designate one eBGP peer as the primary for normal inbound traffic and advertise the DC prefixes normally through that peer.",
      "Advertise the same prefixes to backup eBGP peers but prepend your AS 2–3 times on those backup peers so they are less preferred for inbound traffic.",
      "Configure internal routing so that routes learned from the primary eBGP peer receive a higher local-pref than routes learned from backups (for example local-pref 200 for primary, 100 default for others).",
      "Use BFD or equivalent fast failure detection on eBGP sessions and keep each eBGP session independent so failover is local and deterministic.",
      "Where supported, use selective BGP communities to ask upstreams to set their local-pref for your prefixes or to limit propagation instead of making global-visible attribute churn.",
      "Keep announcements to all eBGP peers rather than withdrawing prefixes on backup links; rely on AS-path prepends to prefer the primary so failover produces predictable, limited changes.",
      "Optionally advertise aggregated less-specific prefixes consistently from all peers (if addressing and upstream policy allow) to reduce global table churn on failover."
    ],
    "explanation": "The DC currently has no eBGP neighbors so the first requirement is to establish external BGP sessions; using at least two diverse upstreams avoids single‑point failures and gives alternative paths. Designating one peer as primary and advertising normally via that peer creates a predictable preferred path for inbound traffic. Advertising to backups but adding 2–3 AS‑path prepends makes the backup advertisements reachable but intentionally less attractive to the global routing system, so inbound traffic will only move to the backups when the primary becomes unavailable. Internal local-pref controls outbound selection inside your AS, so setting a higher local‑pref for routes learned from the primary ensures predictable outbound forwarding choices. Fast failure detection (BFD) and independent sessions allow quick local failover without depending on slow timers, and they avoid correlated failures. Using upstream communities where supported lets you request remote local‑pref changes or limit propagation in a way that is less disruptive than repeated withdraws or large-scale AS‑path manipulations. Keeping prefixes announced to backups (with prepends) instead of withdrawing them reduces global churn because routers see stable announcements with different path attributes rather than flaps and withdrawals. Finally, consistent use of aggregates or less‑specific advertisements (when allowed) reduces the number of prefixes that change on failover, further minimizing global routing churn.",
    "answer_type": "long",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "Map existing eBGP neighbors and remote AS map to identify where additional peering could be established for redundancy.",
          "required_metric": "ebgp_remote_as_map",
          "metric_params": {
            "host": "device3"
          }
        },
        {
          "step": 2,
          "purpose": "Quantify BGP neighbors to assess the scale of policy changes across devices.",
          "required_metric": "bgp_neighbor_count",
          "metric_params": {
            "host": "device3"
          }
        },
        {
          "step": 3,
          "purpose": "Inspect neighbor details to verify current attribute manipulation capabilities and session stability.",
          "intent": "show_bgp_neighbor_detail",
          "params": {
            "host": "device3",
            "neighbor": "10.0.0.1"
          }
        },
        {
          "step": 4,
          "purpose": "Propose a concrete mitigation (route-map) to influence incoming/outgoing selection during failover.",
          "intent": "set_bgp_routemap",
          "params": {
            "host": "device3",
            "routemap": "NEWDC-OUT",
            "action": "set localpref 200"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_1_ebgp_remote_as_map": {},
        "step_2_bgp_neighbor_count": 0
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-006",
    "question": "What security controls (management-plane ACLs, AAA integration, SSH ACLs/jump-host patterns) must be mandated for devices in the new DC to preserve least privilege access and ensure management-plane resilience?",
    "context": "보안 설정 현황:\nCE1: SSH ON, AAA OFF\nCE2: SSH ON, AAA OFF\nsample10: SSH ON, AAA ON\nsample7: SSH ON, AAA ON\nsample8: SSH ON, AAA ON",
    "ground_truth": [
      "Deploy centralized AAA (TACACS+ or RADIUS) with role-based authorization and accounting, enforce MFA for administrative logins, and provision redundant AAA servers with failover and cached/‘break-glass’ local accounts for emergency access.",
      "Enforce management-plane ACLs (MP-ACLs) that restrict management access to a small set of trusted management subnets and bastion/jump-host IPs, applied to control-plane interfaces and device management VRFs.",
      "Mandate a hardened jump-host/bastion pattern: all SSH to devices must originate from vetted bastion hosts; bastions must use key-based auth, MFA, host-based controls, and strict ingress ACLs.",
      "Harden SSH on devices: protocol 2 only, disable password authentication where possible, require key-based auth, set idle/timeouts/session limits, enable rate-limiting and fail2ban-like lockout policies.",
      "Use an out-of-band (OOB) management network or separate management VRF/VLAN for device management and ensure MP-ACLs prevent cross-talk from production networks.",
      "Implement control-plane protection (CoPP/Control Plane Policing) and host-based rate limiting for management services to ensure management-plane resilience under attack or load.",
      "Enable full logging and accounting to a centralized SIEM/syslog (including AAA accounting), enable SNMPv3 for monitoring, and implement real-time alerting for anomalous management access.",
      "Require periodic access reviews, least-privilege RBAC policies, documented change control for MP-ACLs and jump-host rules, and automated auditing to detect deviations.",
      "Provision secure key/certificate management (PKI) for device authentication and encrypt management traffic (SSH, TLS), with automated key rotation where possible.",
      "Test and validate recovery/resilience plans: verify AAA failover, bastion availability, OOB console access, and regular destructive drills to confirm administrators can regain control if primary management paths fail."
    ],
    "explanation": "Given the evidence that six devices have SSH enabled but there is no AAA present and no SSH ACLs applied, the environment is currently exposed to unauthorized administrative access and lacks management-plane protections. Centralized AAA with RBAC and MFA is required to enforce least privilege and provide accounting; redundant AAA servers and cached local break-glass accounts are needed to maintain access if AAA is unavailable. Management-plane ACLs and a dedicated management VRF/OOB network limit the source of management connections to trusted subnets and bastion hosts, preventing direct access from arbitrary networks. A bastion/jump-host pattern centralizes and auditable access, and bastions themselves must be hardened (key-based auth, MFA, strict ingress ACLs). Devices must have hardened SSH configurations (protocol 2, key-only where possible, timeouts, rate-limiting, and lockouts) to reduce brute-force and credential theft risk. Control-plane policing protects device CPU and control-plane services from overload or DDoS, preserving management-plane resilience. Centralized logging and AAA accounting to a SIEM and periodic automated audits support detection and forensic analysis. Finally, PKI-based keys, encrypted management protocols, documented change controls, and regular failover/resilience testing ensure the management plane remains both least-privilege and resilient in operational and failure scenarios.",
    "answer_type": "long",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "Verify which devices have SSH enabled to ensure management access model consistency.",
          "required_metric": "ssh_enabled_devices",
          "metric_params": {
            "host": "device1"
          }
        },
        {
          "step": 2,
          "purpose": "Check whether AAA is present across devices to centralize auth/logging for the new DC.",
          "required_metric": "aaa_present_bool",
          "metric_params": {
            "host": "device1"
          }
        },
        {
          "step": 3,
          "purpose": "Confirm if SSH VTY ACLs are applied or need to be created for management plane protection.",
          "required_metric": "ssh_acl_applied_check",
          "metric_params": {
            "host": "device1"
          }
        },
        {
          "step": 4,
          "purpose": "If missing, propose a remediation to lock down VTY access.",
          "intent": "set_vty_acl",
          "params": {
            "host": "device1",
            "acl_name": "MGMT_ONLY"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_1_ssh_enabled_devices": [
          "CE1",
          "CE2",
          "sample10",
          "sample7",
          "sample8",
          "sample9"
        ],
        "step_2_aaa_present_bool": false,
        "step_3_ssh_acl_applied_check": false
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-007",
    "question": "What is the recommended staged migration workflow (pre-provisioning, connectivity tests, traffic cutover, rollback) to onboard the new DC with minimal customer outage, and which concrete commands/validations should be executed during each stage?",
    "context": "총 장비 수: 6 | 사용 기술: BGP, OSPF, L2VPN, L3VPN | AS 번호: 65000, 65001, 65003",
    "ground_truth": [
      {
        "stage": "pre-provisioning",
        "objective": "Provision new DC devices, VRFs, route targets, ACLs, LB pools and application servers without impacting production.",
        "actions": [
          "Push device configs to new DC routers/switches (candidate config staging). Example commands (Cisco IOS/IOS-XE): configure terminal ; vrf definition <VRF>; ip route vrf <VRF> <prefix> <next-hop>; interface <int>; description \"NEW-DC\"",
          "On Juniper: set routing-instances <VRF> route-distinguisher <RD>; set routing-instances <VRF> protocols bgp group <grp> type external",
          "Create VRF bindings and vrf_bind_map entries as required (example config snippet): vrf_bind_map { <tenant>: { vrf: <VRF>, binds: [...] } }",
          "Create/load LB configurations in staging partition (F5 tmsh: tmsh load sys config from-terminal, or HAProxy/NGINX staging file).",
          "On compute/web servers: deploy application binaries/configs into non-production network namespace or behind disabled LB pool members.",
          "Push ACLs, route-maps and community filters but keep them in a disabled/standby state where supported."
        ],
        "validations": [
          "Configuration syntax check succeeded (vendor CLI or API returns OK).",
          "On devices: show running-config | include <VRF or binding> shows expected entries.",
          "Verify VRF bindings exist: (Cisco) show vrf ; (Juniper) show route-instance",
          "On LB stub: tmsh list ltm pool <pool> or haproxy -c -f <config> returns successful validation.",
          "Verify servers are healthy but not accepting production traffic: curl -sS --connect-timeout 2 http://<staging-server-ip>:<port>/health returns HTTP 200 locally."
        ],
        "rollback_actions": [
          "Discard staged configs or revert candidate config before commit (e.g., Cisco: configure replace or rollback; Juniper: rollback 1).",
          "Disable or delete staging LB objects."
        ]
      },
      {
        "stage": "connectivity tests (control-plane & data-plane)",
        "objective": "Verify control-plane adjacency (BGP/OSPF), route advertisement/acceptance, and basic data-plane reachability between old DC and new DC components.",
        "actions": [
          "Bring up peerings with NO export of production prefixes initially or with route-maps that block production prefixes. (Cisco example: neighbor <ip> remote-as <AS>; neighbor <ip> route-map RM-IN in).",
          "On routers/switches: show bgp summary ; show bgp neighbors <neighbor> ; show ip bgp <prefix> ; show route-map <name>",
          "Use soft-reconfiguration/clear route for soft updates: clear ip bgp <neighbor> soft in/out (if changing import).",
          "Perform VRF-specific pings and traceroutes: ping vrf <VRF> <dest> repeat 5 size 1400 ; traceroute vrf <VRF> <dest>.",
          "Run TCP/UDP sessions between test VMs in old and new DCs: iperf3 -s on server ; iperf3 -c <server> -t 30 -P 4",
          "DNS and name resolution checks: dig @<internal-dns> <service> ; nslookup <service>.",
          "Check ARP/NDP entries and MAC learning: show arp ; show mac address-table | include <MAC/IP>.",
          "Check NAT/conntrack behavior if applicable: sudo conntrack -L | grep <ip> ; sudo iptables -t nat -L -n --line-numbers."
        ],
        "validations": [
          "BGP peers show State = Established and prefix counts are as expected for non-production/test prefixes.",
          "No production prefixes are accepted/installed into RIB from new DC (route-maps must block them) unless explicitly allowed.",
          "Ping/traceroute latency and packet loss within acceptable thresholds (e.g., <5% loss, latency within SLA).",
          "iperf throughput meets minimum expected capacity for test flows.",
          "DNS resolves to test/new-IP where expected.",
          "ARP/MAC tables show correct reachability across L2/L3 fabrics."
        ],
        "rollback_actions": [
          "Shut or remove the peerings on new DC: no neighbor <ip> or deactivate BGP group; revert ACLs/route-maps to previous state."
        ]
      },
      {
        "stage": "staging & traffic mirroring (shadowing)",
        "objective": "Mirror or duplicate a copy of production traffic to the new DC without changing production forwarding; validate application behavior under real load.",
        "actions": [
          "Enable traffic mirroring on edge/ingress devices or load-balancer level (SPAN, ERSPAN, sFlow/tap, F5 packet mirroring). Example F5: tmsh modify ltm virtual <v> { mirror enabled } or use iRules to dup to new pool as passive.",
          "Alternatively, implement asymmetric routing for mirrored sessions using policy-based routing with sampled flows marked for new DC.",
          "On new DC servers: accept mirrored traffic on test interfaces or in a sandboxed path, run full application telemetry and functional tests.",
          "Run end-to-end functional tests against mirrored traffic flows (synthetic requests replayed or passive testing).",
          "Collect metrics: latency, error rates, resource utilization, application logs, DB connectivity traces."
        ],
        "validations": [
          "Mirrored traffic reaches new DC as observed via tcpdump/tshark: tcpdump -i <iface> host <source> and port <p> shows mirrored packets.",
          "Application responses from new DC are correct (compare response codes and payloads) for mirrored requests.",
          "No increase in error rate or latency seen in production path due to mirroring.",
          "Resource utilization in new DC is within expected ranges and no crashes occur."
        ],
        "rollback_actions": [
          "Disable mirroring (stop SPAN/ERSPAN or disable iRule/mirror config on LB).",
          "Remove any PBRs or sampling policies."
        ]
      },
      {
        "stage": "gradual traffic cutover (canary → partial → progressive)",
        "objective": "Shift small, controlled percentages of real traffic to the new DC progressively, verify behavior, then increase until full migration.",
        "actions": [
          "Prepare traffic-splitting mechanism: load-balancer weights, DNS low-TTL, BGP prepend/local-preference changes with limited prefix announcements, or steering via service proxy.",
          "Initial canary: send 1–5% of sessions to new DC. Example LB weight change (F5 tmsh): tmsh modify ltm pool <pool> members modify { <new-member>: { ratio 1 } <old-member>: { ratio 99 } }",
          "Validate stickiness/session persistence behavior: tmsh show ltm persistence, or check cookies/session IDs.",
          "Monitor active sessions and perform connection draining on old pool members when routing more to new DC: tmsh modify ltm pool <pool> members modify { <old-member>: { session user-disabled } } or use 'set server state maintenance' in HAProxy.",
          "For BGP-based cutover: advertise prefixes from new DC with lower MED or higher local-preference as needed, or incrementally withdraw from old DC. Use route-maps to flip next-hop only for subset of prefixes.",
          "After each step, run full transaction and user-journey tests (signup, login, payment flow, file upload/download)."
        ],
        "validations": [
          "Canary traffic success rate meets SLOs (e.g., >=99% success, error rate below threshold).",
          "End-to-end latency and backend error rates acceptable and stable over monitoring window (e.g., 15–30 minutes).",
          "No session-affinity breakage; user sessions remain intact during transitions.",
          "Application-level checks pass: database connections OK, cache hit rates normal, background jobs functioning.",
          "Network checks: show ip bgp <prefix> shows expected path selection; traceroute for live users to new DC resolves to expected path."
        ],
        "rollback_actions": [
          "Decrease weight for new DC back to 0 on LB or switch DNS/BGP back to original advertisement for affected prefixes.",
          "Drain and disable new pool members and re-enable old members if needed. Commands: tmsh modify ltm pool <pool> members modify { <new-member>: { ratio 0 } } ; tmsh modify ltm pool <pool> members modify { <old-member>: { session user-enabled } }",
          "If BGP changes applied, revert route-maps/policy and re-advertise original paths; clear BGP soft to reconverge if needed."
        ]
      },
      {
        "stage": "full cutover & validation",
        "objective": "Complete traffic migration to new DC and validate all functional, performance and operational metrics.",
        "actions": [
          "Switch all traffic distribution to new DC (LB weights, DNS, or BGP advertisements depending on method).",
          "Remove or disable old DC pool members after graceful draining: tmsh modify ltm pool <pool> members delete { <old-member> } or set state disabled and wait for connections to finish.",
          "Update monitoring/alerting to point at new DC endpoints and run full smoke tests and synthetic monitors across regions.",
          "Run a final sweep of control-plane checks: show bgp summary ; show ip route vrf <VRF> ; show route-map statistics.",
          "Validate observability: logs, metrics, tracing are flowing from new DC into central systems."
        ],
        "validations": [
          "Production traffic fully served from new DC and SLOs are met for specified windows (e.g., 1 hour, 24 hours).",
          "No unexpected increase in 5xx errors, timeouts, or dropped connections.",
          "All BGP/OSPF/IGP adjacencies stable and RIB/FIB consistent across devices.",
          "Monitoring alerts only expected ones; no new critical alerts triggered.",
          "Database replicas (if any) catch up and are in sync; data integrity checks pass for critical flows."
        ],
        "rollback_actions": [
          "If broad failure observed, trigger rollback: re-enable old DC advertisement/weights and gradually move traffic back using the gradual cutback procedure.",
          "Reinstate old configurations and monitor until stable."
        ]
      },
      {
        "stage": "post-cutover hardening & cleanup",
        "objective": "Decommission old DC routing/advertisements, finalize configs, and perform lessons-learned.",
        "actions": [
          "Remove old DC-only configs (after confirmation) such as temporary ACLs, PBRs, mirrored flows, and test-only route-maps.",
          "Update documentation, runbook and topology diagrams to reflect new production paths and VRF bindings (including vrf_bind_map updates).",
          "Perform capacity testing on new DC: full-load iperf/soak tests and spike testing.",
          "Schedule decommission tasks for old servers and network elements when safe."
        ],
        "validations": [
          "Configuration state matches intended final design; show running-config contains only desired entries.",
          "No lingering dependencies on old DC services (DNS records, automation scripts, monitoring jobs).",
          "Operational team acceptance testing completed and signed off."
        ],
        "rollback_actions": [
          "Recreate old DC artifacts if a deferred rollback becomes necessary within retention window."
        ]
      },
      {
        "stage": "rollback (emergency & staged)",
        "objective": "Provide concrete, fast rollback methods for each migration stage to restore production service to the old DC with minimal outage.",
        "emergency_actions": [
          "Traffic-level rollback via LB: set new DC pool members ratio=0 and old DC members ratio=100. Example (F5 tmsh): tmsh modify ltm pool <pool> members modify { <new-ip>: { ratio 0 } <old-ip>: { ratio 1 } }",
          "DNS rollback: change A/NS records to old DC IPs and lower TTL prior to migration to allow quick revert.",
          "BGP rollback: withdraw prefixes from new DC and re-advertise from old DC, or revert route-map/local-pref changes; use clear ip bgp * soft to expedite.",
          "Session drain reversal: re-enable old pool members and, if required, send TCP RST to new DC sessions to force re-establishment to old DC (use cautiously).",
          "If config commit caused failure, perform vendor-supported rollback: Juniper 'rollback <n>'; Cisco 'configure replace' with saved-good-config."
        ],
        "validations": [
          "After rollback, production traffic reaches old DC and SLOs restored within target RTO (documented).",
          "All BGP peers show established routes pointing to old DC and routing tables reflect pre-cutover state.",
          "Application-level functional tests succeed as they did pre-migration."
        ]
      }
    ],
    "explanation": "I produced a staged workflow that minimizes customer outage by moving from non-disruptive preparation to limited tests, passive validation (mirroring), then gradual traffic shift and final cutover. Pre-provisioning stages stage configs and VRF bindings (including vrf_bind_map) so the new DC is ready without accepting production traffic. Connectivity tests focus on control-plane (BGP/IGP) and data-plane reachability to catch routing, ARP/MAC, DNS and NAT issues early. Mirroring (shadowing) allows realistic load and application validation against production traffic without affecting customers. The gradual cutover (canary → partial → progressive) uses load-balancer weights, DNS TTLs or incremental BGP announcements so that any regression impacts only a small subset of traffic and can be rolled back quickly. Full cutover and post-cutover hardening ensure observability, capacity and cleanup are performed. For each stage I listed concrete vendor-typical commands and validations (CLI show commands, ping/traceroute/iperf, tcpdump, tmsh/haproxy checks, conntrack, BGP clears) to make tests actionable and measurable. Rollback steps are explicit and map to the corresponding changes (LB weights, DNS, BGP adverts, or config rollback) so the team can restore the previous state rapidly if SLOs are violated. This approach reduces blast radius, provides measurable gates for promotion to the next stage, and ensures reversible operations at every point.",
    "answer_type": "long",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "Get a high-level view of interface addressing and status for devices participating in the cutover.",
          "intent": "show_ip_interface_brief",
          "params": {
            "host": "device6"
          }
        },
        {
          "step": 2,
          "purpose": "Pre-provision or test static routes for candidate prefixes to validate reachability prior to migrating traffic.",
          "intent": "set_static_route",
          "params": {
            "host": "device6",
            "prefix": "192.0.2.0/24",
            "next_hop": "10.0.0.2"
          }
        },
        {
          "step": 3,
          "purpose": "Run an active connectivity test to verify that the new paths and VRFs carry traffic as expected.",
          "intent": "check_connectivity",
          "params": {
            "host": "device6",
            "target": "192.0.2.1"
          }
        },
        {
          "step": 4,
          "purpose": "Confirm VRF bindings to ensure customer segregation during and after migration.",
          "required_metric": "vrf_bind_map",
          "metric_params": {
            "host": "device6"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_4_vrf_bind_map": {}
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-008",
    "question": "Do existing devices have sufficient control-plane capacity (CPU and per-protocol process distribution) to handle the additional BGP/OSPF adjacencies caused by the new DC, or are hardware/process reassignments required?",
    "context": "BGP 설정 현황:\nCE1: AS65001, 1개 피어\nCE2: AS65003, 1개 피어\nsample10: AS65000, 3개 피어\nsample7: AS65000, 3개 피어\nsample8: AS65000, 3개 피어",
    "ground_truth": "BGP: sufficient (no additional neighbors). OSPF: insufficient data — no OSPF processes configured, so OSPF capacity cannot be confirmed and configuration/reassessment (and possibly hardware/process reassignments) will be required before a final determination.",
    "explanation": "The provided evidence shows a BGP neighbor count of 0, indicating no additional BGP adjacencies and therefore no added BGP control-plane load is expected. The evidence also shows that no OSPF process IDs are set, which means there is no current OSPF process to evaluate and the impact of new OSPF adjacencies cannot be measured. Because OSPF is not configured, you must first configure the intended OSPF processes or otherwise identify the expected number of OSPF adjacencies and then measure per-protocol CPU utilization and process distribution. Only after those measurements (or after loading a test/expected adjacency set) can you determine if the existing control-plane CPU and process allocations are sufficient. If measurements show high CPU or process contention when OSPF is active, hardware upgrades or reassigning protocol processes (or moving responsibilities to other devices) will be required.",
    "answer_type": "long",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "Inspect CPU/process usage to check current headroom for additional control-plane load.",
          "intent": "show_processes_cpu",
          "params": {
            "host": "device2"
          }
        },
        {
          "step": 2,
          "purpose": "Enumerate OSPF process IDs to understand how many OSPF instances/processes are active (affects memory/CPU).",
          "required_metric": "ospf_process_ids_set",
          "metric_params": {
            "host": "device2"
          }
        },
        {
          "step": 3,
          "purpose": "Measure BGP neighbor count to estimate incremental session load on candidate routers.",
          "required_metric": "bgp_neighbor_count",
          "metric_params": {
            "host": "device2"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_2_ospf_process_ids_set": [],
        "step_3_bgp_neighbor_count": 0
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-009",
    "question": "What multihoming architecture (single AS with route reflectors vs separate AS per site vs eBGP between DC and providers) provides the best balance of resilience and operational simplicity for the new DC across the existing AS groups?",
    "context": "BGP 설정 현황:\nCE1: AS65001, 1개 피어\nCE2: AS65003, 1개 피어\nsample10: AS65000, 3개 피어\nsample7: AS65000, 3개 피어\nsample8: AS65000, 3개 피어",
    "ground_truth": "Single AS with route reflectors (iBGP route-reflection across the DC and sites)",
    "explanation": "Given the evidence that there are no configured eBGP neighbors and the ebgp_remote_as_map is empty, introducing eBGP peering today would add significant operational complexity. Running a single AS internally with route reflectors provides the best balance of operational simplicity and resilience because it avoids per-site AS management and full-mesh iBGP while still allowing redundant control-plane paths through multiple, geographically redundant route reflectors. This design reduces configuration and operational overhead compared with separate AS per site and is simpler to operate than managing eBGP sessions to multiple providers, while resilience can be achieved by deploying at least two independent RRs, ensuring RRs are themselves redundant and colocated across failure domains, and keeping proper route-reflection cluster IDs and next-hop/attribute handling. If, however, external providers or business requirements force distinct AS relationships or fine-grained per-provider policy and AS-path visibility, then eBGP at the DC edge would be warranted; otherwise, the single-AS-with-RRs approach is the recommended default for balancing simplicity and resilience.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "List eBGP neighbor mappings to see existing multihoming patterns and potential provider adjacencies.",
          "required_metric": "ebgp_remote_as_map",
          "metric_params": {
            "host": "device3"
          }
        },
        {
          "step": 2,
          "purpose": "Get list of eBGP neighbors to plan peer additions and to ensure no unexpected remote AS conflicts.",
          "required_metric": "neighbor_list_ebgp",
          "metric_params": {
            "host": "device3"
          }
        },
        {
          "step": 3,
          "purpose": "Check BGP summary to validate session counts and identify candidate routers for extra links or route-reflection roles.",
          "intent": "show_bgp_summary",
          "params": {
            "host": "device3"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_1_ebgp_remote_as_map": {},
        "step_2_neighbor_list_ebgp": []
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-010",
    "question": "What route-target/export-import policy controls and routing policy enforcement (route-maps/filters) must be put in place around the new DC to guarantee no accidental VRF route leaks, and how should these policies be applied and tested?",
    "context": "VRF 설정 현황:\nsample10: 1개 VRF (exam-l3vpn)\nsample7: 1개 VRF (exam-l3vpn)\nsample8: 1개 VRF (exam-l3vpn)\nsample9: 1개 VRF (exam-l3vpn)",
    "ground_truth": {
      "route_target_policies": [
        "Per‑VRF explicit export policy that only attaches the configured route‑target(s) to routes exported from that VRF (route‑map/route‑policy that sets the correct RT extended‑community and denies other RTs).",
        "Per‑VRF explicit import policy that only accepts routes carrying the configured route‑target(s) for that VRF and denies all other RTs (use extended‑community/RT match lists; implicit/explicit deny all otherwise).",
        "Per‑device (per‑PE) route‑target allowlists derived from the authoritative VRF→RT inventory, deployed to every PE that peers with the new DC; do not rely on broad or default accept behavior.",
        "Apply RT filtering as close to ingress as possible (on the PE facing the DC and on PE interfaces receiving VPNv4/vpnv6 routes) so unauthorized routes are dropped before propagation.",
        "Strip or normalize unexpected route‑target communities on export when necessary to prevent accidental propagation of locally attached RTs (export route‑map that removes non‑allowed RTs).",
        "If route‑reflectors are used, ensure PEs enforce RT import/export rules; do not rely on RRs to enforce VRF isolation. Apply filtering at edge PEs rather than only on RRs.",
        "Adopt a default‑deny posture for VPN route imports and only add explicit permit statements for approved RTs; maintain the approved RT list in configuration management/automation.",
        "Log and count denied import/export events and generate alerts for any refused RT imports/exports to detect misconfigurations or malicious advertisements.",
        "Ensure route distinguishers (RDs) are unique and consistent to avoid accidental VRF collisions when combined with RT enforcement."
      ],
      "application_points": [
        "Deploy export route‑maps/route‑policies on the VRF export path (on the PE when redistributing into BGP VPNv4/VPNv6 or when advertising to CE/other PEs).",
        "Deploy import route‑maps/route‑policies on inbound BGP sessions at the PE that receives routes from the new DC and on any interfaces/processes that import routes into VRFs.",
        "Apply per‑device RT allowlists on every PE that peers with the new DC; use automation to ensure consistency and to populate missing per‑device vrf_rt lists.",
        "If using route‑reflectors, keep RRs transparent for RT enforcement and ensure filtering is present at all PEs rather than relying on RRs to drop invalid RTs."
      ],
      "testing_steps": [
        "Positive test: advertise a test prefix from the new DC with the correct RT and verify it is present only in the intended VRF(s) and on expected remote PEs.",
        "Negative test: advertise prefixes from the DC with wrong or missing RTs and verify they are rejected at ingress (do not appear in any VRF, BGP VPNv4 tables, or on other PEs).",
        "Edge test: from each PE serving the DC, run routing table and BGP VPNv4/VPNv6 checks to confirm only permitted RT entries exist (show bgp vpnv4/vpnv6 sessions and routes, and show route‑target mappings).",
        "Leak simulation: attempt to intentionally leak a route (simulate misconfigured RT) and confirm that import filters on the PE prevent the leak from reaching other VRFs or remote sites.",
        "Policy verification: validate route‑maps/route‑policies are applied where intended (show run/policy, show route‑map counters) and check counters/logs for denied matches.",
        "Staged rollout: apply policies to a single PE first, run the above tests, then progressively roll out to all PEs with rollback plan and configuration snapshots available."
      ]
    },
    "explanation": "To guarantee no accidental VRF route leaks, you must adopt an explicit allowlist approach for route‑targets: only permit the exact RTs that a VRF is supposed to receive and only attach the exact RTs that a VRF is supposed to export. Import filtering should be enforced at ingress on the edge PE so that unauthorized routes never propagate into the VPNv4/VPNv6 tables or to other PEs. Because your inventory indicates there is no per‑device VRF RT list currently (vrf_rt_list_per_device: 없음), you need to create and maintain a per‑PE/per‑VRF RT allowlist and push it via automation to eliminate configuration divergence. Export policies should also ensure that only the intended RT extended communities are attached and that unexpected RTs are stripped before advertising routes outward. Relying on route‑reflectors to enforce RT isolation is risky: RRs should not be the primary enforcement point because they may reflect routes without the edge constraints, so keep enforcement on edge PEs. Adopt a default‑deny import posture and log/alert on denied imports so misconfigurations are visible quickly. Testing must include positive cases (correct RTs are accepted and reach only intended VRFs), negative cases (wrong/missing RTs are rejected and do not appear elsewhere), and deliberate leak simulations to validate filters. Verify policy application by checking route‑map/policy counters and BGP VPNv4/vpnv6 route tables on each PE, and perform a staged rollout with the ability to revert if unexpected behavior is observed. These controls together ensure that only explicitly authorized RTs and routes are exchanged, preventing accidental VRF leaks.",
    "answer_type": "long",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "network_architect",
    "scenario": "네트워크 확장 계획",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "step": 1,
          "purpose": "Extract per-device RT lists to identify inconsistencies that could allow leaking.",
          "required_metric": "vrf_rt_list_per_device",
          "metric_params": {
            "host": "device5"
          }
        },
        {
          "step": 2,
          "purpose": "Show VRF configuration to validate current import/export controls and confirm candidate VRFs for stricter policies.",
          "intent": "show_vrf",
          "params": {
            "host": "device5"
          }
        },
        {
          "step": 3,
          "purpose": "Propose a concrete BGP route-policy to be applied to edge routers to prevent inadvertent import/export.",
          "intent": "set_bgp_routemap",
          "params": {
            "host": "device5",
            "routemap": "BLOCK-LEAK",
            "action": "deny matched-rt"
          }
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_1_vrf_rt_list_per_device": []
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-001",
    "question": "Which devices in AS 65000 have fewer than 3 iBGP peers? (Answer: device name list)",
    "context": "BGP 설정 현황:\nCE1: AS65001, 1개 피어\nCE2: AS65003, 1개 피어\nsample10: AS65000, 3개 피어\nsample7: AS65000, 3개 피어\nsample8: AS65000, 3개 피어",
    "ground_truth": [],
    "explanation": "증거에 따르면 유일한 장비는 호스트명 CE1이며 해당 장비의 bgp_local_as_numeric 값이 0으로 표시되어 있습니다. 즉 CE1은 AS 65000에 속하지 않습니다. CE1의 neighbor_list_ibgp가 '없음'으로 보고되어 iBGP 피어 수는 0이지만, 질문은 AS 65000에 속한 장비를 대상으로 하므로 CE1는 해당되지 않습니다. 증거에 AS 65000에 속한 다른 장비 정보가 전혀 없으므로 AS 65000에 속하면서 iBGP 피어가 3개 미만인 장비는 없습니다.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "system_hostname_text",
          "metric_params": {}
        },
        {
          "required_metric": "bgp_local_as_numeric",
          "metric_params": {
            "host": "{each_host}"
          }
        },
        {
          "required_metric": "neighbor_list_ibgp",
          "metric_params": {
            "host": "{each_host}"
          }
        },
        {
          "description": "For each host: if bgp_local_as_numeric == '65000' and length(neighbor_list_ibgp) < 3 then include hostname in result"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_system_hostname_text": "CE1",
        "step_None_bgp_local_as_numeric": 0,
        "step_None_neighbor_list_ibgp": []
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-002",
    "question": "How many devices have at least one missing iBGP pair? (Answer: number)",
    "context": "BGP 설정 현황:\nCE1: AS65001, 1개 피어\nCE2: AS65003, 1개 피어\nsample10: AS65000, 3개 피어\nsample7: AS65000, 3개 피어\nsample8: AS65000, 3개 피어",
    "ground_truth": 0,
    "explanation": "The provided ibgp_missing_pairs_count is 0, which indicates there are no devices that have at least one missing iBGP pair.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "system_hostname_text",
          "metric_params": {}
        },
        {
          "required_metric": "ibgp_missing_pairs_count",
          "metric_params": {
            "host": "{each_host}"
          }
        },
        {
          "description": "Count hosts where ibgp_missing_pairs_count > 0; return that count"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_system_hostname_text": "CE1",
        "step_None_ibgp_missing_pairs_count": 0
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-003",
    "question": "Which devices are involved in unidirectional L2VPN pairs? (Answer: device name list)",
    "context": "총 장비 수: 6 | 사용 기술: BGP, OSPF, L2VPN, L3VPN | AS 번호: 65000, 65001, 65003",
    "ground_truth": [],
    "explanation": "The provided evidence states that there are no entries for l2vpn_unidirectional_pairs (없음). Therefore, there are no devices involved in unidirectional L2VPN pairs, so the device list is empty.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "l2vpn_unidirectional_pairs",
          "metric_params": {}
        },
        {
          "description": "Extract endpoint device names from each unidirectional pair and deduplicate to produce the device name list"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_l2vpn_unidirectional_pairs": []
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-005",
    "question": "Which devices have SSH disabled (SSH not present)? (Answer: device name list)",
    "context": "보안 설정 현황:\nCE1: SSH ON, AAA OFF\nCE2: SSH ON, AAA OFF\nsample10: SSH ON, AAA ON\nsample7: SSH ON, AAA ON\nsample8: SSH ON, AAA ON",
    "ground_truth": [],
    "explanation": "The provided evidence indicates \"ssh_missing_devices: 없음\" (none), so no devices have SSH disabled; therefore the list of device names with SSH not present is empty.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "ssh_missing_devices",
          "metric_params": {}
        },
        {
          "description": "Return the ssh_missing_devices list (if empty, return empty list)"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_ssh_missing_devices": []
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-006",
    "question": "Which devices participate in L2VPN PWID mismatch pairs? (Answer: device name list)",
    "context": "총 장비 수: 6 | 사용 기술: BGP, OSPF, L2VPN, L3VPN | AS 번호: 65000, 65001, 65003",
    "ground_truth": [
      "sample7",
      "sample9"
    ],
    "explanation": "The evidence shows l2vpn_pwid_mismatch_pairs as sample7<->sample9 and sample9<->sample7, so the unique devices participating in these mismatch pairs are sample7 and sample9.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "l2vpn_pwid_mismatch_pairs",
          "metric_params": {}
        },
        {
          "description": "Extract endpoint device names from each PWID mismatch pair and deduplicate to form the device list"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_l2vpn_pwid_mismatch_pairs": [
          "sample7<->sample9",
          "sample9<->sample7"
        ]
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-007",
    "question": "Which devices have VRFs missing route-target (RT) assignments? (Answer: device name list)",
    "context": "VRF 설정 현황:\nsample10: 1개 VRF (exam-l3vpn)\nsample7: 1개 VRF (exam-l3vpn)\nsample8: 1개 VRF (exam-l3vpn)\nsample9: 1개 VRF (exam-l3vpn)",
    "ground_truth": [],
    "explanation": "제공된 증거에 'None단계 (vrf_without_rt_pairs): 없음'이라고 명시되어 있으므로 RT 할당이 누락된 VRF를 가진 장치가 없습니다. 따라서 장치 이름 목록은 빈 배열입니다.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "vrf_without_rt_pairs",
          "metric_params": {}
        },
        {
          "description": "Extract device names from vrf_without_rt_pairs and deduplicate to produce the device name list"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_vrf_without_rt_pairs": []
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-008",
    "question": "How many devices have more than 2 OSPF area-0 interfaces? (Answer: number)",
    "context": "총 장비 수: 6 | 사용 기술: BGP, OSPF, L2VPN, L3VPN | AS 번호: 65000, 65001, 65003",
    "ground_truth": 0,
    "explanation": "The only device listed is CE1 with ospf_area0_if_count equal to 0, which is not greater than 2; therefore zero devices have more than 2 OSPF area-0 interfaces.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "system_hostname_text",
          "metric_params": {}
        },
        {
          "required_metric": "ospf_area0_if_count",
          "metric_params": {
            "host": "{each_host}"
          }
        },
        {
          "description": "Count hosts where ospf_area0_if_count > 2 and return that count"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_system_hostname_text": "CE1",
        "step_None_ospf_area0_if_count": 0
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-009",
    "question": "Which devices have more than 48 interfaces? (Answer: device name list)",
    "context": "총 장비 수: 6 | 사용 기술: BGP, OSPF, L2VPN, L3VPN | AS 번호: 65000, 65001, 65003",
    "ground_truth": [],
    "explanation": "No devices have more than 48 interfaces. The only device provided in the evidence is CE1 with an interface count of 0, which is not greater than 48.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "system_hostname_text",
          "metric_params": {}
        },
        {
          "required_metric": "interface_count",
          "metric_params": {
            "host": "{each_host}"
          }
        },
        {
          "description": "Return list of hosts where interface_count > 48"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_system_hostname_text": "CE1",
        "step_None_interface_count": 0
      }
    }
  },
  {
    "id": "ENHANCED_ENHANCED-ANALYTICAL-010",
    "question": "What is the hostname of the device with the highest local user count? (Answer: hostname string)",
    "context": "총 장비 수: 6 | 사용 기술: BGP, OSPF, L2VPN, L3VPN | AS 번호: 65000, 65001, 65003",
    "ground_truth": "CE1",
    "explanation": "The provided evidence lists a single device with hostname CE1 and a local user count of 0; because it is the only device reported, CE1 has the highest local user count among the devices in the evidence.",
    "answer_type": "short",
    "category": "advanced",
    "complexity": "analytical",
    "level": 3,
    "persona": "troubleshooter",
    "scenario": "규정 위반 장비 식별",
    "source_files": [
      "ce1.xml",
      "ce2.xml",
      "sample10.xml",
      "sample7.xml",
      "sample8.xml",
      "sample9.xml"
    ],
    "metadata": {
      "origin": "enhanced_llm_with_agent",
      "reasoning_plan": [
        {
          "required_metric": "system_hostname_text",
          "metric_params": {}
        },
        {
          "required_metric": "system_user_count",
          "metric_params": {
            "host": "{each_host}"
          }
        },
        {
          "description": "Find the host with the maximum system_user_count; return that hostname (if tie, return all tied hostnames as a single comma-separated string)"
        }
      ],
      "reasoning_requirement": "",
      "expected_analysis_depth": "detailed",
      "evidence": {
        "step_None_system_hostname_text": "CE1",
        "step_None_system_user_count": 0
      }
    }
  }
]