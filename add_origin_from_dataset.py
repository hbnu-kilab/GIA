#!/usr/bin/env python3
"""
ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ origin ì»¬ëŸ¼ì„ ê°€ì ¸ì™€ì„œ ì‹¤í—˜ ê²°ê³¼ CSVì— ì¶”ê°€
"""
import pandas as pd
import json
from pathlib import Path
from typing import Dict
from Evaluation.pipeline_v2.common.data_utils import clean_ground_truth_text
from Evaluation.pipeline_v2.common.evaluation import calculate_exact_match, calculate_f1_score, calculate_relaxed_exact_match, calculate_relaxed_f1_score

def add_origin_from_dataset(csv_path: str, dataset_path: str = "Evaluation/dataset/test_fin.csv") -> str:
    """ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ origin ì •ë³´ë¥¼ ê°€ì ¸ì™€ì„œ CSVì— ì¶”ê°€"""
    print(f"Origin ì»¬ëŸ¼ ì¶”ê°€ ì¤‘: {csv_path}")
    
    # ì›ë³¸ ë°ì´í„°ì…‹ ì½ê¸°
    dataset = pd.read_csv(dataset_path)
    print(f"ì›ë³¸ ë°ì´í„°ì…‹: {len(dataset)} ì§ˆë¬¸")
    
    # questionì„ í‚¤ë¡œ í•˜ëŠ” origin ë§¤í•‘ ìƒì„±
    origin_map = {}
    for _, row in dataset.iterrows():
        question = str(row['question']).strip()
        origin = str(row['origin']).strip()
        origin_map[question] = origin
    
    print(f"Origin ë§¤í•‘: {len(origin_map)} ì§ˆë¬¸")
    origin_counts = dataset['origin'].value_counts()
    for origin, count in origin_counts.items():
        print(f"  {origin}: {count}ê°œ")
    
    # ì‹¤í—˜ ê²°ê³¼ CSV ì½ê¸°
    df = pd.read_csv(csv_path)
    print(f"ì‹¤í—˜ ê²°ê³¼: {len(df)} ì§ˆë¬¸")
    
    # origin ì»¬ëŸ¼ ì¶”ê°€
    origins = []
    missing_questions = []
    
    for _, row in df.iterrows():
        question = str(row['question']).strip()
        if question in origin_map:
            origins.append(origin_map[question])
        else:
            origins.append('enhanced_llm_with_agent')  # ê¸°ë³¸ê°’
            missing_questions.append(question)
    
    if missing_questions:
        print(f"âš ï¸ ë§¤ì¹­ë˜ì§€ ì•Šì€ ì§ˆë¬¸ {len(missing_questions)}ê°œ (ê¸°ë³¸ê°’ ì ìš©)")
        for q in missing_questions[:3]:  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
            print(f"  - {q[:80]}...")
    
    # origin ì»¬ëŸ¼ ì¶”ê°€
    df['origin'] = origins
    
    # ìƒˆ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
    output_path = csv_path.replace('.csv', '_with_origin.csv')
    df.to_csv(output_path, index=False)
    
    # ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½
    final_counts = pd.Series(origins).value_counts()
    print(f"\nìµœì¢… ë¶„ë¥˜ ê²°ê³¼:")
    for origin, count in final_counts.items():
        print(f"  {origin}: {count}ê°œ ({count/len(origins)*100:.1f}%)")
    
    return output_path

def evaluate_with_origin(csv_path: str) -> Dict:
    """origin ì»¬ëŸ¼ì„ ê³ ë ¤í•œ ì¬í‰ê°€"""
    print(f"Origin ê¸°ë°˜ ì¬í‰ê°€ ì¤‘: {csv_path}")
    
    df = pd.read_csv(csv_path)
    
    # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ì •ë¦¬
    predictions = []
    ground_truths = []
    origins = df['origin'].tolist()
    
    for _, row in df.iterrows():
        # ì˜ˆì¸¡ ê²°ê³¼ ì •ë¦¬ (ì½œë¡  ì œê±° ì ìš©)
        pred_raw = str(row['pre_GT']) if pd.notna(row['pre_GT']) else ""
        pred_clean = clean_ground_truth_text(pred_raw)
        predictions.append(pred_clean)
        
        # ì •ë‹µ ì •ë¦¬
        gt_raw = str(row['ground_truth']) if pd.notna(row['ground_truth']) else ""
        gt_clean = clean_ground_truth_text(gt_raw)
        ground_truths.append(gt_clean)
    
    # ì¸ë±ìŠ¤ ê·¸ë£¹ ë¶„ë¦¬
    rule_idx = [i for i, o in enumerate(origins) if o == "rule_based"]
    enhanced_idx = [i for i, o in enumerate(origins) if o == "enhanced_llm_with_agent"]
    
    print(f"Rule-based ì§ˆë¬¸: {len(rule_idx)}ê°œ")
    print(f"Enhanced LLM ì§ˆë¬¸: {len(enhanced_idx)}ê°œ")
    
    # ì „ì²´ í‰ê°€
    overall_em = calculate_exact_match(predictions, ground_truths)
    overall_f1 = calculate_f1_score(predictions, ground_truths)
    overall_em_relaxed = calculate_relaxed_exact_match(predictions, ground_truths)
    overall_f1_relaxed = calculate_relaxed_f1_score(predictions, ground_truths)
    
    # Rule-based í‰ê°€ (GTë§Œ)
    if rule_idx:
        rule_pred_gt = [predictions[i] for i in rule_idx]
        rule_true_gt = [ground_truths[i] for i in rule_idx]
        rule_em = calculate_exact_match(rule_pred_gt, rule_true_gt)
        rule_f1 = calculate_f1_score(rule_pred_gt, rule_true_gt)
        rule_em_relaxed = calculate_relaxed_exact_match(rule_pred_gt, rule_true_gt)
        rule_f1_relaxed = calculate_relaxed_f1_score(rule_pred_gt, rule_true_gt)
    else:
        rule_em = rule_f1 = rule_em_relaxed = rule_f1_relaxed = 0.0
    
    # Enhanced LLM í‰ê°€ (GT + ì„¤ëª…)
    if enhanced_idx:
        enhanced_pred_gt = [predictions[i] for i in enhanced_idx]
        enhanced_true_gt = [ground_truths[i] for i in enhanced_idx]
        enhanced_em = calculate_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_f1 = calculate_f1_score(enhanced_pred_gt, enhanced_true_gt)
        enhanced_em_relaxed = calculate_relaxed_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_f1_relaxed = calculate_relaxed_f1_score(enhanced_pred_gt, enhanced_true_gt)
    else:
        enhanced_em = enhanced_f1 = enhanced_em_relaxed = enhanced_f1_relaxed = 0.0
    
    results = {
        "total_questions": len(predictions),
        "overall": {
            "exact_match": overall_em,
            "f1_score": overall_f1,
            "exact_match_relaxed": overall_em_relaxed,
            "f1_score_relaxed": overall_f1_relaxed
        },
        "rule_based": {
            "question_count": len(rule_idx),
            "exact_match": rule_em,
            "f1_score": rule_f1,
            "exact_match_relaxed": rule_em_relaxed,
            "f1_score_relaxed": rule_f1_relaxed
        },
        "enhanced_llm_with_agent": {
            "question_count": len(enhanced_idx),
            "exact_match": enhanced_em,
            "f1_score": enhanced_f1,
            "exact_match_relaxed": enhanced_em_relaxed,
            "f1_score_relaxed": enhanced_f1_relaxed
        }
    }
    
    return results

def update_json_with_origin_evaluation(json_path: str, evaluation_results: Dict):
    """JSON íŒŒì¼ì„ origin ê¸°ë°˜ í‰ê°€ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸"""
    print(f"JSON íŒŒì¼ ì—…ë°ì´íŠ¸: {json_path}")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë°±ì—… ìƒì„±
    backup_path = json_path.replace('.json', '_before_origin_fix.json')
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # evaluation ì„¹ì…˜ êµì²´
    if 'evaluation' in data:
        data['evaluation'] = {
            "overall": {
                "exact_match": evaluation_results["overall"]["exact_match"],
                "f1_score": evaluation_results["overall"]["f1_score"],
                "total_questions": evaluation_results["total_questions"]
            },
            "overall_relaxed": {
                "exact_match": evaluation_results["overall"]["exact_match_relaxed"],
                "f1_score": evaluation_results["overall"]["f1_score_relaxed"],
                "total_questions": evaluation_results["total_questions"]
            },
            "rule_based": {
                "exact_match": evaluation_results["rule_based"]["exact_match"],
                "f1_score": evaluation_results["rule_based"]["f1_score"],
                "question_count": evaluation_results["rule_based"]["question_count"]
            },
            "rule_based_relaxed": {
                "exact_match": evaluation_results["rule_based"]["exact_match_relaxed"],
                "f1_score": evaluation_results["rule_based"]["f1_score_relaxed"],
                "question_count": evaluation_results["rule_based"]["question_count"]
            },
            "enhanced_llm": {
                "ground_truth": {
                    "exact_match": evaluation_results["enhanced_llm_with_agent"]["exact_match"],
                    "f1_score": evaluation_results["enhanced_llm_with_agent"]["f1_score"]
                },
                "explanation": data['evaluation'].get('enhanced_llm', {}).get('explanation', {}),
                "question_count": evaluation_results["enhanced_llm_with_agent"]["question_count"]
            }
        }
    
    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ì €ì¥
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def main():
    base_dir = Path("runs/exp_ab_k10/results")
    dataset_path = "Evaluation/dataset/test_fin.csv"
    
    # ì²˜ë¦¬í•  íŒŒì¼ë“¤
    files_to_process = [
        ("ragA_after_k10.csv", "ragA_k10.json"),
        ("ragB_after_k10.csv", "ragB_k10.json"),
    ]
    
    for csv_file, json_file in files_to_process:
        csv_path = base_dir / csv_file
        json_path = base_dir / json_file
        
        if csv_path.exists() and json_path.exists():
            print(f"\n{'='*70}")
            print(f"ì²˜ë¦¬ ì¤‘: {csv_file} -> {json_file}")
            print(f"{'='*70}")
            
            # 1. Origin ì»¬ëŸ¼ ì¶”ê°€ (ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ ë§¤ì¹­)
            csv_with_origin = add_origin_from_dataset(str(csv_path), dataset_path)
            print(f"âœ… Origin ì»¬ëŸ¼ì´ ì¶”ê°€ëœ íŒŒì¼: {csv_with_origin}")
            
            # 2. Origin ê¸°ë°˜ ì¬í‰ê°€
            evaluation_results = evaluate_with_origin(csv_with_origin)
            
            # 3. ê²°ê³¼ ì¶œë ¥
            print(f"\nì¬í‰ê°€ ê²°ê³¼:")
            print(f"  ì „ì²´ ì§ˆë¬¸: {evaluation_results['total_questions']}ê°œ")
            print(f"  Overall EM: {evaluation_results['overall']['exact_match']:.4f}")
            print(f"  Overall F1: {evaluation_results['overall']['f1_score']:.4f}")
            print(f"  Rule-based ({evaluation_results['rule_based']['question_count']}ê°œ):")
            print(f"    EM: {evaluation_results['rule_based']['exact_match']:.4f}")
            print(f"    F1: {evaluation_results['rule_based']['f1_score']:.4f}")
            print(f"  Enhanced LLM ({evaluation_results['enhanced_llm_with_agent']['question_count']}ê°œ):")
            print(f"    EM: {evaluation_results['enhanced_llm_with_agent']['exact_match']:.4f}")
            print(f"    F1: {evaluation_results['enhanced_llm_with_agent']['f1_score']:.4f}")
            
            # 4. JSON íŒŒì¼ ì—…ë°ì´íŠ¸
            update_json_with_origin_evaluation(str(json_path), evaluation_results)
            print(f"âœ… {json_file} ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        else:
            print(f"âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file} ë˜ëŠ” {json_file}")
    
    print(f"\n{'='*70}")
    print("ğŸ‰ ëª¨ë“  íŒŒì¼ ì¬ì²˜ë¦¬ ì™„ë£Œ!")
    print("ì›ë³¸ ë°ì´í„°ì…‹ì—ì„œ origin ì •ë³´ë¥¼ ê°€ì ¸ì™€ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ë°±ì—… íŒŒì¼ë“¤ì´ _before_origin_fix.jsonìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"{'='*70}")

if __name__ == "__main__":
    main() 