"""
í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ: ì—ì´ì „íŠ¸ + ë¡œì§ ê¸°ë°˜ ê²€ì¦ì˜ ì™„ë²½í•œ ì¡°í•©
ì‹¤ì œ AIê°€ ë¬¸ì œë¥¼ í’€ê³ , BuilderCoreë¡œ ì •ë‹µì„ í™•ì¸í•˜ëŠ” ì´ì¤‘ ê²€ì¦
"""

from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, field
from enum import Enum
import json
import time
from datetime import datetime

from agents.answer_agent import AnswerAgent
from inspectors.evaluation_system import ExactMatchEvaluator, F1ScoreEvaluator
from utils.builder_core import BuilderCore
from utils.llm_adapter import _call_llm_json
from utils.config_manager import get_settings

class ValidationMode(Enum):
    """ê²€ì¦ ëª¨ë“œ"""
    AGENT_ONLY = "agent_only"  # ì—ì´ì „íŠ¸ë§Œ ì‚¬ìš©
    LOGIC_ONLY = "logic_only"  # BuilderCoreë§Œ ì‚¬ìš©
    HYBRID = "hybrid"          # ë‘˜ ë‹¤ ì‚¬ìš© (ì¶”ì²œ!)

class ErrorCategory(Enum):
    """ì˜¤ë¥˜ ì¹´í…Œê³ ë¦¬ - ë” ì„¸ë¶„í™”ëœ ë¶„ë¥˜"""
    # ì§ˆë¬¸ ë¬¸ì œ
    AMBIGUOUS_QUESTION = "ambiguous_question"
    IMPOSSIBLE_QUESTION = "impossible_question"
    
    # ë‹µë³€ ë¬¸ì œ
    WRONG_GROUND_TRUTH = "wrong_ground_truth"
    HALLUCINATION = "hallucination"
    
    # ì¶”ë¡  ë¬¸ì œ
    REASONING_ERROR = "reasoning_error"
    CALCULATION_ERROR = "calculation_error"
    
    # ë°ì´í„° ë¬¸ì œ
    MISSING_DATA = "missing_data"
    INCONSISTENT_DATA = "inconsistent_data"

@dataclass
class ValidationResult:
    """í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ê²°ê³¼"""
    question_id: str
    question: str
    
    # 3ê°€ì§€ ë‹µë³€
    ground_truth: Any  # ë°ì´í„°ì…‹ì˜ ì •ë‹µ
    agent_answer: Any  # ì—ì´ì „íŠ¸ê°€ í‘¼ ë‹µ
    logic_answer: Any  # BuilderCore ê³„ì‚° ë‹µ
    
    # 3ì¤‘ ë¹„êµ ê²°ê³¼
    agent_vs_gt: bool  # ì—ì´ì „íŠ¸ == ì •ë‹µ
    agent_vs_logic: bool  # ì—ì´ì „íŠ¸ == ë¡œì§
    gt_vs_logic: bool  # ì •ë‹µ == ë¡œì§
    
    # ë©”íƒ€ ì •ë³´
    agent_confidence: float = 0.0
    agent_reasoning: List[str] = field(default_factory=list)
    agent_time: float = 0.0
    
    # ì˜¤ë¥˜ ë¶„ì„
    is_valid: bool = False
    error_category: Optional[ErrorCategory] = None
    error_details: Optional[str] = None
    improvement_suggestions: List[str] = field(default_factory=list)
    # ì„¤ëª…(í•´ì„¤) í‰ê°€
    explanation_eval: Optional[Dict[str, Any]] = None
    explanation_quality: Optional[float] = None
    # ë¶„ë¦¬ í†µê³„ë¥¼ ìœ„í•œ ë¼ë²¨
    category: Optional[str] = None
    answer_type: Optional[str] = None
    
    def get_validation_status(self) -> str:
        """ê²€ì¦ ìƒíƒœë¥¼ í•œëˆˆì— íŒŒì•…"""
        if self.agent_vs_gt and self.agent_vs_logic and self.gt_vs_logic:
            return "âœ… PERFECT"  # ëª¨ë‘ ì¼ì¹˜
        elif self.gt_vs_logic:
            return "âš ï¸ AGENT_FAILED"  # ì—ì´ì „íŠ¸ë§Œ í‹€ë¦¼
        elif self.agent_vs_logic:
            return "ğŸ”´ GT_WRONG"  # Ground Truthê°€ í‹€ë¦¼
        elif self.agent_vs_gt:
            return "â“ LOGIC_DIFFERS"  # ë¡œì§ë§Œ ë‹¤ë¦„ (ì´ìƒí•œ ì¼€ì´ìŠ¤)
        else:
            return "âŒ ALL_DIFFERENT"  # ëª¨ë‘ ë‹¤ë¦„

class HybridValidationSystem:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ
    
    ì´ ì‹œìŠ¤í…œì˜ ëª©í‘œ:
    1. ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ í’€ ìˆ˜ ìˆëŠ” ë¬¸ì œì¸ì§€ í™•ì¸
    2. Ground Truthê°€ ì •í™•í•œì§€ BuilderCoreë¡œ ê²€ì¦
    3. í‹€ë¦° ë¶€ë¶„ì„ ì •í™•íˆ íŒŒì•…í•˜ì—¬ ê°œì„ 
    """
    
    def __init__(
        self, 
        network_facts: Dict[str, Any],
        mode: ValidationMode = ValidationMode.HYBRID,
        xml_base_dir: Optional[str] = None,
    ):
        self.network_facts = network_facts
        self.mode = mode
        self.xml_base_dir = xml_base_dir
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.builder_core = BuilderCore(network_facts.get("devices", []))
        
        # ë‹¤ì–‘í•œ ìˆ˜ì¤€ì˜ ì—ì´ì „íŠ¸ ì¤€ë¹„
        self.agents = self._initialize_agents()
        
        # í†µê³„ ì¶”ì 
        self.validation_stats = {
            "total": 0,
            "perfect": 0,
            "agent_failed": 0,
            "gt_wrong": 0,
            "all_different": 0
        }
        # í…ìŠ¤íŠ¸ ë¹„êµ ë³´ì¡°ê¸°
        self._em_eval = ExactMatchEvaluator()
        self._f1_eval = F1ScoreEvaluator()
    
    def _initialize_agents(self) -> Dict[str, Any]:
        """ë‹¤ì–‘í•œ ë‚œì´ë„ì˜ ê²€ì¦ ì—ì´ì „íŠ¸ ì„¤ì • ì´ˆê¸°í™” (ì„¤ì • ê¸°ë°˜ + í´ë°± ì²´ì¸)."""
        settings = get_settings()
        # ëª¨ë¸ í´ë°± ì²´ì¸: answer_synthesis â†’ question_generation â†’ default â†’ enhanced_generation
        chain = [
            settings.models.answer_synthesis,
            settings.models.question_generation,
            settings.models.default,
            settings.models.enhanced_generation,
        ]
        # ì¤‘ë³µ ì œê±°(ìˆœì„œ ìœ ì§€)
        seen = set(); fallback_models = []
        for m in chain:
            if m and m not in seen:
                seen.add(m); fallback_models.append(m)

        return {
            "beginner": {
                "model": fallback_models[0],
                "fallback_models": fallback_models,
                "temperature": 0.3,
                "system_prompt": """ë‹¹ì‹ ì€ ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ ì´ˆê¸‰ìì…ë‹ˆë‹¤.
ê¸°ë³¸ì ì¸ ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì„ ì´í•´í•˜ê³  ê°„ë‹¨í•œ ì§ˆë¬¸ì— ë‹µí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì£¼ì–´ì§„ ë„¤íŠ¸ì›Œí¬ ì„¤ì • ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µí•˜ì„¸ìš”."""
            },
            "intermediate": {
                "model": fallback_models[0],
                "fallback_models": fallback_models,
                "temperature": 0.2,
                "system_prompt": """ë‹¹ì‹ ì€ 3ë…„ ê²½ë ¥ì˜ ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ì…ë‹ˆë‹¤.
ë³µì¡í•œ ì„¤ì •ì„ ë¶„ì„í•˜ê³  ë¬¸ì œë¥¼ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ì£¼ì–´ì§„ ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì„ ì¢…í•©ì ìœ¼ë¡œ ë¶„ì„í•˜ì—¬ ì •í™•í•œ ë‹µë³€ì„ ì œì‹œí•˜ì„¸ìš”."""
            },
            "expert": {
                "model": fallback_models[0],
                "fallback_models": fallback_models,
                "temperature": 0.1,
                "system_prompt": """ë‹¹ì‹ ì€ 10ë…„ ê²½ë ¥ì˜ ì‹œë‹ˆì–´ ë„¤íŠ¸ì›Œí¬ ì•„í‚¤í…íŠ¸ì…ë‹ˆë‹¤.
ê°€ì¥ ë³µì¡í•œ ë„¤íŠ¸ì›Œí¬ êµ¬ì„±ë„ ì™„ë²½íˆ ì´í•´í•˜ê³  ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
ëª¨ë“  ì„¤ì •ì˜ ìƒí˜¸ì‘ìš©ì„ ê³ ë ¤í•˜ì—¬ ì •ë°€í•œ ë‹µë³€ì„ ì œê³µí•˜ì„¸ìš”."""
            }
        }
    
    def validate_single_item(
        self,
        item: Dict[str, Any],
        agent_level: str = "intermediate"
    ) -> ValidationResult:
        """
        ë‹¨ì¼ Q&A í•­ëª©ì„ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ê²€ì¦
        
        3ë‹¨ê³„ í”„ë¡œì„¸ìŠ¤:
        1. ì—ì´ì „íŠ¸ê°€ ë¬¸ì œ í’€ê¸°
        2. BuilderCoreë¡œ ì •ë‹µ ê³„ì‚°
        3. 3ì¤‘ ë¹„êµ ë° ë¶„ì„
        """
        
        question = item.get("question", "")
        ground_truth = item.get("ground_truth")
        question_id = item.get("test_id") or item.get("id") or item.get("question_id") or (item.get("question") and str(abs(hash(item.get("question"))))[:8]) or "unknown"
        
        print(f"\nê²€ì¦ ì¤‘: {question_id}")
        print(f"ì§ˆë¬¸: {question[:100]}...")
        
        # Step 1: ì—ì´ì „íŠ¸ê°€ ë¬¸ì œ í’€ê¸°
        agent_answer = None
        agent_confidence = 0.0
        agent_reasoning = []
        agent_time = 0.0
        
        if self.mode in [ValidationMode.HYBRID, ValidationMode.AGENT_ONLY]:
            agent_result = self._solve_with_agent(
                question=question,
                context=self._get_question_context(item),
                agent_level=agent_level,
                item=item
            )
            agent_answer = agent_result["answer"]
            agent_confidence = agent_result["confidence"]
            agent_reasoning = agent_result["reasoning"]
            agent_time = agent_result["time_taken"]
            
            print(f"  ì—ì´ì „íŠ¸ ë‹µ: {agent_answer} (ì‹ ë¢°ë„: {agent_confidence:.2f})")
        
        # Step 2: BuilderCoreë¡œ ì •ë‹µ ê³„ì‚°
        logic_answer = None
        
        if self.mode in [ValidationMode.HYBRID, ValidationMode.LOGIC_ONLY]:
            try:
                logic_answer = self._compute_with_logic(item)
                print(f"  ë¡œì§ ë‹µ: {logic_answer}")
            except Exception as e:
                print(f"  ë¡œì§ ê³„ì‚° ì‹¤íŒ¨: {e}")
                logic_answer = None
        
        # Step 3: 3ì¤‘ ë¹„êµ
        agent_vs_gt = self._smart_compare(agent_answer, ground_truth, item)
        agent_vs_logic = self._smart_compare(agent_answer, logic_answer, item)
        gt_vs_logic = self._smart_compare(ground_truth, logic_answer, item)
        
        print(f"  Ground Truth: {ground_truth}")
        print(f"  ë¹„êµ ê²°ê³¼: Agent==GT:{agent_vs_gt}, Agent==Logic:{agent_vs_logic}, GT==Logic:{gt_vs_logic}")
        
        # Step 4: ê²€ì¦ ê²°ê³¼ ìƒì„±
        result = ValidationResult(
            question_id=question_id,
            question=question,
            ground_truth=ground_truth,
            agent_answer=agent_answer,
            logic_answer=logic_answer,
            agent_vs_gt=agent_vs_gt,
            agent_vs_logic=agent_vs_logic,
            gt_vs_logic=gt_vs_logic,
            agent_confidence=agent_confidence,
            agent_reasoning=agent_reasoning,
            agent_time=agent_time,
            category=item.get("category"),
            answer_type=item.get("answer_type")
        )
        
        # Step 5: ì˜¤ë¥˜ ë¶„ì„
        self._analyze_validation_result(result, item)
        
        # í†µê³„ ì—…ë°ì´íŠ¸
        self._update_statistics(result)
        
        print(f"  ìƒíƒœ: {result.get_validation_status()}")
        # ì„¤ëª… í’ˆì§ˆ ì¸¡ì • (ENHANCED/long ìœ„ì£¼)
        try:
            exp_eval = self._evaluate_explanation(item, agent_answer, logic_answer)
            result.explanation_eval = exp_eval
            result.explanation_quality = exp_eval.get("quality")
            if result.explanation_quality is not None:
                print(f"  ì„¤ëª… í’ˆì§ˆ: {result.explanation_quality:.2f}")
        except Exception:
            pass
        
        return result
    
    def _solve_with_agent(
        self,
        question: str,
        context: str,
        agent_level: str = "intermediate",
        item: Optional[Dict[str, Any]] = None,
    ) -> Dict[str, Any]:
        """ì—ì´ì „íŠ¸ê°€ ì‹¤ì œë¡œ ë¬¸ì œë¥¼ í’‰ë‹ˆë‹¤"""
        
        # ì—ì´ì „íŠ¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        agent_config = self.agents.get(agent_level, self.agents["intermediate"])
        system_prompt = agent_config["system_prompt"]
        temperature = agent_config["temperature"]
        model_chain = agent_config.get("fallback_models") or [agent_config.get("model")]
        
        prompt = f"""
ë„¤íŠ¸ì›Œí¬ ì„¤ì • ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.
ë‹¨ê³„ë³„ë¡œ ì¶”ë¡  ê³¼ì •ì„ ì„¤ëª…í•˜ê³ , ìµœì¢… ë‹µë³€ì„ ì œì‹œí•˜ì„¸ìš”.
"""
        
        schema = {
            "type": "object",
            "properties": {
                "reasoning": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ë‹¨ê³„ë³„ ì¶”ë¡  ê³¼ì •"
                },
                "calculations": {
                    "type": "object",
                    "description": "ìˆ˜í–‰í•œ ê³„ì‚°ë“¤"
                },
                "answer": {
                    "description": "ìµœì¢… ë‹µë³€"
                },
                "confidence": {
                    "type": "number",
                    "minimum": 0,
                    "maximum": 1,
                    "description": "ë‹µë³€ í™•ì‹ ë„"
                },
                "data_sources": {
                    "type": "array",
                    "items": {"type": "string"},
                    "description": "ì°¸ì¡°í•œ ë°ì´í„°"
                }
            },
            "required": ["reasoning", "answer", "confidence"]
        }
        
        start_time = time.time()
        
        # ë‹¤ì¤‘ ëª¨ë¸ í´ë°± ì‹œë„
        last_err = None
        for m in model_chain:
            try:
                response = _call_llm_json(
                    messages=[
                        {"role": "system", "content": system_prompt},
                        {"role": "user", "content": prompt}
                    ],
                    schema=schema,
                    temperature=temperature,
                    model=m,
                    max_output_tokens=8000
                )
                ans = response.get("answer") if isinstance(response, dict) else None
                if ans is None or (isinstance(ans, str) and not ans.strip()):
                    raise ValueError("empty agent answer")
                return {
                    "answer": ans,
                    "confidence": response.get("confidence", 0.6),
                    "reasoning": response.get("reasoning", []),
                    "time_taken": time.time() - start_time,
                    "success": True
                }
            except Exception as e:
                last_err = e
                continue
        # ì „ë¶€ ì‹¤íŒ¨
        return {
            "answer": None,
            "confidence": 0.0,
            "reasoning": [f"Agent LLM failed: {str(last_err) if last_err else 'unknown'}"],
            "time_taken": time.time() - start_time,
            "success": False
        }
    
    def _compute_with_logic(self, item: Dict[str, Any]) -> Any:
        """BuilderCoreë¡œ ì •ë‹µì„ ê³„ì‚°í•©ë‹ˆë‹¤"""
        # Intent ì¶”ì¶œ (ë©”íƒ€ë°ì´í„° í¬í•¨ ê²€ìƒ‰)
        intent = item.get("intent")
        if not intent:
            intent = (item.get("metadata") or {}).get("intent")
        if not intent:
            # ì§ˆë¬¸ì—ì„œ intent ì¶”ë¡ 
            intent = self._infer_intent_from_question(item.get("question", ""))

        # Commandí˜• ì˜ë„ëŠ” ë¡œì§ ê³„ì‚° ë¶ˆê°€ â†’ GTë¡œ ëŒ€ì²´ ë¹„êµ
        if isinstance(intent, dict) and "command" in intent:
            return item.get("ground_truth")

        # BuilderCore ì‹¤í–‰ (compute ë˜ëŠ” calculate_metric ê²½ë¡œ)
        if isinstance(intent, dict) and intent.get("metric"):
            try:
                val, _files = self.builder_core.calculate_metric(intent["metric"], intent.get("params") or intent.get("scope") or {})
                return val
            except Exception:
                pass
        result = self.builder_core.compute(intent, self.network_facts)
        if result.get("answer_type") == "error":
            raise ValueError(f"BuilderCore error: {result.get('value')}")
        return result.get("value")
    
    def _compare_answers(self, answer1: Any, answer2: Any) -> bool:
        """ë‘ ë‹µë³€ì„ ë¹„êµí•©ë‹ˆë‹¤ (íƒ€ì…ë³„ ìœ ì—°í•œ ë¹„êµ)"""
        
        if answer1 is None or answer2 is None:
            return answer1 == answer2
        
        # ìˆ«ì ë¹„êµ (5% ì˜¤ì°¨ í—ˆìš©)
        try:
            num1 = float(answer1) if not isinstance(answer1, (int, float)) else answer1
            num2 = float(answer2) if not isinstance(answer2, (int, float)) else answer2
            if abs(num1 - num2) <= abs(num2) * 0.05:
                return True
        except (ValueError, TypeError):
            pass
        
        # ë¶ˆë¦° ë¹„êµ
        if isinstance(answer1, bool) or isinstance(answer2, bool):
            bool_map = {
                True: ["true", "yes", "í™œì„±", "enabled", "up", True],
                False: ["false", "no", "ë¹„í™œì„±", "disabled", "down", False]
            }
            
            def to_bool(val):
                if isinstance(val, bool):
                    return val
                val_lower = str(val).lower()
                for bool_val, keywords in bool_map.items():
                    if val_lower in [str(k).lower() for k in keywords]:
                        return bool_val
                return None
            
            bool1 = to_bool(answer1)
            bool2 = to_bool(answer2)
            if bool1 is not None and bool2 is not None:
                return bool1 == bool2
        
        # ë¦¬ìŠ¤íŠ¸/ì§‘í•© ë¹„êµ
        if isinstance(answer1, list) and isinstance(answer2, list):
            return set(map(str, answer1)) == set(map(str, answer2))
        
        # ë¬¸ìì—´ ë¹„êµ (ì •ê·œí™”)
        str1 = str(answer1).strip().lower()
        str2 = str(answer2).strip().lower()
        
        return str1 == str2

    def _smart_compare(self, a: Any, b: Any, item: Optional[Dict[str, Any]]) -> bool:
        """ë‹µë³€ íƒ€ì…/ê¸¸ì´ì— ë”°ë¼ ì ì ˆí•œ ë¹„êµ ë°©ì‹ì„ ì„ íƒ"""
        # ìš°ì„  ê¸°ì¡´ íƒ€ì…ë³„ ë¹„êµ ì‹œë„
        if self._compare_answers(a, b):
            return True
        if a is None or b is None:
            return a == b
        try:
            ans_type = (item.get("answer_type") or "").lower() if isinstance(item, dict) else ""
        except Exception:
            ans_type = ""
        sa = str(a); sb = str(b)
        # ê¸´ ì„œìˆ í˜• ë˜ëŠ” advanced ì¹´í…Œê³ ë¦¬ â†’ F1 ê¸°ì¤€ í—ˆìš©
        is_long = (ans_type == "long") or (len(sa) > 80 or len(sb) > 80) or ((item or {}).get("category") == "advanced")
        if is_long:
            try:
                em = self._em_eval.evaluate(sa, sb)
                if em == 1.0:
                    return True
                f1 = self._f1_eval.evaluate(sa, sb)
                return f1 >= 0.7
            except Exception:
                return False
        return False
    
    def _analyze_validation_result(
        self,
        result: ValidationResult,
        item: Dict[str, Any]
    ) -> None:
        """ê²€ì¦ ê²°ê³¼ë¥¼ ë¶„ì„í•˜ê³  ì˜¤ë¥˜ ì¹´í…Œê³ ë¦¬ë¥¼ ê²°ì •í•©ë‹ˆë‹¤"""
        
        status = result.get_validation_status()
        
        if status == "âœ… PERFECT":
            result.is_valid = True
            result.error_category = None
            
        elif status == "ğŸ”´ GT_WRONG":
            result.is_valid = False
            result.error_category = ErrorCategory.WRONG_GROUND_TRUTH
            result.error_details = f"Ground Truth({result.ground_truth})ê°€ ì‹¤ì œ ë‹µ({result.logic_answer})ê³¼ ë‹¤ë¦…ë‹ˆë‹¤"
            result.improvement_suggestions = [
                f"Ground Truthë¥¼ {result.logic_answer}ë¡œ ìˆ˜ì •",
                "ë°ì´í„° ìƒì„± í”„ë¡œì„¸ìŠ¤ ì¬ê²€í† "
            ]
            
        elif status == "âš ï¸ AGENT_FAILED":
            result.is_valid = True  # GTëŠ” ë§ìŒ
            result.error_category = ErrorCategory.REASONING_ERROR
            
            # ì—ì´ì „íŠ¸ê°€ ì™œ í‹€ë ¸ëŠ”ì§€ ë¶„ì„
            if result.agent_confidence < 0.3:
                result.error_details = "ì—ì´ì „íŠ¸ê°€ ë‹µë³€ì— í™•ì‹ ì´ ì—†ìŒ"
                result.improvement_suggestions = [
                    "ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê²Œ ìˆ˜ì •",
                    "í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€"
                ]
            else:
                result.error_details = "ì—ì´ì „íŠ¸ì˜ ì¶”ë¡  ê³¼ì •ì— ì˜¤ë¥˜"
                result.improvement_suggestions = [
                    "ì¶”ë¡  ë‹¨ê³„ë¥¼ ë” ëª…ì‹œì ìœ¼ë¡œ ìœ ë„",
                    "ì—ì´ì „íŠ¸ í”„ë¡¬í”„íŠ¸ ê°œì„ "
                ]
                
        elif status == "âŒ ALL_DIFFERENT":
            result.is_valid = False
            result.error_category = ErrorCategory.AMBIGUOUS_QUESTION
            result.error_details = "ëª¨ë“  ë‹µë³€ì´ ë‹¤ë¦„ - ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ê±°ë‚˜ ë°ì´í„°ê°€ ë¶ˆì¶©ë¶„"
            result.improvement_suggestions = [
                "ì§ˆë¬¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±",
                "ì •ë‹µì„ BuilderCore ê²°ê³¼ë¡œ êµì²´",
                "í•„ìš”í•œ ë°ì´í„° í™•ì¸"
            ]
    
    def validate_dataset(
        self,
        dataset: List[Dict[str, Any]],
        sample_size: Optional[int] = None,
        parallel: bool = False
    ) -> Tuple[List[ValidationResult], Dict[str, Any]]:
        """
        ì „ì²´ ë°ì´í„°ì…‹ì„ í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ìœ¼ë¡œ ê²€ì¦í•©ë‹ˆë‹¤
        """
        
        print("\n" + "="*60)
        print("í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œì‘")
        print("="*60)
        
        # ìƒ˜í”Œë§
        if sample_size and len(dataset) > sample_size:
            import random
            dataset = random.sample(dataset, sample_size)
            print(f"ìƒ˜í”Œë§: {sample_size}ê°œ í•­ëª©")
        
        results = []
        
        # ë³µì¡ë„ë³„ë¡œ ì—ì´ì „íŠ¸ ë ˆë²¨ ê²°ì •
        def get_agent_level(item):
            complexity = item.get("complexity", "medium")
            if complexity in ["basic", "simple"]:
                return "beginner"
            elif complexity in ["complex", "advanced", "synthetic"]:
                return "expert"
            else:
                return "intermediate"
        
        # ê²€ì¦ ìˆ˜í–‰
        for i, item in enumerate(dataset, 1):
            print(f"\nì§„í–‰ë¥ : {i}/{len(dataset)}")
            
            agent_level = get_agent_level(item)
            result = self.validate_single_item(item, agent_level)
            results.append(result)
            
            # ì¤‘ê°„ í†µê³„ ì¶œë ¥ (5ê°œë§ˆë‹¤)
            if i % 5 == 0:
                self._print_intermediate_stats()
        
        # ìµœì¢… ë¶„ì„
        final_stats = self._generate_final_report(results)
        
        return results, final_stats
    
    def _generate_final_report(
        self, 
        results: List[ValidationResult]
    ) -> Dict[str, Any]:
        """ìµœì¢… ê²€ì¦ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        total = len(results)
        
        # ìƒíƒœë³„ ì§‘ê³„
        status_counts = {}
        for result in results:
            status = result.get_validation_status()
            status_counts[status] = status_counts.get(status, 0) + 1
        
        # ì˜¤ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
        error_counts = {}
        for result in results:
            if result.error_category:
                cat = result.error_category.value
                error_counts[cat] = error_counts.get(cat, 0) + 1
        
        # ì—ì´ì „íŠ¸ ì„±ëŠ¥ ë¶„ì„
        agent_correct = sum(1 for r in results if r.agent_vs_gt)
        agent_accuracy = agent_correct / total if total > 0 else 0
        
        avg_confidence = sum(r.agent_confidence for r in results) / total if total > 0 else 0
        avg_time = sum(r.agent_time for r in results) / total if total > 0 else 0
        
        # Ground Truth ì •í™•ë„
        gt_correct = sum(1 for r in results if r.gt_vs_logic)
        gt_accuracy = gt_correct / total if total > 0 else 0
        
        # BASIC/ENHANCED ë¶„ë¦¬ í†µê³„
        def _acc(lst, key):
            return (sum(1 for r in lst if getattr(r, key)) / len(lst)) if lst else 0.0
        basic = [r for r in results if (r.category or "").lower() == "basic"]
        enhanced = [r for r in results if (r.category or "").lower() == "advanced"]
        enhanced_exp = [r for r in enhanced if r.explanation_quality is not None]
        enhanced_exp_avg = (sum(r.explanation_quality for r in enhanced_exp) / len(enhanced_exp)) if enhanced_exp else 0.0

        # ENHANCED ì „ìš© ë¦¬í¬íŠ¸: ë‚®ì€ ì„¤ëª… í’ˆì§ˆ ì‚¬ë¡€
        low_examples = []
        for r in enhanced:
            if r.explanation_quality is not None and r.explanation_quality < 0.4:
                low_examples.append({
                    "question_id": r.question_id,
                    "quality": r.explanation_quality,
                })
                if len(low_examples) >= 10:
                    break

        report = {
            "summary": {
                "total_items": total,
                "validation_mode": self.mode.value,
                "timestamp": datetime.now().isoformat()
            },
            "status_distribution": status_counts,
            "error_categories": error_counts,
            "agent_performance": {
                "accuracy": agent_accuracy,
                "average_confidence": avg_confidence,
                "average_time_seconds": avg_time,
                "correct_answers": agent_correct
            },
            "ground_truth_quality": {
                "accuracy": gt_accuracy,
                "correct_items": gt_correct,
                "incorrect_items": total - gt_correct
            },
            "type_breakdown": {
                "basic": {
                    "count": len(basic),
                    "agent_vs_gt_accuracy": _acc(basic, "agent_vs_gt"),
                    "gt_vs_logic_accuracy": _acc(basic, "gt_vs_logic"),
                },
                "enhanced": {
                    "count": len(enhanced),
                    "agent_vs_gt_accuracy": _acc(enhanced, "agent_vs_gt"),
                    "gt_vs_logic_accuracy": _acc(enhanced, "gt_vs_logic"),
                    "explanation_quality_avg": enhanced_exp_avg,
                }
            },
            "enhanced_summary": {
                "explanation_quality_avg": enhanced_exp_avg,
                "low_quality_examples": low_examples,
            },
            "recommendations": self._generate_recommendations(results)
        }
        
        # ì½˜ì†” ì¶œë ¥
        self._print_final_report(report)
        
        return report
    
    def _generate_recommendations(
        self, 
        results: List[ValidationResult]
    ) -> List[str]:
        """ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê°œì„  ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        
        recommendations = []
        
        # Ground Truth ì •í™•ë„ ì²´í¬
        gt_errors = sum(1 for r in results if r.error_category == ErrorCategory.WRONG_GROUND_TRUTH)
        if gt_errors > len(results) * 0.1:  # 10% ì´ìƒ
            recommendations.append(
                f"âš ï¸ {gt_errors}ê°œ({gt_errors/len(results)*100:.1f}%)ì˜ Ground Truthê°€ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. "
                "ë°ì´í„° ìƒì„± í”„ë¡œì„¸ìŠ¤ë¥¼ ì¬ê²€í† í•˜ì„¸ìš”."
            )
        
        # ì—ì´ì „íŠ¸ ì„±ëŠ¥ ì²´í¬
        agent_failures = sum(1 for r in results if not r.agent_vs_gt)
        if agent_failures > len(results) * 0.3:  # 30% ì´ìƒ
            recommendations.append(
                f"ğŸ“ ì—ì´ì „íŠ¸ê°€ {agent_failures}ê°œ({agent_failures/len(results)*100:.1f}%) ë¬¸ì œë¥¼ í‹€ë ¸ìŠµë‹ˆë‹¤. "
                "ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê²Œ ë§Œë“¤ê±°ë‚˜ ë‚œì´ë„ë¥¼ ì¡°ì •í•˜ì„¸ìš”."
            )
        
        # ëª¨í˜¸í•œ ì§ˆë¬¸ ì²´í¬
        ambiguous = sum(1 for r in results if r.error_category == ErrorCategory.AMBIGUOUS_QUESTION)
        if ambiguous > 0:
            recommendations.append(
                f"â“ {ambiguous}ê°œì˜ ëª¨í˜¸í•œ ì§ˆë¬¸ì´ ë°œê²¬ë˜ì—ˆìŠµë‹ˆë‹¤. "
                "ì§ˆë¬¸ì„ êµ¬ì²´ì ìœ¼ë¡œ ì¬ì‘ì„±í•˜ì„¸ìš”."
            )
        
        if not recommendations:
            recommendations.append("âœ… ë°ì´í„°ì…‹ í’ˆì§ˆì´ ì–‘í˜¸í•©ë‹ˆë‹¤!")
        
        return recommendations
    
    def _print_final_report(self, report: Dict[str, Any]) -> None:
        """ì½˜ì†”ì— ë³´ê¸° ì¢‹ê²Œ ë¦¬í¬íŠ¸ ì¶œë ¥"""
        
        print("\n" + "="*60)

    def _evaluate_explanation(self, item: Dict[str, Any], agent_answer: Any, logic_answer: Any) -> Dict[str, Any]:
        """ENHANCED ì§ˆë¬¸ ì¤‘ì‹¬ìœ¼ë¡œ explanation í…ìŠ¤íŠ¸ì˜ í’ˆì§ˆì„ í‰ê°€.
        - GT ë° Logic ê°’ê³¼ì˜ ì •í•©ì„±(EM/F1)
        - ê¸¸ì´/ì¡´ì¬ ì—¬ë¶€ ì²´í¬
        ë°˜í™˜: {quality: float, em_gt, f1_gt, em_logic, f1_logic, length}
        """
        explanation = item.get("explanation") or (item.get("metadata") or {}).get("explanation")
        if not isinstance(explanation, str) or not explanation.strip():
            return {"quality": 0.0, "length": 0, "reason": "empty_explanation"}

        gt = item.get("ground_truth")
        txt = explanation.strip()
        s_gt = "" if gt is None else str(gt)
        s_logic = "" if logic_answer is None else str(logic_answer)

        try:
            em_gt = self._em_eval.evaluate(txt, s_gt) if s_gt else 0.0
            f1_gt = self._f1_eval.evaluate(txt, s_gt) if s_gt else 0.0
        except Exception:
            em_gt, f1_gt = 0.0, 0.0

        try:
            em_logic = self._em_eval.evaluate(txt, s_logic) if s_logic else 0.0
            f1_logic = self._f1_eval.evaluate(txt, s_logic) if s_logic else 0.0
        except Exception:
            em_logic, f1_logic = 0.0, 0.0

        # í’ˆì§ˆ ìŠ¤ì½”ì–´: GTì™€ì˜ ì¼ì¹˜ 60%, Logicì™€ì˜ ì¼ì¹˜ 40% ê°€ì¤‘ í‰ê· (ê°€ìš© ì‹œ)
        parts = []
        if s_gt:
            parts.append(max(em_gt, f1_gt))
        if s_logic:
            parts.append(max(em_logic, f1_logic))
        quality = sum(parts) / len(parts) if parts else 0.0

        return {
            "quality": float(quality),
            "em_gt": float(em_gt),
            "f1_gt": float(f1_gt),
            "em_logic": float(em_logic),
            "f1_logic": float(f1_logic),
            "length": len(txt),
        }
        print("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ë¦¬í¬íŠ¸")
        print("="*60)
        
        print(f"\nì´ ê²€ì¦ í•­ëª©: {report['summary']['total_items']}ê°œ")
        
        print("\nìƒíƒœë³„ ë¶„í¬:")
        for status, count in report['status_distribution'].items():
            percentage = count / report['summary']['total_items'] * 100
            print(f"  {status}: {count}ê°œ ({percentage:.1f}%)")
        
        if report['error_categories']:
            print("\nì˜¤ë¥˜ ìœ í˜•:")
            for category, count in report['error_categories'].items():
                print(f"  - {category}: {count}ê°œ")
        
        print(f"\nì—ì´ì „íŠ¸ ì„±ëŠ¥:")
        perf = report['agent_performance']
        print(f"  - ì •í™•ë„: {perf['accuracy']:.1%}")
        print(f"  - í‰ê·  í™•ì‹ ë„: {perf['average_confidence']:.2f}")
        print(f"  - í‰ê·  ì‘ë‹µ ì‹œê°„: {perf['average_time_seconds']:.2f}ì´ˆ")
        
        print(f"\nGround Truth í’ˆì§ˆ:")
        gt = report['ground_truth_quality']
        print(f"  - ì •í™•ë„: {gt['accuracy']:.1%}")
        print(f"  - ì˜¬ë°”ë¥¸ í•­ëª©: {gt['correct_items']}ê°œ")
        print(f"  - ì˜ëª»ëœ í•­ëª©: {gt['incorrect_items']}ê°œ")
        
        print("\nğŸ“Œ ê¶Œì¥ì‚¬í•­:")
        for i, rec in enumerate(report['recommendations'], 1):
            print(f"{i}. {rec}")
        
        print("\n" + "="*60)
    
    def _get_question_context(self, item: Dict[str, Any]) -> str:
        """ì§ˆë¬¸ì— í•„ìš”í•œ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        
        # ê¸°ë³¸ ë„¤íŠ¸ì›Œí¬ ì •ë³´
        devices = self.network_facts.get("devices", [])
        context_parts = [
            f"ë„¤íŠ¸ì›Œí¬ì—ëŠ” ì´ {len(devices)}ê°œì˜ ì¥ë¹„ê°€ ìˆìŠµë‹ˆë‹¤:",
            ""
        ]
        
        # ì¥ë¹„ë³„ ê¸°ë³¸ ì •ë³´
        for device in devices[:5]:  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
            sys = device.get("system", {}) or {}
            device_name = sys.get("hostname") or device.get("name") or device.get("file") or "Unknown"
            device_type = device.get("vendor") or device.get("os") or "Unknown"
            context_parts.append(f"- {device_name} ({device_type})")
        
        if len(devices) > 5:
            context_parts.append(f"- ... ì™¸ {len(devices) - 5}ê°œ ì¥ë¹„")
        
        # ì†ŒìŠ¤ íŒŒì¼ ì •ë³´
        source_files = item.get("source_files", [])
        if source_files:
            context_parts.extend([
                "",
                "ê´€ë ¨ ì„¤ì • íŒŒì¼:",
                *[f"- {f}" for f in source_files[:3]]  # ìµœëŒ€ 3ê°œë§Œ
            ])

        # XML ìŠ¤ë‹ˆí«/ì¦ê±° í¬í•¨ (ê°€ëŠ¥ ì‹œ)
        snippets = item.get("evidence_snippets") or []
        if not snippets:
            meta = item.get("metadata") or {}
            ev = meta.get("evidence")
            if isinstance(ev, (list, tuple)):
                snippets = [{"snippet": str(x)} for x in ev[:3]]
        # xml_base_dirê°€ ìˆê³  source_filesê°€ ìˆìœ¼ë©´ ì§ì ‘ ì¼ë¶€ ë¼ì¸ ì¶”ì¶œ (ê°œì„ ëœ ìŠ¤ë‹ˆí«: ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        if not snippets and self.xml_base_dir and source_files:
            try:
                import os
                keywords = ["bgp", "vrf", "ssh", "ospf", "neighbor", "route-target", "aaa"]
                def _snippet_lines(lines: List[str], needle_list: List[str], window: int = 1) -> List[str]:
                    hits=[]
                    for i, raw in enumerate(lines):
                        low = raw.lower()
                        if any(n in low for n in needle_list):
                            s=max(0,i-window); e=min(len(lines), i+window+1)
                            snip="\n".join([ln.rstrip() for ln in lines[s:e]]).strip()
                            if snip and snip not in hits:
                                hits.append(snip)
                        if len(hits)>=3:
                            break
                    return hits
                found_snips = []
                for f in source_files[:3]:
                    path = os.path.join(self.xml_base_dir, f)
                    if not os.path.exists(path):
                        continue
                    with open(path, 'r', encoding='utf-8', errors='ignore') as fh:
                        lines = fh.readlines()
                    for sn in _snippet_lines(lines, keywords, window=1):
                        found_snips.append({"snippet": sn})
                        if len(found_snips) >= 3:
                            break
                    if len(found_snips) >= 3:
                        break
                if found_snips:
                    snippets = found_snips
            except Exception:
                pass
        if snippets:
            context_parts.extend(["", "ì¦ê±° ìŠ¤ë‹ˆí«:"])
            for s in snippets[:3]:
                sn = s.get("snippet") if isinstance(s, dict) else str(s)
                if sn:
                    context_parts.append(f"â€¢ {sn[:240]}")

        return "\n".join(context_parts)
    
    def _infer_intent_from_question(self, question: str) -> Dict[str, Any]:
        """ì§ˆë¬¸ì—ì„œ intentë¥¼ ì¶”ë¡ í•©ë‹ˆë‹¤"""
        
        question_lower = question.lower()
        
        # ê°„ë‹¨í•œ í‚¤ì›Œë“œ ê¸°ë°˜ ë§¤í•‘ (ì •ë°€)
        if ("full-mesh" in question_lower) or ("fullmesh" in question_lower) or ("í’€ë©”ì‹œ" in question_lower):
            return {"metric": "ibgp_fullmesh_ok", "type": "boolean", "params": {}}
        if ("ëˆ„ë½" in question_lower) or ("missing" in question_lower):
            if "bgp" in question_lower or "ibgp" in question_lower:
                return {"metric": "ibgp_missing_pairs_count", "type": "numeric", "params": {}}
        if any(keyword in question_lower for keyword in ["bgp", "neighbor", "ì´ì›ƒ"]):
            return {"metric": "bgp_neighbor_count", "type": "numeric", "params": {}}
        elif any(keyword in question_lower for keyword in ["ssh", "ë³´ì•ˆ"]):
            if any(k in question_lower for k in ["ë¶ˆê°€ëŠ¥", "ë¶ˆê°€", "ë¯¸ì„¤ì •", "ì—†ëŠ”", "ë¹„í™œì„±"]):
                return {"metric": "ssh_missing_count", "type": "numeric", "params": {}}
            return {"metric": "ssh_present_bool", "type": "boolean", "params": {}}
        elif any(keyword in question_lower for keyword in ["vrf", "route-target"]):
            return {"metric": "vrf_without_rt_count", "type": "numeric", "params": {}}
        elif any(keyword in question_lower for keyword in ["ospf", "area"]):
            return {"metric": "ospf_area0_if_count", "type": "numeric", "params": {}}
        elif any(keyword in question_lower for keyword in ["interface", "ì¸í„°í˜ì´ìŠ¤"]):
            if "vrf" in question_lower:
                return {"metric": "vrf_interface_bind_count", "type": "numeric", "params": {}}
            return {"metric": "interface_count", "type": "numeric", "params": {}}
        else:
            # ê¸°ë³¸ê°’
            return {"metric": "GENERAL_INFO", "type": "text"}
    
    def _update_statistics(self, result: ValidationResult) -> None:
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        self.validation_stats["total"] += 1
        
        status = result.get_validation_status()
        if status == "âœ… PERFECT":
            self.validation_stats["perfect"] += 1
        elif status == "âš ï¸ AGENT_FAILED":
            self.validation_stats["agent_failed"] += 1
        elif status == "ğŸ”´ GT_WRONG":
            self.validation_stats["gt_wrong"] += 1
        elif status == "âŒ ALL_DIFFERENT":
            self.validation_stats["all_different"] += 1
    
    def _print_intermediate_stats(self) -> None:
        """ì¤‘ê°„ í†µê³„ ì¶œë ¥"""
        stats = self.validation_stats
        total = stats["total"]
        
        if total == 0:
            return
        
        print(f"\nğŸ“Š ì¤‘ê°„ í†µê³„ (ì´ {total}ê°œ):")
        print(f"  âœ… ì™„ë²½: {stats['perfect']}ê°œ ({stats['perfect']/total*100:.1f}%)")
        print(f"  âš ï¸ ì—ì´ì „íŠ¸ ì‹¤íŒ¨: {stats['agent_failed']}ê°œ ({stats['agent_failed']/total*100:.1f}%)")
        print(f"  ğŸ”´ GT ì˜¤ë¥˜: {stats['gt_wrong']}ê°œ ({stats['gt_wrong']/total*100:.1f}%)")
        print(f"  âŒ ëª¨ë‘ ë‹¤ë¦„: {stats['all_different']}ê°œ ({stats['all_different']/total*100:.1f}%)")
