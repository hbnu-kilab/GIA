"""
í•˜ì´ë¸Œë¦¬ë“œ í”¼ë“œë°± ë£¨í”„: ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ì…‹ ê°œì„ 
ì—ì´ì „íŠ¸ì™€ ë¡œì§ ê²€ì¦ ê²°ê³¼ë¥¼ ëª¨ë‘ í™œìš©í•˜ì—¬ ìµœì ì˜ ê°œì„ ì•ˆ ë„ì¶œ
"""

from typing import Dict, Any, List, Tuple, Optional
from dataclasses import dataclass
import json

from agents.hybrid_validation_system import ValidationResult, ErrorCategory
from utils.llm_adapter import _call_llm_json
from utils.builder_core import BuilderCore

@dataclass
class ImprovementAction:
    """ê°œì„  ì•¡ì…˜"""
    action_type: str  # "fix_answer", "clarify_question", "add_context", "remove"
    original_item: Dict[str, Any]
    improved_item: Optional[Dict[str, Any]]
    reason: str
    confidence: float

class HybridFeedbackLoop:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ê°œì„ í•˜ëŠ” ì‹œìŠ¤í…œ
    """
    
    def __init__(self, network_facts: Dict[str, Any]):
        self.network_facts = network_facts
        self.builder_core = BuilderCore(network_facts.get("devices", []))
        self.improvement_stats = {
            "fixed_answers": 0,
            "clarified_questions": 0,
            "added_context": 0,
            "removed_items": 0
        }
    
    def improve_dataset(
        self,
        validation_results: List[ValidationResult],
        original_dataset: List[Dict[str, Any]]
    ) -> Tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """
        ê²€ì¦ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë°ì´í„°ì…‹ì„ ê°œì„ í•©ë‹ˆë‹¤
        
        ê°œì„  ì „ëµ:
        1. GT_WRONG â†’ ì •ë‹µì„ BuilderCore ê°’ìœ¼ë¡œ ìˆ˜ì •
        2. AGENT_FAILED â†’ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê²Œ
        3. ALL_DIFFERENT â†’ ì „ë©´ ì¬ê²€í† 
        4. PERFECT â†’ ê·¸ëŒ€ë¡œ ìœ ì§€
        """
        
        print("\n" + "="*60)
        print("ğŸ”§ í•˜ì´ë¸Œë¦¬ë“œ í”¼ë“œë°± ë£¨í”„ ì‹œì‘")
        print("="*60)
        
        # ì›ë³¸ ë°ì´í„°ì…‹ ì¸ë±ìŠ¤ êµ¬ì¶• (id / test_id ëª¨ë‘ ì§€ì›)
        try:
            self._original_map = {}
            for it in original_dataset:
                key = it.get("test_id") or it.get("id")
                if key:
                    self._original_map[str(key)] = it
        except Exception:
            self._original_map = {}

        # ê²°ê³¼ë¥¼ ìƒíƒœë³„ë¡œ ë¶„ë¥˜
        categorized = self._categorize_results(validation_results)
        
        # ê°œì„  ì•¡ì…˜ ìƒì„±
        improvement_actions = []
        
        # 1. Ground Truthê°€ í‹€ë¦° ê²½ìš° - ì¦‰ì‹œ ìˆ˜ì •
        if categorized["gt_wrong"]:
            print(f"\nìˆ˜ì • ì¤‘: {len(categorized['gt_wrong'])}ê°œì˜ ì˜ëª»ëœ ì •ë‹µ")
            actions = self._fix_wrong_ground_truths(categorized["gt_wrong"])
            improvement_actions.extend(actions)
        
        # 2. ì—ì´ì „íŠ¸ë§Œ í‹€ë¦° ê²½ìš° - ì§ˆë¬¸ ê°œì„ 
        if categorized["agent_failed"]:
            print(f"\nê°œì„  ì¤‘: {len(categorized['agent_failed'])}ê°œì˜ ì–´ë ¤ìš´ ì§ˆë¬¸")
            actions = self._improve_difficult_questions(categorized["agent_failed"])
            improvement_actions.extend(actions)
        
        # 3. ëª¨ë‘ ë‹¤ë¥¸ ê²½ìš° - ì „ë©´ ì¬ê²€í† 
        if categorized["all_different"]:
            print(f"\nì¬ê²€í†  ì¤‘: {len(categorized['all_different'])}ê°œì˜ ë¬¸ì œ í•­ëª©")
            actions = self._handle_problematic_items(categorized["all_different"])
            improvement_actions.extend(actions)
        
        # ê°œì„ ëœ ë°ì´í„°ì…‹ ìƒì„±
        improved_dataset = self._apply_improvements(
            original_dataset,
            improvement_actions
        )
        
        # ê°œì„  í†µê³„ (ê³„ì†)
        improvement_report = {
           "total_improvements": len(improvement_actions),
           "improvement_stats": self.improvement_stats,
           "success_rate": self._calculate_improvement_success_rate(improvement_actions),
           "actions_by_type": self._count_actions_by_type(improvement_actions)
       }
       
        self._print_improvement_report(improvement_report)
       
        return improved_dataset, improvement_report

    def _find_original_item(self, question_id: str) -> Optional[Dict[str, Any]]:
        """ê²€ì¦ ê²°ê³¼ì˜ question_idë¡œ ì›ë³¸ í•­ëª©ì„ ì¡°íšŒ"""
        if not hasattr(self, "_original_map"):
            return None
        return self._original_map.get(str(question_id))

    def _mark_for_removal(self, result: ValidationResult) -> Optional[ImprovementAction]:
        """ì¬ìƒì„± ë¶ˆê°€ í•­ëª©ì„ ì œê±° ëŒ€ìƒìœ¼ë¡œ í‘œì‹œ"""
        original_item = self._find_original_item(result.question_id)
        if not original_item:
            return None
        action = ImprovementAction(
            action_type="remove",
            original_item=original_item,
            improved_item=None,
            reason="ëª¨í˜¸/ë¶ˆê°€í•´ í•­ëª© ì œê±°",
            confidence=0.6,
        )
        self.improvement_stats["removed_items"] += 1
        return action
   
    def _categorize_results(
       self,
       validation_results: List[ValidationResult]
   ) -> Dict[str, List[ValidationResult]]:
       """ê²€ì¦ ê²°ê³¼ë¥¼ ìƒíƒœë³„ë¡œ ë¶„ë¥˜"""
       
       categorized = {
           "perfect": [],
           "gt_wrong": [],
           "agent_failed": [],
           "all_different": [],
           "other": []
       }
       
       for result in validation_results:
           status = result.get_validation_status()
           if status == "âœ… PERFECT":
               categorized["perfect"].append(result)
           elif status == "ğŸ”´ GT_WRONG":
               categorized["gt_wrong"].append(result)
           elif status == "âš ï¸ AGENT_FAILED":
               categorized["agent_failed"].append(result)
           elif status == "âŒ ALL_DIFFERENT":
               categorized["all_different"].append(result)
           else:
               categorized["other"].append(result)
       
       return categorized
   
    def _fix_wrong_ground_truths(
       self,
       results: List[ValidationResult]
   ) -> List[ImprovementAction]:
       """ì˜ëª»ëœ Ground Truthë¥¼ BuilderCore ê°’ìœ¼ë¡œ ìˆ˜ì •"""
       
       actions = []
       
       for result in results:
           # BuilderCore ê°’ì´ ì •ë‹µ
           correct_answer = result.logic_answer
           
           if correct_answer is not None:
               # ì›ë³¸ í•­ëª© ì°¾ê¸°
               original_item = self._find_original_item(result.question_id)
               
               if original_item:
                   # ê°œì„ ëœ í•­ëª© ìƒì„±
                   improved_item = original_item.copy()
                   improved_item["ground_truth"] = correct_answer
                   improved_item["validation_status"] = "CORRECTED_BY_LOGIC"
                   improved_item["correction_log"] = {
                       "original_answer": result.ground_truth,
                       "corrected_answer": correct_answer,
                       "agent_answer": result.agent_answer,
                       "reason": "BuilderCore ê³„ì‚°ê°’ìœ¼ë¡œ ìˆ˜ì •"
                   }
                   
                   action = ImprovementAction(
                       action_type="fix_answer",
                       original_item=original_item,
                       improved_item=improved_item,
                       reason=f"Ground Truth ìˆ˜ì •: {result.ground_truth} â†’ {correct_answer}",
                       confidence=1.0  # BuilderCoreëŠ” 100% ì‹ ë¢°
                   )
                   
                   actions.append(action)
                   self.improvement_stats["fixed_answers"] += 1
       
       return actions
   
    def _improve_difficult_questions(
       self,
       results: List[ValidationResult]
   ) -> List[ImprovementAction]:
       """ì—ì´ì „íŠ¸ê°€ í‹€ë¦° ì§ˆë¬¸ë“¤ì„ ê°œì„ """
       
       actions = []
       
       for result in results:
           # ì—ì´ì „íŠ¸ê°€ ì™œ í‹€ë ¸ëŠ”ì§€ ë¶„ì„
           improvement_needed = self._analyze_agent_failure(result)
           
           if improvement_needed == "clarify":
               action = self._clarify_question(result)
           elif improvement_needed == "add_context":
               action = self._add_context_to_question(result)
           elif improvement_needed == "simplify":
               action = self._simplify_question(result)
           else:
               continue
           
           if action:
               actions.append(action)
       
       return actions
   
    def _analyze_agent_failure(
       self, 
       result: ValidationResult
   ) -> str:
       """ì—ì´ì „íŠ¸ê°€ ì‹¤íŒ¨í•œ ì›ì¸ ë¶„ì„"""
       
       # ë‚®ì€ í™•ì‹ ë„ â†’ ì§ˆë¬¸ì´ ëª¨í˜¸í•¨
       if result.agent_confidence < 0.3:
           return "clarify"
       
       # ì¶”ë¡  ë‹¨ê³„ê°€ ë„ˆë¬´ ë³µì¡ â†’ ë‹¨ìˆœí™” í•„ìš”
       if len(result.agent_reasoning) > 10:
           return "simplify"
       
       # ì»¨í…ìŠ¤íŠ¸ ë¶€ì¡± ì–¸ê¸‰ â†’ ì •ë³´ ì¶”ê°€
       reasoning_text = " ".join(result.agent_reasoning).lower()
       if any(word in reasoning_text for word in ["unclear", "missing", "unknown", "assume"]):
           return "add_context"
       
       return "none"
   
    def _clarify_question(
       self,
       result: ValidationResult
   ) -> Optional[ImprovementAction]:
       """ëª¨í˜¸í•œ ì§ˆë¬¸ì„ ëª…í™•í•˜ê²Œ ê°œì„ """
       
       # LLMì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ ê°œì„ 
       clarification_prompt = f"""
ë‹¤ìŒ ì§ˆë¬¸ì„ ì—ì´ì „íŠ¸ê°€ ì´í•´í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.

ì›ë³¸ ì§ˆë¬¸: {result.question}
ì •ë‹µ: {result.ground_truth}
ì—ì´ì „íŠ¸ ë‹µë³€: {result.agent_answer}
ì—ì´ì „íŠ¸ì˜ í˜¼ë€: {result.agent_reasoning[:3] if result.agent_reasoning else "ì¶”ë¡  ì‹¤íŒ¨"}

ì´ ì§ˆë¬¸ì„ ë” ëª…í™•í•˜ê³  êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ì‹œ ì‘ì„±í•˜ì„¸ìš”.
ëª¨í˜¸í•œ ë¶€ë¶„ì„ ì œê±°í•˜ê³ , ì •í™•íˆ ë¬´ì—‡ì„ ë¬»ëŠ”ì§€ ëª…ì‹œí•˜ì„¸ìš”.
"""
       
       schema = {
           "type": "object",
           "properties": {
               "improved_question": {"type": "string"},
               "changes_made": {"type": "array", "items": {"type": "string"}},
               "clarity_score": {"type": "number", "minimum": 0, "maximum": 1}
           },
           "required": ["improved_question", "changes_made"]
       }
       
       try:
           response = _call_llm_json(
               messages=[
                   {"role": "user", "content": clarification_prompt}
               ],
               schema=schema,
               temperature=0.2
           )
           
           original_item = self._find_original_item(result.question_id)
           if original_item and response.get("improved_question"):
               improved_item = original_item.copy()
               improved_item["question"] = response["improved_question"]
               improved_item["improvement_log"] = {
                   "original_question": result.question,
                   "changes": response.get("changes_made", []),
                   "reason": "ì—ì´ì „íŠ¸ ì´í•´ë„ ê°œì„ "
               }
               
               action = ImprovementAction(
                   action_type="clarify_question",
                   original_item=original_item,
                   improved_item=improved_item,
                   reason="ì§ˆë¬¸ ëª…í™•í™”",
                   confidence=response.get("clarity_score", 0.7)
               )
               
               self.improvement_stats["clarified_questions"] += 1
               return action
               
       except Exception as e:
           print(f"ì§ˆë¬¸ ê°œì„  ì‹¤íŒ¨: {e}")
       
       return None
   
    def _handle_problematic_items(
       self,
       results: List[ValidationResult]
   ) -> List[ImprovementAction]:
       """ëª¨ë“  ë‹µì´ ë‹¤ë¥¸ ë¬¸ì œ í•­ëª©ë“¤ ì²˜ë¦¬"""
       
       actions = []
       
       for result in results:
           # ì „ë©´ ì¬ìƒì„± ì‹œë„
           action = self._regenerate_item_completely(result)
           if action:
               actions.append(action)
           else:
               # ì¬ìƒì„± ì‹¤íŒ¨ ì‹œ ì œê±°
               action = self._mark_for_removal(result)
               if action:
                   actions.append(action)
       
       return actions
   
    def _regenerate_item_completely(
        self,
       result: ValidationResult
   ) -> Optional[ImprovementAction]:
       """í•­ëª©ì„ ì™„ì „íˆ ì¬ìƒì„±"""
       
       # BuilderCore ë‹µë³€ì„ ê¸°ì¤€ìœ¼ë¡œ ìƒˆ ì§ˆë¬¸ ìƒì„±
       if result.logic_answer is None:
           return None
       
       regeneration_prompt = f"""
ë„¤íŠ¸ì›Œí¬ ì„¤ì •ì—ì„œ ë‹µì´ "{result.logic_answer}"ì¸ ìƒˆë¡œìš´ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

ì›ë˜ ì‹¤íŒ¨í•œ ì§ˆë¬¸: {result.question}
ë¬¸ì œì : ì—ì´ì „íŠ¸({result.agent_answer}), ì›ë˜ ì •ë‹µ({result.ground_truth}), ì‹¤ì œ ë‹µ({result.logic_answer})ì´ ëª¨ë‘ ë‹¬ëìŠµë‹ˆë‹¤.

ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ ìƒˆ ì§ˆë¬¸ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.
"""
       
       schema = {
           "type": "object",
           "properties": {
               "new_question": {"type": "string"},
               "question_type": {"type": "string"},
               "complexity": {"type": "string", "enum": ["basic", "intermediate", "advanced"]},
               "reasoning": {"type": "string"}
           },
           "required": ["new_question", "complexity"]
       }
       
       try:
           response = _call_llm_json(
               messages=[{"role": "user", "content": regeneration_prompt}],
               schema=schema,
               temperature=0.3
           )
           
           if response.get("new_question"):
               original_item = self._find_original_item(result.question_id)
               if original_item:
                   improved_item = {
                       "test_id": result.question_id,
                       "question": response["new_question"],
                       "ground_truth": result.logic_answer,
                       "complexity": response.get("complexity", "intermediate"),
                       "category": original_item.get("category", "unknown"),
                       "validation_status": "REGENERATED",
                       "regeneration_log": {
                           "original_question": result.question,
                           "reason": "ì™„ì „ ì¬ìƒì„± - ëª¨ë“  ë‹µë³€ ë¶ˆì¼ì¹˜",
                           "logic_answer_used": True
                       }
                   }
                   
                   return ImprovementAction(
                       action_type="regenerate",
                       original_item=original_item,
                       improved_item=improved_item,
                       reason="ì „ë©´ ì¬ìƒì„±",
                       confidence=0.8
                   )
                   
       except Exception as e:
           print(f"ì¬ìƒì„± ì‹¤íŒ¨: {e}")
       
       return None
   
    def _apply_improvements(
       self,
       original_dataset: List[Dict[str, Any]],
       improvement_actions: List[ImprovementAction]
   ) -> List[Dict[str, Any]]:
       """ê°œì„  ì•¡ì…˜ì„ ë°ì´í„°ì…‹ì— ì ìš©"""
       
       # IDë¥¼ í‚¤ë¡œ í•˜ëŠ” ë§µ ìƒì„±
       improvements_map = {}
       for action in improvement_actions:
           if action.improved_item:
               test_id = action.original_item.get("test_id")
               if test_id:
                   improvements_map[test_id] = action.improved_item
       
       # ê°œì„ ëœ ë°ì´í„°ì…‹ ìƒì„±
       improved_dataset = []
       removed_count = 0
       
       for item in original_dataset:
           test_id = item.get("test_id")
           
           if test_id in improvements_map:
               # ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ êµì²´
               improved_dataset.append(improvements_map[test_id])
           else:
               # ì œê±° ëŒ€ìƒ ì²´í¬
               should_remove = any(
                   action.action_type == "remove" and 
                   action.original_item.get("test_id") == test_id
                   for action in improvement_actions
               )
               
               if not should_remove:
                   improved_dataset.append(item)
               else:
                   removed_count += 1
       
       print(f"\nê°œì„  ì™„ë£Œ: {len(improvements_map)}ê°œ ìˆ˜ì •, {removed_count}ê°œ ì œê±°")
       
       return improved_dataset
   
    def _print_improvement_report(self, report: Dict[str, Any]) -> None:
       """ê°œì„  ë¦¬í¬íŠ¸ ì¶œë ¥"""
       
       print("\n" + "="*60)
       print("ğŸ“ˆ ê°œì„  ê²°ê³¼")
       print("="*60)
       
       print(f"\nì´ ê°œì„  ì•¡ì…˜: {report['total_improvements']}ê°œ")
       
       stats = report['improvement_stats']
       print("\nê°œì„  ìœ í˜•ë³„ í†µê³„:")
       print(f"  - ì •ë‹µ ìˆ˜ì •: {stats['fixed_answers']}ê°œ")
       print(f"  - ì§ˆë¬¸ ëª…í™•í™”: {stats['clarified_questions']}ê°œ")
       print(f"  - ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€: {stats['added_context']}ê°œ")
       print(f"  - ì œê±°ëœ í•­ëª©: {stats['removed_items']}ê°œ")
       
       if report.get('actions_by_type'):
           print("\nì•¡ì…˜ íƒ€ì…ë³„ ë¶„í¬:")
           for action_type, count in report['actions_by_type'].items():
               print(f"  - {action_type}: {count}ê°œ")
       
       print(f"\nê°œì„  ì„±ê³µë¥ : {report['success_rate']:.1%}")
       print("="*60)
