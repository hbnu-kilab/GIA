"""
í†µí•© ë„¤íŠ¸ì›Œí¬ ì„¤ì • í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸
ë¡œì§ ê¸°ë°˜(ê¸°ì´ˆ) + LLM ê¸°ë°˜(ì‹¬í™”) ì§ˆë¬¸ ìƒì„± ë° ë‹¤ë©´ì  í‰ê°€ ì‹œìŠ¤í…œ
"""

from __future__ import annotations
from typing import Dict, Any, List, Optional, Tuple
from dataclasses import dataclass, asdict, field
from enum import Enum
import hashlib
import json
from pathlib import Path
import logging
import re
import logging




# ê¸°ì¡´ ëª¨ë“ˆë“¤
from parsers.universal_parser import UniversalParser
from generators.rule_based_generator import RuleBasedGenerator, RuleBasedGeneratorConfig
# from generators.llm_explorer import LLMExplorer
from assemblers.test_assembler import TestAssembler, AssembleOptions
from inspectors.intent_inspector import IntentInspector
from utils.builder_core import BuilderCore
from agents.answer_agent import AnswerAgent
from agents.command_agent import CommandAgent
# from agents.validation_agent import ValidationAgent
# from agents.feedback_loop import FeedbackLoop
from agents.hybrid_validation_system import HybridValidationSystem, ValidationMode
from agents.hybrid_feedback_loop import HybridFeedbackLoop
from utils.config_manager import get_settings

# ìƒˆë¡œìš´ í–¥ìƒëœ ëª¨ë“ˆë“¤ (ìœ„ì—ì„œ ìƒì„±í•œ ê²ƒë“¤)
from generators.enhanced_llm_generator import EnhancedLLMQuestionGenerator, QuestionComplexity, PersonaType
from inspectors.evaluation_system import ComprehensiveEvaluator, AnswerType, EvaluationResult


class PipelineStage(Enum):
    """íŒŒì´í”„ë¼ì¸ ë‹¨ê³„ ì •ì˜"""
    PARSING = "parsing"
    BASIC_GENERATION = "basic_generation"
    ENHANCED_GENERATION = "enhanced_generation"
    ASSEMBLY = "assembly"
    VALIDATION = "validation"
    EVALUATION = "evaluation"


@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ì…ë ¥ ì„¤ì •
    xml_data_dir: str
    policies_path: str
    
    # ìƒì„± ì„¤ì •
    target_categories: List[str]
    basic_questions_per_category: int = field(
        default_factory=lambda: get_settings().generation.basic_questions_per_category
    )
    enhanced_questions_per_category: int = field(
        default_factory=lambda: get_settings().generation.enhanced_questions_per_category
    )

    # ì‹œë‚˜ë¦¬ì˜¤ ì„¤ì •
    scenario_type: str = "normal"  # normal, failure, expansion
    scenario_overrides: Optional[Dict[str, Any]] = None
    
    # ë³µì¡ë„ ë° í˜ë¥´ì†Œë‚˜ ì„¤ì •
    target_complexities: List[QuestionComplexity] = None
    target_personas: List[PersonaType] = None
    
    # í‰ê°€ ì„¤ì •
    enable_bert_score: bool = False  # BERT-ScoreëŠ” ë³„ë„ ë¼ì´ë¸ŒëŸ¬ë¦¬ í•„ìš”
    short_answer_threshold: int = 20  # 20ë‹¨ì–´ ì´í•˜ëŠ” short answer
    
    # ì¶œë ¥ ì„¤ì •
    output_dir: str = "output"
    save_intermediate: bool = True

    # ê· í˜•/ì¤‘ë³µ ì„¤ì •
    balance_max_per_category: Optional[int] = None  # Noneì´ë©´ ì»· ë¹„í™œì„±í™”
    balance_group_key: str = "topic"  # topic(ë©”íƒ€) ìš°ì„ , ì—†ìœ¼ë©´ category
    
    def __post_init__(self):
        if self.target_complexities is None:
            # ê¸°ë³¸ê°’: ëª¨ë“  ë³µì¡ë„ ì‚¬ìš©
            self.target_complexities = [
                QuestionComplexity.BASIC,
                QuestionComplexity.ANALYTICAL, 
                QuestionComplexity.SYNTHETIC,
                QuestionComplexity.DIAGNOSTIC,
                QuestionComplexity.SCENARIO,
            ]
        if self.target_personas is None:
            self.target_personas = [
                PersonaType.NETWORK_ENGINEER,
                PersonaType.SECURITY_AUDITOR,
                PersonaType.NOC_OPERATOR,
                PersonaType.ARCHITECT,
                PersonaType.TROUBLESHOOTER,
                PersonaType.COMPLIANCE_OFFICER
            ]


@dataclass
class DatasetSample:
    """ë°ì´í„°ì…‹ ìƒ˜í”Œ êµ¬ì¡°"""
    id: str
    question: str
    context: str  # ë„¤íŠ¸ì›Œí¬ ì„¤ì • ì»¨í…ìŠ¤íŠ¸
    ground_truth: Any
    explanation: str
    answer_type: str  # "short" or "long"
    category: str
    complexity: str
    level: int = 1
    persona: Optional[str] = None
    scenario: Optional[str] = None
    source_files: Optional[List[str]] = None
    metadata: Optional[Dict[str, Any]] = None


def assign_task_category(sample: DatasetSample) -> str:
    """ì§ˆë¬¸ì˜ íŠ¹ì„±ì— ë”°ë¼ ì—…ë¬´ ë¶„ë¥˜ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
    question = sample.question.lower()
    persona = sample.persona
    complexity = sample.complexity

    if "ëª…ë ¹ì–´" in question or "cli" in question or persona == "automation_engineer":
        return "ëª…ë ¹ì–´ ì‚¬ìš©ë²•/ì§ˆì˜"

    if sample.metadata and (sample.metadata.get("topic") == "Security_Policy" or persona == "security_auditor"):
        return "ë³´ì•ˆ ì§ˆì˜"

    if complexity == "diagnostic" or persona == "troubleshooter":
        return "ê¸°ìˆ ì  ì˜¤ë¥˜ ì§ˆì˜"

    if ("í† í´ë¡œì§€" in question or "ì—°ê²°" in question or "êµ¬ì„±" in question or (sample.source_files and len(sample.source_files) > 1)):
        if complexity in ["analytical", "synthetic"]:
            return "ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ êµ¬ì„± ì§ˆì˜"

    if complexity == "basic":
        return "ë‹¨ìˆœ ì¡°íšŒ ì—…ë¬´"

    return "ë„¤íŠ¸ì›Œí¬ í† í´ë¡œì§€ êµ¬ì„± ì§ˆì˜"


class NetworkConfigDatasetGenerator:
    """í†µí•© ë„¤íŠ¸ì›Œí¬ ì„¤ì • ë°ì´í„°ì…‹ ìƒì„±ê¸°"""
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        Path(self.config.output_dir).mkdir(exist_ok=True)
        self.logger = self._setup_logger()
        
        # íŒŒì´í”„ë¼ì¸ ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.parser = UniversalParser()
        self.rule_generator = RuleBasedGenerator(
            RuleBasedGeneratorConfig(
                policies_path=config.policies_path,
                min_per_cat=config.basic_questions_per_category * 2,  # ì¹´í…Œê³ ë¦¬ë‹¹ ë” ë§ì€ ì§ˆë¬¸
                scenario_type=config.scenario_type,
            )
        )
        self.evaluator = ComprehensiveEvaluator()
        # í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì‹œìŠ¤í…œ (ë‚˜ì¤‘ì— ì´ˆê¸°í™”)
        self.hybrid_validator = None
        self.hybrid_feedback = None
        self.validation_mode = ValidationMode.HYBRID  # ê¸°ë³¸ê°’
        self.max_validation_iterations = 3


        # self.llm_explorer = LLMExplorer()
        self.enhanced_generator = EnhancedLLMQuestionGenerator()
        self.assembler = TestAssembler(
            AssembleOptions(base_xml_dir=config.xml_data_dir)
        )
        self.evaluator = ComprehensiveEvaluator()
        
        # ì§„í–‰ ìƒí™© ì¶”ì 
        self.stage_results = {}
        
    def generate_complete_dataset(self) -> Dict[str, Any]:
        """ì™„ì „í•œ ë°ì´í„°ì…‹ ìƒì„± - ë©”ì¸ í•¨ìˆ˜"""
        self.logger.info("="*20 + " ë°ì´í„°ì…‹ ìƒì„± íŒŒì´í”„ë¼ì¸ ì‹œì‘ " + "="*20)
        
        try:
            # 1ë‹¨ê³„: XML íŒŒì‹±
            self.logger.info("ğŸ”„ 1ë‹¨ê³„: XML íŒŒì‹± ì‹œì‘")
            network_facts = self._execute_stage_parsing()
            self.logger.info(f"âœ… 1ë‹¨ê³„ ì™„ë£Œ: {network_facts.get('device_count', 0)}ê°œ ì¥ë¹„ íŒŒì‹±")
            
            # 2ë‹¨ê³„: ê¸°ì´ˆ ì§ˆë¬¸ ìƒì„± (Rule-based)
            self.logger.info("ğŸ”„ 2ë‹¨ê³„: ê¸°ì´ˆ ì§ˆë¬¸ ìƒì„± ì‹œì‘")
            basic_dataset = self._execute_stage_basic_generation(network_facts)
            self.logger.info(f"âœ… 2ë‹¨ê³„ ì™„ë£Œ: {len(basic_dataset)}ê°œ ê¸°ì´ˆ ì§ˆë¬¸ ìƒì„±")
            
            # 3ë‹¨ê³„: ì‹¬í™” ì§ˆë¬¸ ìƒì„± (Enhanced LLM)
            self.logger.info("ğŸ”„ 3ë‹¨ê³„: ì‹¬í™” ì§ˆë¬¸ ìƒì„± ì‹œì‘")
            enhanced_dataset = self._execute_stage_enhanced_generation(network_facts)
            self.logger.info(f"âœ… 3ë‹¨ê³„ ì™„ë£Œ: {len(enhanced_dataset)}ê°œ ì‹¬í™” ì§ˆë¬¸ ìƒì„±")
            
            # 4ë‹¨ê³„: í†µí•© ë° ì–´ì…ˆë¸”ë¦¬
            self.logger.info("ğŸ”„ 4ë‹¨ê³„: í†µí•© ë° ì–´ì…ˆë¸”ë¦¬ ì‹œì‘")
            self.logger.info(f"í†µí•© ì „ ì´ ê°œìˆ˜: {len(basic_dataset) + len(enhanced_dataset)}")
            integrated_dataset = self._execute_stage_assembly(
                network_facts, basic_dataset, enhanced_dataset
            )
            self.logger.info(f"âœ… 4ë‹¨ê³„ ì™„ë£Œ: {len(integrated_dataset)}ê°œ ìµœì¢… í†µí•©")
            
            # (ì‹ ê·œ) í”„ë¦¬â€‘ë°¸ë¦¬ë°ì´ì…˜: BASIC/Rule ê¸°ë°˜ í•­ëª©ì˜ GTë¥¼ ë¡œì§ê°’ìœ¼ë¡œ ìë™ ê²€ì¦/êµì •
            self.logger.info("ğŸ”„ í”„ë¦¬ ê²€ì¦ ì‹œì‘")
            integrated_dataset = self._execute_pre_validation(network_facts, integrated_dataset)
            self.logger.info(f"âœ… í”„ë¦¬ ê²€ì¦ ì™„ë£Œ: {len(integrated_dataset)}ê°œ ê²€ì¦")

            # 6ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ë£¨í”„ (ìƒˆë¡œìš´!)
            self.logger.info("ğŸ”„ í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ë£¨í”„ ì‹œì‘")
            final_dataset, validation_report = self._execute_hybrid_validation_loop(
                integrated_dataset, network_facts)
            self.logger.info(f"âœ… í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì™„ë£Œ: {len(final_dataset)}ê°œ")

            # 6ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ìê°€ í‰ê°€)
            self.logger.info("ğŸ”„ í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° ì‹œì‘")
            evaluation_results = self._execute_stage_evaluation(final_dataset)
            self.stage_results[PipelineStage.EVALUATION] = evaluation_results
            self.logger.info("âœ… í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° ì™„ë£Œ")
            
            # ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±
            self.logger.info("ğŸ”„ ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„± ì‹œì‘")
            final_dataset = self._compose_final_dataset(final_dataset, evaluation_results)
            self._save_results(final_dataset)
            self.logger.info("âœ… ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„± ì™„ë£Œ")
            
            self.logger.info("="*20 + " ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ " + "="*20)
            return final_dataset
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
            self.logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback
            self.logger.error(f"ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            raise

    

    # ìƒˆë¡œìš´ ë©”ì„œë“œ ì¶”ê°€
    def _execute_hybrid_validation_loop(
        self,
        dataset: List[DatasetSample],
        network_facts: Dict[str, Any]
    ) -> Tuple[List[DatasetSample], Dict[str, Any]]:
        """
        6ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ë£¨í”„
        ì—ì´ì „íŠ¸ê°€ ë¬¸ì œë¥¼ í’€ê³ , BuilderCoreë¡œ ì •ë‹µì„ í™•ì¸í•˜ëŠ” ì´ì¤‘ ê²€ì¦
        """
        
        self.logger.info("="*60)
        self.logger.info("6ë‹¨ê³„: í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ë£¨í”„ ì‹œì‘")
        self.logger.info("="*60)
        
        # í•˜ì´ë¸Œë¦¬ë“œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        if not self.hybrid_validator:
            self.hybrid_validator = HybridValidationSystem(
                network_facts=network_facts,
                mode=self.validation_mode,
                xml_base_dir=self.config.xml_data_dir
            )
            self.hybrid_feedback = HybridFeedbackLoop(network_facts)
        
        # ë°ì´í„°ì…‹ ë”•ì…”ë„ˆë¦¬ ë³€í™˜
        dataset_dicts = [asdict(sample) for sample in dataset]
        
        iteration = 0
        validation_history = []
        total_improvements = 0
        
        while iteration < self.max_validation_iterations:
            self.logger.info(f"\nê²€ì¦ ë°˜ë³µ {iteration + 1}/{self.max_validation_iterations}")
            
            try:
                # Step 1: í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ìˆ˜í–‰
                # ìƒ˜í”Œ í¬ê¸° ì œì–´: config.validation_sample_size(ì—†ê±°ë‚˜ 0ì´ë©´ ì „ì²´)
                initial_sample = getattr(self.config, "validation_sample_size", None)
                sample_size = (initial_sample if (iteration == 0) else None)
                if sample_size in (0, None):
                    sample_size = None

                validation_results, validation_stats = self.hybrid_validator.validate_dataset(
                    dataset_dicts,
                    sample_size=sample_size
                )
                
                validation_history.append(validation_stats)
                
                # ì£¼ìš” ì§€í‘œ ì¶œë ¥
                self.logger.info(
                    f"ì—ì´ì „íŠ¸ ì •í™•ë„: {validation_stats['agent_performance']['accuracy']:.1%}"
                )
                self.logger.info(
                    f"Ground Truth ì •í™•ë„: {validation_stats['ground_truth_quality']['accuracy']:.1%}"
                )
                
                # Step 2: ëª©í‘œ ë‹¬ì„± í™•ì¸
                if validation_stats['ground_truth_quality']['accuracy'] >= 0.95:
                    self.logger.info("âœ… ëª©í‘œ ì •í™•ë„ 95% ë‹¬ì„±!")
                    break
                
                # Step 3: í”¼ë“œë°± ë£¨í”„ ì‹¤í–‰
                improved_dataset, improvement_report = self.hybrid_feedback.improve_dataset(
                    validation_results,
                    dataset_dicts
                )
                
                if improvement_report['total_improvements'] == 0:
                    self.logger.info("ê°œì„ í•  í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤.")
                    break
                
                # Step 4: ê°œì„ ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ì—…ë°ì´íŠ¸
                dataset_dicts = improved_dataset
                total_improvements += improvement_report['total_improvements']
                
                self.logger.info(
                    f"ì´ë²ˆ ë°˜ë³µì—ì„œ {improvement_report['total_improvements']}ê°œ ê°œì„ "
                )
                
            except Exception as e:
                self.logger.error(f"ê²€ì¦ ë°˜ë³µ {iteration + 1}ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
                self.logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
                import traceback
                self.logger.error(f"ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
                
                # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë£¨í”„ë¥¼ ê³„ì† ì§„í–‰
                self.logger.warning("ì˜¤ë¥˜ ë°œìƒìœ¼ë¡œ ì´ë²ˆ ë°˜ë³µì„ ê±´ë„ˆëœë‹ˆë‹¤.")
                
            iteration += 1
        
        # ìµœì¢… ë¦¬í¬íŠ¸ ìƒì„±
        final_report = {
            "validation_mode": self.validation_mode.value,
            "iterations": iteration + 1,
            "total_improvements": total_improvements,
            "validation_history": validation_history,
            "final_stats": validation_history[-1] if validation_history else {}
        }
        
        self.logger.info("\n" + "="*60)
        self.logger.info("í•˜ì´ë¸Œë¦¬ë“œ ê²€ì¦ ì™„ë£Œ!")
        self.logger.info(f"ì´ ë°˜ë³µ: {iteration + 1}íšŒ")
        self.logger.info(f"ì´ ê°œì„ : {total_improvements}ê°œ")
        if validation_history:
            final_accuracy = validation_history[-1]['ground_truth_quality']['accuracy']
            self.logger.info(f"ìµœì¢… Ground Truth ì •í™•ë„: {final_accuracy:.1%}")
        self.logger.info("="*60)
        
        # DatasetSampleë¡œ ë³€í™˜
        final_samples = [
            self._dict_to_dataset_sample(d) for d in dataset_dicts
        ]
        
        return final_samples, final_report
    
    def _execute_stage_parsing(self) -> Dict[str, Any]:
        """1ë‹¨ê³„: XML íŒŒì‹±"""
        self.logger.info("1ë‹¨ê³„: ë„¤íŠ¸ì›Œí¬ ì„¤ì • íŒŒì¼ íŒŒì‹±")
        
        network_facts = self.parser.parse_dir(self.config.xml_data_dir)
        
        device_count = len(network_facts.get("devices", []))
        self.logger.info(f"íŒŒì‹± ì™„ë£Œ: {device_count}ê°œ ì¥ë¹„")
        
        self.stage_results[PipelineStage.PARSING] = {
            "device_count": device_count,
            "success": True
        }
        
        if self.config.save_intermediate:
            self._save_intermediate("parsed_facts.json", network_facts)
        
        return network_facts
    
    def _execute_stage_basic_generation(self, network_facts: Dict[str, Any]) -> List[DatasetSample]:
        """2ë‹¨ê³„: ê¸°ì´ˆ ì§ˆë¬¸ ìƒì„± (Rule-based) - ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤ ì§€ì›"""
        self.logger.info("2ë‹¨ê³„: ê¸°ì´ˆ ì§ˆë¬¸ ìƒì„± (Rule-based) - ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤")
        
        # ë‹¤ì¤‘ ì‹œë‚˜ë¦¬ì˜¤ë¡œ ë” ë§ì€ ì§ˆë¬¸ ìƒì„±
        scenarios = ["normal", "failure", "expansion"]
        all_dsl_items = []
        all_command_items = []
        
        for scenario in scenarios:
            self.logger.info(f"ì‹œë‚˜ë¦¬ì˜¤ '{scenario}' ì§ˆë¬¸ ìƒì„± ì¤‘...")
            
            dsl_items = self.rule_generator.compile(
                capabilities=network_facts,
                categories=self.config.target_categories,
                scenario_type=scenario,
            )
            
            # ì‹œë‚˜ë¦¬ì˜¤ ì •ë³´ íƒœê¹…
            for item in dsl_items:
                item["scenario"] = scenario
                
            command_items = [d for d in dsl_items if d.get("category") == "Command_Generation"]
            regular_items = [d for d in dsl_items if d.get("category") != "Command_Generation"]
            
            all_dsl_items.extend(regular_items)
            all_command_items.extend(command_items)
        
        self.logger.info(f"ì´ DSL ì•„ì´í…œ: {len(all_dsl_items)}, ëª…ë ¹ì–´ ì•„ì´í…œ: {len(all_command_items)}")
        
        # ì–´ì…ˆë¸”ë¦¬ë¥¼ í†µí•œ ë‹µë³€ ê³„ì‚°
        assembled_tests = self.assembler.assemble(
            network_facts,
            all_dsl_items,
            scenario_conditions=self.config.scenario_overrides,
        )

        command_agent = CommandAgent(network_facts)
        
        # DatasetSampleë¡œ ë³€í™˜
        basic_samples = []
        for category, tests in assembled_tests.items():
            for test in tests:
                gt, expl = self._format_answer(test.get('expected_answer', {}))
                sample = DatasetSample(
                    id=f"BASIC_{test.get('test_id', f'{category}_{len(basic_samples)}')}",
                    question=test.get('question', ''),
                    context=self._create_context(network_facts, test.get('source_files', [])),
                    ground_truth=gt,
                    explanation=expl,
                    answer_type=self._determine_answer_type(gt),
                    category="basic",
                    level=test.get('level', 1),
                    complexity="basic",
                    scenario=test.get('scenario', 'normal'),
                    source_files=test.get('source_files', []),
                    metadata={
                        "origin": "rule_based",
                        "intent": test.get('intent', {}),
                        "evidence_hint": test.get('evidence_hint', {}),
                        "topic": category
                    }
                )
                basic_samples.append(sample)

        # Command Generation ì¹´í…Œê³ ë¦¬ ì „ìš© ì²˜ë¦¬ (ì‹œë‚˜ë¦¬ì˜¤ë³„ ì¤‘ë³µ ë°©ì§€)
        # rule_based_generatorì—ì„œ _generate_command_questionsë¡œ ìƒì„±ëœ ëª…ë ¹ì–´ ì§ˆë¬¸ë“¤ ì²˜ë¦¬
        command_questions = self.rule_generator._generate_command_questions(network_facts)
        
        for idx, item in enumerate(command_questions):
            intent = item.get('intent', {})
            metric = intent.get('metric', '')
            params = intent.get('params', {})
            
            # CommandAgentë¥¼ í†µí•´ ì‹¤ì œ ëª…ë ¹ì–´ ìƒì„±
            if metric.startswith('cmd_'):
                metric_name = metric[4:]  # 'cmd_' ì ‘ë‘ì‚¬ ì œê±°
            else:
                metric_name = metric
                
            try:
                command = command_agent.generate(metric_name, params)
            except Exception as e:
                self.logger.error(f"Command generation failed for {metric_name}: {e}")
                # ëŒ€ì•ˆìœ¼ë¡œ ê¸°ë³¸ ëª…ë ¹ì–´ íŒ¨í„´ ì‚¬ìš©
                command = f"show {metric_name}"
                
            gt, expl = self._format_answer({"ground_truth": command, "explanation": ""})
            sample = DatasetSample(
                id=f"BASIC_CMD_{idx}",
                question=item.get('question', ''),
                context=self._create_context(network_facts, item.get('source_files', [])),
                ground_truth=gt,
                explanation=expl,
                answer_type=self._determine_answer_type(gt),
                category="basic",
                level=item.get('level', 1),
                complexity="basic",
                scenario="normal",  # Command ì§ˆë¬¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ normal ì‹œë‚˜ë¦¬ì˜¤
                source_files=item.get('source_files', []),
                metadata={
                    "origin": "rule_based",
                    "intent": {"command": metric_name, "params": params},
                    "evidence_hint": {},
                    "topic": "Command_Generation",
                },
            )
            basic_samples.append(sample)
        
        self.logger.info(f"ê¸°ì´ˆ ì§ˆë¬¸ ìƒì„± ì™„ë£Œ: {len(basic_samples)}ê°œ")
        
        self.stage_results[PipelineStage.BASIC_GENERATION] = {
            "question_count": len(basic_samples),
            "categories": list(assembled_tests.keys()),
            "scenarios": scenarios,
            "success": True
        }
        
        if self.config.save_intermediate:
            self._save_intermediate("basic_dataset.json", [asdict(s) for s in basic_samples])
        
        return basic_samples
    
    def _execute_stage_enhanced_generation(self, network_facts: Dict[str, Any]) -> List[DatasetSample]:
        """3ë‹¨ê³„: ì‹¬í™” ì§ˆë¬¸ ìƒì„± ë° 'AnswerAgent'ë¥¼ í†µí•œ ì •ë‹µ ìƒì„±"""
        self.logger.info("3ë‹¨ê³„: ì‹¬í™” ì§ˆë¬¸ ìƒì„± (Enhanced LLM) ë° ì •ë‹µ ìƒì„± (AnswerAgent)")

        try:
            print(f"[Pipeline] target_complexities: {[c.value for c in self.config.target_complexities]}")
            print(f"[Pipeline] questions_per_template: {self.config.enhanced_questions_per_category}")
            
            enhanced_questions = self.enhanced_generator.generate_enhanced_questions(
                network_facts=network_facts,
                target_complexities=self.config.target_complexities,
                questions_per_template=self.config.enhanced_questions_per_category,
            )
            self.logger.info(f"LLMì´ ìƒì„±í•œ ì´ˆê¸° ì§ˆë¬¸ ìˆ˜: {len(enhanced_questions)}")
            
            reviewed_questions = self.enhanced_generator._review_generated_questions(enhanced_questions)
            self.logger.info(f"LLM ë¦¬ë·° í›„ ìœ íš¨í•œ ì§ˆë¬¸ ìˆ˜: {len(reviewed_questions)}")

            answer_agent = AnswerAgent(network_facts)
            enhanced_samples: List[DatasetSample] = []

            for eq in reviewed_questions:
                if not isinstance(eq, dict):
                    self.logger.warning(f"Skipping non-dict entry: {type(eq)}")
                    continue

                question_text = eq.get("question")
                reasoning_plan = eq.get("reasoning_plan")
                if not question_text or not reasoning_plan:
                    self.logger.debug(f"ì§ˆë¬¸ ë˜ëŠ” ì¶”ë¡  ê³„íš ëˆ„ë½: question={bool(question_text)}, plan={bool(reasoning_plan)}")
                    continue

                try:
                    result = answer_agent.execute_plan(question_text, reasoning_plan)
                    final_answer = result.get("ground_truth")
                    explanation = result.get("explanation", "")
                    if final_answer in (None, ""):
                        self.logger.warning(f"AnswerAgentê°€ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤: {question_text}")
                        continue

                    sample = DatasetSample(
                        id=f"ENHANCED_{eq.get('test_id', 'ENH')}",
                        question=question_text,
                        context="",
                        ground_truth=final_answer,
                        explanation=explanation,
                        answer_type=self._determine_answer_type(final_answer),
                        category=eq.get("category", "Enhanced_Analysis"),
                        complexity=eq.get("complexity", "analytical"),
                        level=eq.get("level", 3),
                        persona=eq.get("persona"),
                        scenario=eq.get("scenario"),
                        source_files=result.get("source_files"),
                        metadata={
                            "origin": "enhanced_llm_with_agent",
                            "reasoning_plan": reasoning_plan,
                            "reasoning_requirement": eq.get("reasoning_requirement", ""),
                            "expected_analysis_depth": eq.get("expected_analysis_depth", "detailed"),
                            "evidence": result.get("evidence", answer_agent.evidence),
                        },
                    )
                    sample.context = self._create_enhanced_context(network_facts, sample)
                    enhanced_samples.append(sample)
                    
                except Exception as e:
                    self.logger.error(f"AnswerAgent ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: Q='{question_text}', Error: {e}")
                    continue

            self.logger.info(f"ì‹¬í™” ì§ˆë¬¸ ë° ì •ë‹µ ìƒì„± ì™„ë£Œ: {len(enhanced_samples)}ê°œ")

            self.stage_results[PipelineStage.ENHANCED_GENERATION] = {
                "question_count": len(enhanced_samples),
                "complexities": list(set(s.complexity for s in enhanced_samples)),
                "personas": list(set(s.persona for s in enhanced_samples if s.persona)),
                "success": True,
            }

            if self.config.save_intermediate:
                self._save_intermediate("enhanced_dataset.json", [asdict(s) for s in enhanced_samples])

            return enhanced_samples
            
        except Exception as e:
            self.logger.error(f"ì‹¬í™” ì§ˆë¬¸ ìƒì„± ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            self.logger.error(f"ì˜¤ë¥˜ íƒ€ì…: {type(e).__name__}")
            import traceback
            self.logger.error(f"ìŠ¤íƒíŠ¸ë ˆì´ìŠ¤:\n{traceback.format_exc()}")
            
            # ì˜¤ë¥˜ê°€ ë°œìƒí•´ë„ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜í•˜ì—¬ íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰
            self.stage_results[PipelineStage.ENHANCED_GENERATION] = {
                "question_count": 0,
                "complexities": [],
                "personas": [],
                "success": False,
                "error": str(e)
            }
            return []
    
    def _execute_stage_assembly(
        self, 
        network_facts: Dict[str, Any],
        basic_samples: List[DatasetSample],
        enhanced_samples: List[DatasetSample]
    ) -> List[DatasetSample]:
        """4ë‹¨ê³„: í†µí•© ë° ì–´ì…ˆë¸”ë¦¬"""
        self.logger.info("4ë‹¨ê³„: ë°ì´í„°ì…‹ í†µí•© ë° ì–´ì…ˆë¸”ë¦¬")
        
        # ê¸°ì´ˆ + ì‹¬í™” ì§ˆë¬¸ í†µí•©
        all_samples = basic_samples + enhanced_samples
        
        # ì¤‘ë³µ ì œê±° (ì§ˆë¬¸+ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ+ì†ŒìŠ¤/ì‹œë‚˜ë¦¬ì˜¤/í˜ë¥´ì†Œë‚˜/ë³µì¡ë„ ì¡°í•©)
        seen_combinations = set()
        deduplicated_samples = []
        duplicate_details = []
        
        self.logger.info(f"ì¤‘ë³µ ì œê±° ì‹œì‘: ì „ì²´ ìƒ˜í”Œ {len(all_samples)}ê°œ")
        
        for i, sample in enumerate(all_samples):
            # ì§ˆë¬¸ + ì»¨í…ìŠ¤íŠ¸ í•´ì‹œ + ì†ŒìŠ¤/ì‹œë‚˜ë¦¬ì˜¤/í˜ë¥´ì†Œë‚˜/ë³µì¡ë„ í‚¤ë¡œ ê³ ìœ ì„± íŒë‹¨
            question_normalized = (sample.question or "").lower().strip()
            ctx = sample.context or ""
            ctx_hash = hashlib.sha1(ctx.encode("utf-8")).hexdigest()[:12] if ctx else ""
            files = sample.source_files or []
            source_files_key = ",".join(sorted(map(str, files)))
            persona_key = sample.persona or ""
            scenario_key = sample.scenario or ""
            complexity_key = sample.complexity or ""
            combination_key = (
                question_normalized,
                ctx_hash,
                source_files_key,
                scenario_key,
                persona_key,
                complexity_key,
            )
            
            if combination_key not in seen_combinations:
                seen_combinations.add(combination_key)
                deduplicated_samples.append(sample)
                self.logger.debug(f"ìƒ˜í”Œ {i+1} ìœ ì§€: {sample.question[:50]}...")
            else:
                duplicate_details.append({
                    "index": i+1,
                    "question": sample.question[:100],
                    "id": sample.id,
                    "combination_key": str(combination_key)[:100]
                })
                self.logger.debug(f"ìƒ˜í”Œ {i+1} ì¤‘ë³µ ì œê±°: {sample.question[:50]}...")
        
        removed_count = len(all_samples) - len(deduplicated_samples)
        self.logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(all_samples)}ê°œ â†’ {len(deduplicated_samples)}ê°œ (ì œê±°: {removed_count}ê°œ)")
        
        # ì¤‘ë³µ ì œê±° ìƒì„¸ ì •ë³´ ì €ì¥
        if self.config.save_intermediate and duplicate_details:
            self._save_intermediate("duplicate_removal_details.json", {
                "total_duplicates": len(duplicate_details),
                "duplicate_samples": duplicate_details[:20]  # ìƒìœ„ 20ê°œë§Œ ì €ì¥
            })
        
        # ì¹´í…Œê³ ë¦¬ë³„ ê· í˜• ì¡°ì • (ê¸°ë³¸ ë¹„í™œì„±í™”: config.balance_max_per_category=None)
        balanced_samples = self._balance_categories(deduplicated_samples)
        
        # Context ì •ë³´ ë³´ê°•
        for sample in balanced_samples:
            if not sample.context or sample.context.strip() == "":
                sample.context = self._create_enhanced_context(network_facts, sample)

        # ë³µì¡ë„ë³„ ê·¸ë£¹í™” ì €ì¥
        samples_by_complexity: Dict[str, List[DatasetSample]] = {}
        for sample in balanced_samples:
            samples_by_complexity.setdefault(sample.complexity, []).append(sample)
        self.samples_by_complexity = samples_by_complexity

        # í•„ìš” ì‹œ ë³µì¡ë„ë³„ ì¤‘ê°„ ê²°ê³¼ ì €ì¥
        if self.config.save_intermediate:
            for comp, items in samples_by_complexity.items():
                self._save_intermediate(f"assembled_{comp}.json", [asdict(s) for s in items])

        dedup_removed = len(all_samples) - len(deduplicated_samples)
        balance_trim = len(deduplicated_samples) - len(balanced_samples)
        self.logger.info(
            f"í†µí•© ì™„ë£Œ: {len(balanced_samples)}ê°œ (ì¤‘ë³µ ì œê±°: {dedup_removed}ê°œ, ê· í˜• ì»·: {balance_trim}ê°œ)"
        )

        self.stage_results[PipelineStage.ASSEMBLY] = {
            "total_samples": len(balanced_samples),
            "basic_count": len(basic_samples),
            "enhanced_count": len(enhanced_samples),
            "dedup_removed": dedup_removed,
            "balance_trim": balance_trim,
            "complexity_counts": {k: len(v) for k, v in samples_by_complexity.items()},
            "success": True,
        }

        return balanced_samples
    
    def _execute_validation_and_feedback_loop(
        self, dataset: List[DatasetSample]
    ) -> Tuple[List[DatasetSample], Dict[str, Any]]:
        """(ì‹ ê·œ) ê²€ì¦ê³¼ êµì •ì„ ë°˜ë³µí•˜ì—¬ ë°ì´í„°ì…‹ í’ˆì§ˆì„ ì ì§„ì ìœ¼ë¡œ í–¥ìƒì‹œí‚¤ëŠ” ë£¨í”„"""
        self.logger.info("="*10 + " 5ë‹¨ê³„: ììœ¨ ê²€ì¦ ë° ìë™ êµì • ë£¨í”„ ì‹œì‘ " + "="*10)
        
        dataset_dicts = [asdict(s) for s in dataset]
        max_iterations = 3
        target_accuracy = 0.98
        validation_history = []
        
        for i in range(max_iterations):
            self.logger.info(f"--- ê²€ì¦ ë£¨í”„ Iteration {i+1}/{max_iterations} ---")
            
            validation_results, stats = self.validation_agent.validate_dataset(dataset_dicts)
            validation_history.append(stats)
            self.logger.info(f"ê²€ì¦ ê²°ê³¼: ì •í™•ë„ {stats['accuracy']:.2%}, ì˜¤ë¥˜ {stats['incorrect']}ê°œ")
            self._save_intermediate(f"validation_results_iter_{i+1}.json", [asdict(r) for r in validation_results])

            if stats['accuracy'] >= target_accuracy or stats['incorrect'] == 0:
                self.logger.info(f"ëª©í‘œ ì •í™•ë„({target_accuracy:.0%}) ë„ë‹¬. ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            regenerated, regen_stats = self.feedback_loop.regenerate_failed_items(validation_results)
            self.logger.info(f"ìë™ êµì • ì‹œë„: {regen_stats['regenerated_and_corrected']}ê°œ í•­ëª© ìˆ˜ì •ë¨")
            
            if regen_stats['regenerated_and_corrected'] == 0:
                self.logger.warning("ë” ì´ìƒ êµì •í•  ìˆ˜ ìˆëŠ” í•­ëª©ì´ ì—†ìŠµë‹ˆë‹¤. ë£¨í”„ë¥¼ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            # êµì •ëœ í•­ëª©ìœ¼ë¡œ ë°ì´í„°ì…‹ ì—…ë°ì´íŠ¸
            regen_map = {item['id']: item for item in regenerated}
            dataset_dicts = [regen_map.get(item['id'], item) for item in dataset_dicts]
            self._save_intermediate(f"corrected_dataset_iter_{i+1}.json", dataset_dicts)

        initial_stats = validation_history[0]
        final_stats = validation_history[-1]
        total_corrected = final_stats['total_items'] - final_stats['correct']

        # ìµœì¢… ë¦¬í¬íŠ¸ìš© í†µê³„ ê³„ì‚°
        report_stats = {
            "initial_accuracy": initial_stats['accuracy'],
            "final_accuracy": final_stats['accuracy'],
            "total_iterations": len(validation_history),
            "total_corrected_items": initial_stats['incorrect'] - final_stats['incorrect'],
            "initial_errors": initial_stats['incorrect'],
            "final_errors": final_stats['incorrect'],
            "auto_correction_success_rate": ((initial_stats['incorrect'] - final_stats['incorrect']) / initial_stats['incorrect']) if initial_stats['incorrect'] > 0 else 1.0,
            "validation_history": validation_history
        }
        
        final_samples = [DatasetSample(**d) for d in dataset_dicts]
        return final_samples, report_stats
    
    def _execute_stage_validation(self, samples: List[DatasetSample]) -> List[DatasetSample]:
        """5ë‹¨ê³„: ê²€ì¦ ë° í’ˆì§ˆ ê´€ë¦¬"""
        self.logger.info("5ë‹¨ê³„: ë°ì´í„° ê²€ì¦ ë° ë‹µë³€ í˜•ì‹ í‘œì¤€í™”")

        validated_samples = []
        rejected_count = 0

        for sample in samples:
            # ê¸°ë³¸ í’ˆì§ˆ ì²´í¬
            if not self._validate_sample_quality(sample):
                rejected_count += 1
                continue

            # Ground truth í‘œì¤€í™”
            sample = self._standardize_ground_truth(sample)

            # Answer type ì¬ë¶„ë¥˜
            sample.answer_type = self._reclassify_answer_type(sample)

            # Context ë° ë©”íƒ€ë°ì´í„° ë³´ì™„
            sample = self._enrich_sample_metadata(sample)
            # ì—…ë¬´ ë¶„ë¥˜ íƒœê¹…
            sample.metadata["task_category"] = assign_task_category(sample)

            validated_samples.append(sample)

        self.logger.info(f"ê²€ì¦ ì™„ë£Œ: {len(validated_samples)}ê°œ í†µê³¼, {rejected_count}ê°œ ê±°ë¶€")

        self.stage_results[PipelineStage.VALIDATION] = {
            "validated_count": len(validated_samples),
            "rejected_count": rejected_count,
            "short_answer_count": len([s for s in validated_samples if s.answer_type == "short"]),
            "long_answer_count": len([s for s in validated_samples if s.answer_type == "long"]),
            "success": True
        }

        if self.config.save_intermediate:
            self._save_intermediate("validated_dataset.json", [asdict(s) for s in validated_samples])

        return validated_samples

    def _standardize_ground_truth(self, sample: DatasetSample) -> DatasetSample:
        """Ground truthë¥¼ í‰ê°€í•˜ê¸° ì‰¬ìš´ í˜•ì‹ìœ¼ë¡œ í‘œì¤€í™”"""

        question_lower = sample.question.lower()
        gt = sample.ground_truth

        # ê°œìˆ˜/ìˆ«ì ì§ˆë¬¸ -> ìˆ«ìë§Œ
        if any(word in question_lower for word in ["ìˆ˜ëŠ”", "ê°œìˆ˜", "ëª‡ ê°œ"]):
            if isinstance(gt, (list, dict)):
                sample.ground_truth = str(len(gt))
            elif isinstance(gt, int):
                sample.ground_truth = str(gt)
            elif isinstance(gt, str) and gt.isdigit():
                sample.ground_truth = gt
            else:
                numbers = re.findall(r"\d+", str(gt))
                sample.ground_truth = numbers[0] if numbers else "0"

        # ëª…ë ¹ì–´ ì§ˆë¬¸ -> ê°œí–‰ êµ¬ë¶„
        elif "ëª…ë ¹ì–´" in question_lower or "cli" in question_lower:
            if isinstance(gt, list):
                sample.ground_truth = "\n".join(gt)
            else:
                sample.ground_truth = str(gt).strip()

        # ëª©ë¡ ì§ˆë¬¸ -> ê³µë°± êµ¬ë¶„ ë¬¸ìì—´
        elif any(word in question_lower for word in ["ëª©ë¡", "ë¦¬ìŠ¤íŠ¸", "ì¥ë¹„ë“¤"]):
            if isinstance(gt, list):
                sample.ground_truth = " ".join(sorted(str(item).strip() for item in gt))
            elif isinstance(gt, str):
                items = re.split(r"[\s,\n]+", gt)
                sample.ground_truth = " ".join(sorted(item for item in items if item))

        # IP/ì¥ë¹„ëª… ë“± ë‹¨ì¼ ê°’
        elif any(word in question_lower for word in ["ip", "ì£¼ì†Œ", "ì¥ë¹„", "í˜¸ìŠ¤íŠ¸"]):
            if isinstance(gt, list):
                if gt:  # ë¦¬ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°
                    # ë‹¨ì¼ ê°’ì„ ê¸°ëŒ€í•˜ë¯€ë¡œ ì²« ë²ˆì§¸ ìš”ì†Œë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
                    sample.ground_truth = str(gt[0]).strip()
            else:
                # ë¬¸ìì—´, ìˆ«ì ë“± ë‹¤ë¥¸ íƒ€ì…ì€ ë¬¸ìì—´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.
                sample.ground_truth = str(gt).strip()

        # ìƒíƒœ/ì—¬ë¶€ ì§ˆë¬¸ -> ì •ê·œí™”
        elif any(word in question_lower for word in ["ìƒíƒœ", "ì—¬ë¶€", "ì ì ˆ", "ì •ìƒ"]):
            gt_str = str(gt).lower()
            if any(token in gt_str for token in ["ì •ìƒ", "ok", "true"]):
                sample.ground_truth = "ì •ìƒ"
            elif any(token in gt_str for token in ["ë¹„ì •ìƒ", "false", "ë¬¸ì œ"]):
                sample.ground_truth = "ë¹„ì •ìƒ"
            else:
                sample.ground_truth = "ì•Œ ìˆ˜ ì—†ìŒ"

        # ë³µì¡í•œ ê°ì²´ -> ì£¼ìš” ì •ë³´ë§Œ ì¶”ì¶œ
        elif isinstance(gt, dict):
            if "status" in gt:
                sample.ground_truth = gt["status"]
            elif "result" in gt:
                sample.ground_truth = gt["result"]
            elif len(gt) == 1:
                sample.ground_truth = next(iter(gt.values()))
            else:
                sample.ground_truth = "; ".join(f"{k}: {v}" for k, v in gt.items())
        return sample
    
    def _execute_stage_evaluation(self, samples: List[DatasetSample]) -> Dict[str, Any]:
        """6ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚° (ìê°€ í‰ê°€)"""
        self.logger.info("6ë‹¨ê³„: í‰ê°€ ë©”íŠ¸ë¦­ ê³„ì‚°")

        predictions = []
        for sample in samples:
            mock_prediction = self._generate_mock_prediction(sample)
            predictions.append({
                "predicted": mock_prediction,
                "ground_truth": sample.ground_truth,
                "question_id": sample.id,
                "answer_type": sample.answer_type,
            })

        eval_output = self.evaluator.evaluate_dataset(predictions)
        evaluation_data = eval_output.get("individual_results", [])

        # í‰ê°€ ê²°ê³¼ë¥¼ ê° ìƒ˜í”Œì— ë³‘í•©
        eval_map = {e["question_id"]: e for e in evaluation_data}
        for sample in samples:
            if sample.id in eval_map:
                sample.metadata = sample.metadata or {}
                sample.metadata["evaluation"] = eval_map[sample.id]
                sample.metadata["overall_score"] = eval_map[sample.id]["overall_score"]

        # ë³µì¡ë„ë³„ í†µê³„ ê³„ì‚°
        complexity_breakdown: Dict[str, Dict[str, int]] = {}
        for comp, comp_samples in getattr(self, "samples_by_complexity", {}).items():
            complexity_breakdown[comp] = {
                "total": len(comp_samples),
                "short": len([s for s in comp_samples if s.answer_type == "short"]),
                "long": len([s for s in comp_samples if s.answer_type == "long"]),
            }

        # ë°°ì¹˜ í†µê³„ ê³„ì‚°
        batch_stats = eval_output.get("overall_statistics", {})
        batch_stats.update({
            "sample_evaluation_count": len(evaluation_data),
            "total_dataset_size": len(samples),
            "category_distribution": self._calculate_category_distribution(samples),
            "complexity_distribution": self._calculate_complexity_distribution(samples),
            "complexity_breakdown": complexity_breakdown,
            "answer_type_distribution": {
                "short": len([s for s in samples if s.answer_type == "short"]),
                "long": len([s for s in samples if s.answer_type == "long"]),
            },
        })

        self.stage_results[PipelineStage.EVALUATION] = {
            "evaluation_data": evaluation_data,
            "batch_statistics": batch_stats,
            "success": True,
        }

        return {
            "sample_evaluations": evaluation_data,
            "dataset_statistics": batch_stats,
        }
    
    def _make_serializable(self, obj):
        """Enum ë“±ì„ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
        if hasattr(obj, 'value'):  # Enumì¸ ê²½ìš°
            return obj.value
        elif isinstance(obj, dict):
            return {k: self._make_serializable(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._make_serializable(item) for item in obj]
        elif hasattr(obj, '__dict__'):  # dataclassë‚˜ ê°ì²´ì¸ ê²½ìš°
            return {k: self._make_serializable(v) for k, v in obj.__dict__.items()}
        else:
            return obj

    def _save_intermediate(self, filename: str, data: Any):
        """ì¤‘ê°„ ê²°ê³¼ë¬¼ì„ output ë””ë ‰í† ë¦¬ì— ì €ì¥í•©ë‹ˆë‹¤."""
        if not self.config.save_intermediate:
            return
        output_path = Path(self.config.output_dir) / filename
        self.logger.info(f"ì¤‘ê°„ ê²°ê³¼ ì €ì¥ -> {output_path}")
        try:
            # JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜
            serializable_data = self._make_serializable(data)
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(serializable_data, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.error(f"'{filename}' ì €ì¥ ì‹¤íŒ¨: {e}")


    def _compose_final_dataset(
        self, 
        samples: List[DatasetSample],
        evaluation_results: Dict[str, Any]
    ) -> Dict[str, Any]:
        """ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±"""
        self.logger.info("ìµœì¢… ë°ì´í„°ì…‹ êµ¬ì„±")
        
        # ë°ì´í„°ì…‹ì„ train/validation/testë¡œ ë¶„í• 
        train_samples, val_samples, test_samples = self._split_dataset(samples)
        
        # PipelineConfig ë‚´ Enumì„ ì§ë ¬í™” ê°€ëŠ¥í•˜ë„ë¡ ë¬¸ìì—´ë¡œ ë³€í™˜
        cfg = asdict(self.config)
        def _enum_to_value_list(v):
            if isinstance(v, list):
                out = []
                for it in v:
                    try:
                        from enum import Enum as _Enum
                        out.append(it.value if isinstance(it, _Enum) else it)
                    except Exception:
                        out.append(getattr(it, "value", it))
                return out
            return v
        cfg["target_complexities"] = _enum_to_value_list(cfg.get("target_complexities"))
        cfg["target_personas"] = _enum_to_value_list(cfg.get("target_personas"))
        
        final_dataset = {
            "metadata": {
                "dataset_name": "NetworkConfigQA",
                "version": "1.0",
                "description": "LLM ë„¤íŠ¸ì›Œí¬ ì„¤ì • íŒŒì•… ì„±ëŠ¥ í‰ê°€ ë°ì´í„°ì…‹",
                "generation_config": cfg,
                "pipeline_results": {stage.value: result for stage, result in self.stage_results.items()},
                "total_samples": len(samples),
                "categories": list(set(s.category for s in samples)),
                "complexities": list(set(s.complexity for s in samples)),
                "complexity_counts": {k: len(v) for k, v in getattr(self, "samples_by_complexity", {}).items()},
                "answer_types": ["short", "long"],
            },
            "train": [asdict(s) for s in train_samples],
            "validation": [asdict(s) for s in val_samples],
            "test": [asdict(s) for s in test_samples],
            "evaluation_results": evaluation_results
        }
        
        return final_dataset
    
    # === ë³´ì¡° ë©”ì„œë“œë“¤ ===
    
    def _setup_logger(self):
        """íŒŒì¼ ë° ì½˜ì†” ë¡œê±°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤."""
        logger = logging.getLogger("DatasetGenerator")
        logger.setLevel(logging.DEBUG)  # DEBUG ë ˆë²¨ë¡œ ë³€ê²½
        if logger.hasHandlers():
            logger.handlers.clear()
        
        log_file = Path(self.config.output_dir) / 'pipeline.log'
        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')
        file_handler.setLevel(logging.DEBUG)  # íŒŒì¼ì—ëŠ” ëª¨ë“  ë¡œê·¸ ê¸°ë¡
        file_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(funcName)s:%(lineno)d - %(message)s'))
        logger.addHandler(file_handler)
        
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)  # ì½˜ì†”ì€ INFO ì´ìƒë§Œ
        console_handler.setFormatter(logging.Formatter('[%(levelname)s] %(message)s'))
        logger.addHandler(console_handler)
        
        return logger
    
    def _create_context(self, network_facts: Dict[str, Any], source_files: List[str]) -> str:
        """ì§ˆë¬¸ìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        if not source_files:
            return self._create_global_context(network_facts)
        
        # íŠ¹ì • íŒŒì¼ë“¤ì— ëŒ€í•œ ì»¨í…ìŠ¤íŠ¸
        devices = network_facts.get("devices", [])
        relevant_devices = [d for d in devices if d.get("file") in source_files]
        
        if not relevant_devices:
            return self._create_global_context(network_facts)
        
        context_parts = []
        for device in relevant_devices:
            device_info = [
                f"ì¥ë¹„: {device.get('system', {}).get('hostname', device.get('file', 'unknown'))}",
                f"OS: {device.get('vendor', 'unknown')}"
            ]
            
            # BGP ì •ë³´
            bgp = device.get('routing', {}).get('bgp', {})
            if bgp:
                device_info.append(f"BGP AS: {bgp.get('local_as', 'N/A')}")
                device_info.append(f"BGP í”¼ì–´ ìˆ˜: {len(bgp.get('neighbors', []))}")
            
            # VRF ì •ë³´
            vrfs = device.get('services', {}).get('vrf', [])
            if vrfs:
                device_info.append(f"VRF ìˆ˜: {len(vrfs)}")
            
            context_parts.append(" | ".join(device_info))
        
        return "\n".join(context_parts)
    
    def _create_global_context(self, network_facts: Dict[str, Any]) -> str:
        """ì „ì—­ ë„¤íŠ¸ì›Œí¬ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        devices = network_facts.get("devices", [])
        
        summary = [
            f"ì´ ì¥ë¹„ ìˆ˜: {len(devices)}",
            f"ì‚¬ìš© ê¸°ìˆ : BGP, OSPF, L2VPN, L3VPN"
        ]
        
        # AS ì •ë³´
        as_numbers = set()
        for device in devices:
            las = device.get('routing', {}).get('bgp', {}).get('local_as')
            if las:
                as_numbers.add(str(las))
        
        if as_numbers:
            summary.append(f"AS ë²ˆí˜¸: {', '.join(sorted(as_numbers))}")
        
        return " | ".join(summary)
    
    def _format_answer(self, expected_answer: Dict[str, Any]) -> Tuple[Any, str]:
        """Extract ground truth and explanation from expected_answer."""
        if not expected_answer:
            return None, ""
        ground_truth = expected_answer.get("ground_truth")
        explanation = expected_answer.get("explanation", "")
        if ground_truth is None:
            ground_truth = expected_answer.get("value")
        return ground_truth, explanation
    
    def _validate_sample_quality(self, sample: DatasetSample) -> bool:
        """ìƒ˜í”Œ í’ˆì§ˆì„ ë”ìš± ì—„ê²©í•˜ê²Œ ê²€ì¦í•©ë‹ˆë‹¤."""
        if not all([sample.question, sample.ground_truth, sample.category]):
            self.logger.debug(f"í’ˆì§ˆ ë¯¸ë‹¬ (í•„ìˆ˜ í•„ë“œ ëˆ„ë½): {sample.id}")
            return False
        if len(sample.question.strip()) < 15:
            self.logger.debug(f"í’ˆì§ˆ ë¯¸ë‹¬ (ë„ˆë¬´ ì§§ì€ ì§ˆë¬¸): {sample.question}")
            return False

        question_lower = sample.question.lower()

        generic_patterns = [
            "ë€ ë¬´ì—‡ì¸ê°€",
            "ë¬´ì—‡ì…ë‹ˆê¹Œ",
            "ì„¤ëª…í•˜ì‹œì˜¤",
            "ì•Œë ¤ì¤˜",
            "ì–´ë–»ê²Œ ìƒê°í•´",
            "ìš”ì•½í•´ì¤˜",
        ]
        if any(pattern in question_lower for pattern in generic_patterns):
            self.logger.debug(f"í’ˆì§ˆ ë¯¸ë‹¬ (ë„ˆë¬´ ì¼ë°˜ì ì„): {sample.question}")
            return False

        gt_str = str(sample.ground_truth)
        if gt_str.isdigit():
            if not any(word in question_lower for word in ["ëª‡ ê°œ", "ìˆ˜", "ì–¼ë§ˆë‚˜", "ê°œìˆ˜", "ëª‡ ëª…"]):
                self.logger.debug(
                    f"í’ˆì§ˆ ë¯¸ë‹¬ (ë‹µë³€<->ì§ˆë¬¸ ë¶ˆì¼ì¹˜): Q='{sample.question}', A='{gt_str}'"
                )
                return False

        if "ìƒíƒœëŠ” ì–´ë–¤ê°€" in question_lower:
            if gt_str not in ["ì •ìƒ", "ë¹„ì •ìƒ", "ì–‘í˜¸", "ë¶ˆëŸ‰", "í™œì„±", "ë¹„í™œì„±"]:
                self.logger.debug(
                    f"í’ˆì§ˆ ë¯¸ë‹¬ (ëª¨í˜¸í•œ ìƒíƒœ ì§ˆë¬¸): Q='{sample.question}', A='{gt_str}'"
                )
                return False

        return True

    def _determine_answer_type(self, answer: Any) -> str:
        """ë‹¨ì–´ ìˆ˜ ê¸°ë°˜ answer type ê²°ì •"""
        tokens = str(answer).split()
        return "short" if len(tokens) <= self.config.short_answer_threshold else "long"

    def _reclassify_answer_type(self, sample: DatasetSample) -> str:
        """ë‹µë³€ íƒ€ì… ì¬ë¶„ë¥˜"""
        return self._determine_answer_type(sample.ground_truth)
    
    def _enrich_sample_metadata(self, sample: DatasetSample) -> DatasetSample:
        """ìƒ˜í”Œ ë©”íƒ€ë°ì´í„° ë³´ê°•"""
        if sample.metadata is None:
            sample.metadata = {}
        
        # ì§ˆë¬¸ íŠ¹ì„± ë¶„ì„
        sample.metadata["question_length"] = len(sample.question)
        sample.metadata["answer_length"] = len(str(sample.ground_truth))
        sample.metadata["has_numbers"] = bool(re.search(r'\d+', sample.question))
        sample.metadata["has_technical_terms"] = bool(re.search(r'\b(BGP|OSPF|VRF|L2VPN|SSH|AAA)\b', sample.question, re.IGNORECASE))
        
        return sample
    
    def _balance_categories(self, samples: List[DatasetSample]) -> List[DatasetSample]:
        """ì¹´í…Œê³ ë¦¬ë³„ ê· í˜• ì¡°ì • (ê¸°ë³¸ ë¹„í™œì„±í™”). 
        - ê·¸ë£¹ í‚¤: metadata.topic ìš°ì„ , ì—†ìœ¼ë©´ sample.category
        - ìƒí•œ: config.balance_max_per_category(Noneì´ë©´ ì»· ì—†ìŒ)
        """
        max_per_category = getattr(self.config, "balance_max_per_category", None)
        # ì»· ë¹„í™œì„±í™”: ê·¸ëŒ€ë¡œ ë°˜í™˜
        if max_per_category is None or max_per_category <= 0:
            return samples

        # ê·¸ë£¹í•‘: topic ìš°ì„ 
        group_key_pref = getattr(self.config, "balance_group_key", "topic")
        by_group: Dict[str, List[DatasetSample]] = {}
        for sample in samples:
            topic = None
            if group_key_pref == "topic":
                meta = sample.metadata or {}
                topic = meta.get("topic")
            key = topic or sample.category or "unknown"
            by_group.setdefault(key, []).append(sample)

        balanced: List[DatasetSample] = []
        for key, items in by_group.items():
            # ë³µì¡ë„ ë‹¤ì–‘ì„± ìœ ì§€: ê· ë“± ë¶„ë°°
            by_complexity: Dict[str, List[DatasetSample]] = {}
            for s in items:
                by_complexity.setdefault(s.complexity or "", []).append(s)
            share = max(1, max_per_category // max(1, len(by_complexity)))
            selected: List[DatasetSample] = []
            for comp, comp_items in by_complexity.items():
                selected.extend(comp_items[:share])
            balanced.extend(selected[:max_per_category])

        return balanced
    
    def _create_enhanced_context(self, network_facts: Dict[str, Any], sample: DatasetSample) -> str:
        """í–¥ìƒëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±"""
        # ì§ˆë¬¸ ë‚´ìš©ì— ë§ëŠ” íŠ¹í™”ëœ ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        question_lower = sample.question.lower()
        
        if "bgp" in question_lower:
            return self._create_bgp_context(network_facts)
        elif "ssh" in question_lower or "ë³´ì•ˆ" in question_lower:
            return self._create_security_context(network_facts)
        elif "vrf" in question_lower or "l3vpn" in question_lower:
            return self._create_vrf_context(network_facts)
        else:
            return self._create_global_context(network_facts)
    
    def _create_bgp_context(self, network_facts: Dict[str, Any]) -> str:
        """BGP íŠ¹í™” ì»¨í…ìŠ¤íŠ¸"""
        devices = network_facts.get("devices", [])
        bgp_info = []
        
        for device in devices:
            bgp = device.get('routing', {}).get('bgp', {})
            if bgp:
                hostname = device.get('system', {}).get('hostname', device.get('file', 'unknown'))
                local_as = bgp.get('local_as', 'N/A')
                neighbor_count = len(bgp.get('neighbors', []))
                bgp_info.append(f"{hostname}: AS{local_as}, {neighbor_count}ê°œ í”¼ì–´")
        
        return "BGP ì„¤ì • í˜„í™©:\n" + "\n".join(bgp_info[:5])  # ìµœëŒ€ 5ê°œ
    
    def _create_security_context(self, network_facts: Dict[str, Any]) -> str:
        """ë³´ì•ˆ íŠ¹í™” ì»¨í…ìŠ¤íŠ¸"""
        devices = network_facts.get("devices", [])
        security_info = []
        
        for device in devices:
            hostname = device.get('system', {}).get('hostname', device.get('file', 'unknown'))
            ssh_enabled = device.get('security', {}).get('ssh', {}).get('present', False)
            aaa_enabled = device.get('security', {}).get('aaa', {}).get('present', False)
            security_info.append(f"{hostname}: SSH {'ON' if ssh_enabled else 'OFF'}, AAA {'ON' if aaa_enabled else 'OFF'}")
        
        return "ë³´ì•ˆ ì„¤ì • í˜„í™©:\n" + "\n".join(security_info[:5])
    
    def _create_vrf_context(self, network_facts: Dict[str, Any]) -> str:
        """VRF íŠ¹í™” ì»¨í…ìŠ¤íŠ¸"""
        devices = network_facts.get("devices", [])
        vrf_info = []
        
        for device in devices:
            hostname = device.get('system', {}).get('hostname', device.get('file', 'unknown'))
            vrfs = device.get('services', {}).get('vrf', [])
            vrf_count = len(vrfs)
            if vrf_count > 0:
                vrf_names = [v.get('name', 'unnamed') for v in vrfs[:3]]
                vrf_info.append(f"{hostname}: {vrf_count}ê°œ VRF ({', '.join(vrf_names)})")
        
        return "VRF ì„¤ì • í˜„í™©:\n" + "\n".join(vrf_info[:5])
    
    def _generate_mock_prediction(self, sample: DatasetSample) -> str:
        """ëª¨ì˜ LLM ë‹µë³€ ìƒì„± (í‰ê°€ìš©)"""
        # ì‹¤ì œë¡œëŠ” ì™¸ë¶€ LLMì— ì§ˆë¬¸ì„ ë³´ë‚´ì„œ ë‹µë³€ì„ ë°›ì•„ì•¼ í•¨
        # ì—¬ê¸°ì„œëŠ” ê°„ë‹¨í•œ ëª¨ì˜ ë‹µë³€ ìƒì„±
        
        if sample.answer_type == "short":
            answer_text = str(sample.ground_truth)
            if "ê°œ" in answer_text or "ìˆ˜" in answer_text:
                try:
                    num = re.search(r'\d+', answer_text)
                    if num:
                        original = int(num.group())
                        predicted = max(0, original + (1 if original % 2 == 0 else -1))
                        return str(predicted)
                except:
                    pass

            if answer_text in ["ì˜ˆ", "ì•„ë‹ˆì˜¤"]:
                return "ì•„ë‹ˆì˜¤" if answer_text == "ì˜ˆ" else "ì˜ˆ"

        # ê¸°ë³¸ì ìœ¼ë¡œ ì •ë‹µ ë°˜í™˜ (ì•½ê°„ì˜ í‘œí˜„ ë³€ê²½)
        answer = str(sample.ground_truth)
        replacements = {
            "ì˜ˆ": "yes",
            "ì•„ë‹ˆì˜¤": "no", 
            "ì—†ìŒ": "None",
            "ì •ë³´ ì—†ìŒ": "N/A"
        }
        
        for old, new in replacements.items():
            answer = answer.replace(old, new)
        
        return answer
    
    def _calculate_category_distribution(self, samples: List[DatasetSample]) -> Dict[str, int]:
        """ì¹´í…Œê³ ë¦¬ ë¶„í¬ ê³„ì‚°"""
        distribution = {}
        for sample in samples:
            distribution[sample.category] = distribution.get(sample.category, 0) + 1
        return distribution
    
    def _calculate_complexity_distribution(self, samples: List[DatasetSample]) -> Dict[str, int]:
        """ë³µì¡ë„ ë¶„í¬ ê³„ì‚°"""
        distribution = {}
        for sample in samples:
            distribution[sample.complexity] = distribution.get(sample.complexity, 0) + 1
        return distribution
    
    def _split_dataset(self, samples: List[DatasetSample]) -> Tuple[List[DatasetSample], List[DatasetSample], List[DatasetSample]]:
        """ë°ì´í„°ì…‹ ë¶„í•  (train/val/test)"""
        import random
        random.seed(42)  # ì¬í˜„ ê°€ëŠ¥í•œ ë¶„í• 
        
        shuffled = samples.copy()
        random.shuffle(shuffled)
        
        total = len(shuffled)
        train_size = int(total * 0.7)
        val_size = int(total * 0.15)
        
        train = shuffled[:train_size]
        val = shuffled[train_size:train_size + val_size]
        test = shuffled[train_size + val_size:]
        
        return train, val, test
    
    def _save_intermediate(self, filename: str, data: Any) -> None:
        """ì¤‘ê°„ ê²°ê³¼ ì €ì¥"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        filepath = output_dir / filename
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
    
    def _save_results(self, final_dataset: Dict[str, Any]) -> None:
        """ìµœì¢… ê²°ê³¼ ì €ì¥"""
        output_dir = Path(self.config.output_dir)
        output_dir.mkdir(exist_ok=True)
        
        # ì „ì²´ ë°ì´í„°ì…‹ ì €ì¥
        with open(output_dir / "network_config_qa_dataset.json", 'w', encoding='utf-8') as f:
            json.dump(final_dataset, f, ensure_ascii=False, indent=2)
        
        # ë¶„í• ëœ ë°ì´í„°ì…‹ ê°œë³„ ì €ì¥
        for split_name in ["train", "validation", "test"]:
            if split_name in final_dataset:
                with open(output_dir / f"{split_name}.json", 'w', encoding='utf-8') as f:
                    json.dump(final_dataset[split_name], f, ensure_ascii=False, indent=2)
        
        # ë©”íƒ€ë°ì´í„°ë§Œ ë³„ë„ ì €ì¥
        with open(output_dir / "metadata.json", 'w', encoding='utf-8') as f:
            json.dump(final_dataset["metadata"], f, ensure_ascii=False, indent=2)
        
        self.logger.info(f"ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_dir}")

    def _execute_pre_validation(self, network_facts: Dict[str, Any], samples: List[DatasetSample]) -> List[DatasetSample]:
        """í”„ë¦¬â€‘ë°¸ë¦¬ë°ì´ì…˜: BASIC/Rule ê¸°ë°˜ í•­ëª©ì— ëŒ€í•´ Logic vs GT ê²€ì¦ ë° ìë™ êµì •.
        - ëŒ€ìƒ: category=='basic' ë˜ëŠ” metadata.origin=='rule_based'
        - intentê°€ ìˆëŠ” í•­ëª©ë§Œ ì²˜ë¦¬
        """
        from utils.builder_core import BuilderCore

        builder = BuilderCore(network_facts.get("devices", []))

        def _cmp(a, b) -> bool:
            if a is None or b is None:
                return a == b
            # ìˆ«ì
            try:
                fa = float(a); fb = float(b)
                return abs(fa - fb) <= max(1.0, abs(fb) * 0.01)
            except Exception:
                pass
            # ë¶ˆë¦° í…ìŠ¤íŠ¸ ë™ë“±ì„±
            tmap = {"true": True, "false": False, "yes": True, "no": False, "í™œì„±": True, "ë¹„í™œì„±": False, "ì •ìƒ": True}
            def _t(x):
                xs = str(x).strip().lower();
                return tmap.get(xs, None)
            ta, tb = _t(a), _t(b)
            if ta is not None and tb is not None:
                return ta == tb
            # ë¦¬ìŠ¤íŠ¸/ì§‘í•©
            if isinstance(a, list) and isinstance(b, list):
                return set(map(str, a)) == set(map(str, b))
            # ë¬¸ìì—´ ì •ê·œí™” ë¹„êµ
            return str(a).strip().lower() == str(b).strip().lower()

        corrected = 0; checked = 0
        out: List[DatasetSample] = []
        for s in samples:
            out.append(s)
            if (s.category or "").lower() != "basic" and not ((s.metadata or {}).get("origin") == "rule_based"):
                continue
            intent = ((s.metadata or {}).get("intent")) or {}
            metric = intent.get("metric"); params = intent.get("params") or intent.get("scope") or {}
            if not metric:
                continue
            try:
                checked += 1
                logic_val, _files = builder.calculate_metric(metric, params)
                if not _cmp(s.ground_truth, logic_val):
                    s.metadata = s.metadata or {}
                    s.metadata["correction_log"] = {
                        "reason": "pre-validation logic vs GT mismatch",
                        "old_ground_truth": s.ground_truth,
                        "new_ground_truth": logic_val,
                        "metric": metric,
                        "params": params,
                    }
                    s.ground_truth = logic_val
                    corrected += 1
            except Exception:
                continue

        self.logger.info(f"í”„ë¦¬â€‘ë°¸ë¦¬ë°ì´ì…˜: ê²€ì‚¬ {checked}ê°œ, ìë™ êµì • {corrected}ê°œ")

        if self.config.save_intermediate:
            try:
                self._save_intermediate("prevalidation_corrections.json", [
                    {
                        "id": s.id,
                        "question": s.question,
                        "correction_log": (s.metadata or {}).get("correction_log")
                    }
                    for s in out if (s.metadata or {}).get("correction_log")
                ])
            except Exception:
                pass
        return out
    
    def _dict_to_dataset_sample(self, data: Dict[str, Any]) -> DatasetSample:
        """ë”•ì…”ë„ˆë¦¬ë¥¼ DatasetSample ê°ì²´ë¡œ ë³€í™˜"""
        return DatasetSample(
            id=data.get("id", ""),
            question=data.get("question", ""),
            context=data.get("context", ""),
            ground_truth=data.get("ground_truth"),
            explanation=data.get("explanation", ""),
            answer_type=data.get("answer_type", "short"),
            category=data.get("category", ""),
            complexity=data.get("complexity", "basic"),
            level=data.get("level", 1),
            persona=data.get("persona"),
            scenario=data.get("scenario"),
            source_files=data.get("source_files", []),
            metadata=data.get("metadata", {})
        )


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # policies.jsonì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìë™ ì¶”ì¶œ
    def get_all_categories(policies_path: str) -> List[str]:
        """policies.jsonì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ"""
        import json
        with open(policies_path, 'r', encoding='utf-8') as f:
            policies_data = json.load(f)
        
        categories = set()
        for policy in policies_data.get("policies", []):
            category = policy.get("category")
            if category:
                categories.add(category)
        
        return sorted(list(categories))
    
    # ì„¤ì •
    policies_path = "policies.json"
    all_categories = get_all_categories(policies_path)

    config = PipelineConfig(
        xml_data_dir="data/raw/XML_Data",
        policies_path=policies_path,
        target_categories=all_categories,  # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ìë™ í¬í•¨
        basic_questions_per_category=30,  # ëŒ€í­ ì¦ê°€: ì¹´í…Œê³ ë¦¬ë‹¹ 30ê°œ
        enhanced_questions_per_category=50,  # ì•ˆì •ì ì¸ ìˆ˜ì¹˜: ì¹´í…Œê³ ë¦¬ë‹¹ 20ê°œ
        target_complexities=[
            QuestionComplexity.BASIC,         # ê¸°ë³¸
            QuestionComplexity.ANALYTICAL,    # ë¶„ì„ì  ì¶”ë¡ 
            QuestionComplexity.DIAGNOSTIC,    # ë¬¸ì œ ì§„ë‹¨
            QuestionComplexity.SCENARIO,      # ì‹œë‚˜ë¦¬ì˜¤ ê¸°ë°˜
            QuestionComplexity.SYNTHETIC      # í†µí•© ë¶„ì„
        ],
        target_personas=[
            PersonaType.NETWORK_ENGINEER,
            PersonaType.SECURITY_AUDITOR,
            PersonaType.NOC_OPERATOR,
            PersonaType.ARCHITECT,
            PersonaType.TROUBLESHOOTER,
            PersonaType.COMPLIANCE_OFFICER,
            # PersonaType.AUTOMATION_ENGINEER
        ],
        output_dir="output"
    )
    
    # ì¶”ì¶œëœ ì¹´í…Œê³ ë¦¬ ë¡œê·¸ ì¶œë ¥
    print(f"ìë™ ì¶”ì¶œëœ ì¹´í…Œê³ ë¦¬: {all_categories}")
    
    # ë°ì´í„°ì…‹ ìƒì„±ê¸° ì´ˆê¸°í™” ë° ì‹¤í–‰
    generator = NetworkConfigDatasetGenerator(config)
    
    try:
        dataset = generator.generate_complete_dataset()
        
        print("\n=== ë°ì´í„°ì…‹ ìƒì„± ì™„ë£Œ ===")
        print(f"ì´ ìƒ˜í”Œ ìˆ˜: {dataset['metadata']['total_samples']}")
        print(f"Train: {len(dataset['train'])}")
        print(f"Validation: {len(dataset['validation'])}")
        print(f"Test: {len(dataset['test'])}")
        print(f"ì¹´í…Œê³ ë¦¬: {dataset['metadata']['categories']}")
        print(f"ë³µì¡ë„: {dataset['metadata']['complexities']}")
        
    except Exception as e:
        print(f"ë°ì´í„°ì…‹ ìƒì„± ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    main()
