# -*- coding: utf-8 -*-
"""
LLM Adapter (OpenAI) â€” robust JSON with multi-stage fallbacks.

- Prefers Responses API Structured Outputs (text.format)
- If Responses fails (e.g., 400 on formatting), fallback to Chat Completions
  * Chat path: json_schema â†’ json_object
- Final fallback: Chat tools(function calling) with an OBJECT-wrapped schema
- For GPT-5 Chat, use max_completion_tokens (not max_tokens)
"""

from __future__ import annotations
from typing import List, Dict, Any, Optional
import json, os, time, re
from dataclasses import dataclass

from utils.config_manager import get_settings

try:
    from openai import OpenAI
except Exception as e:
    raise RuntimeError("Install OpenAI SDK: `pip install openai`") from e


@dataclass
class _ClientConfig:
    api_key: Optional[str]
    base_url: Optional[str]
    org_id: Optional[str]
    project: Optional[str]
    timeout: float
    max_retries: int

    @classmethod
    def from_settings(cls) -> "_ClientConfig":
        api = get_settings().api
        return cls(
            api_key=api.api_key,
            base_url=api.base_url,
            org_id=api.org_id,
            project=api.project,
            timeout=api.timeout,
            max_retries=api.max_retries,
        )


def _build_client() -> OpenAI:
    cfg = _ClientConfig.from_settings()
    if not cfg.api_key:
        raise RuntimeError("OPENAI_API_KEY is not set.")
    return OpenAI(
        api_key=cfg.api_key,
        base_url=cfg.base_url or None,
        organization=cfg.org_id or None,
        project=cfg.project or None,
        timeout=cfg.timeout,
        max_retries=0,  # we manage retries ourselves
    )


def _ensure_schema_strict(schema: Dict[str, Any]) -> Dict[str, Any]:
    """Recursively apply additionalProperties=False for objects."""
    def _walk(node: Any):
        if isinstance(node, dict):
            t = node.get("type")
            if t == "object":
                # Force strictness on every object node
                node["additionalProperties"] = False
                props = node.get("properties", {})
                if isinstance(props, dict):
                    for _, v in list(props.items()):
                        _walk(v)
            elif t == "array":
                if "items" in node:
                    _walk(node["items"])
    # make a deep copy
    data = json.loads(json.dumps(schema, ensure_ascii=False))
    _walk(data)
    return data


def _extract_output_text(resp: Any) -> str:
    """Try various SDK fields to extract text."""
    txt = getattr(resp, "output_text", None)
    if isinstance(txt, str) and txt.strip():
        return txt

    out = getattr(resp, "output", None)
    if isinstance(out, list) and out:
        pieces = []
        for item in out:
            val = getattr(item, "text", None)
            if isinstance(val, str) and val:
                pieces.append(val)
            content = getattr(item, "content", None)
            if isinstance(content, str):
                pieces.append(content)
            elif isinstance(content, list):
                for c in content:
                    val = getattr(c, "text", None) or getattr(c, "content", None)
                    if isinstance(val, str) and val:
                        pieces.append(val)
        if pieces:
            return "\n".join(pieces).strip()

    choices = getattr(resp, "choices", None)
    if isinstance(choices, list) and choices:
        msg = getattr(choices[0], "message", None) or {}
        content = getattr(msg, "content", None)
        if isinstance(content, str) and content.strip():
            return content

    return ""


def _safe_json_loads(s: str) -> Any:
    s = (s or "").strip()
    if not s:
        raise ValueError("Empty model response; cannot parse JSON.")
    try:
        return json.loads(s)
    except Exception:
        # try largest object
        first = s.find("{"); last = s.rfind("}")
        if first != -1 and last != -1 and last > first:
            return json.loads(s[first:last+1])
        # try largest array
        first = s.find("["); last = s.rfind("]")
        if first != -1 and last != -1 and last > first:
            return json.loads(s[first:last+1])
        raise


def _extract_json_from_codeblock(s: str) -> Optional[str]:
    if not s:
        return None
    m = re.search(r"```json\s*([\s\S]*?)```", s, flags=re.MULTILINE)
    if m: return m.group(1).strip()
    m = re.search(r"```\s*([\s\S]*?)```", s, flags=re.MULTILINE)
    if m: return m.group(1).strip()
    return None


def _retry_backoff(attempt: int, base: float = 0.8, cap: float = 6.0) -> float:
    delay = min(cap, base * (2 ** attempt))
    return delay * (0.8 + 0.4 * (os.urandom(1)[0] / 255.0))


def _schema_has_optional_props(schema: Dict[str, Any]) -> bool:
    """Detect if any object node declares optional properties (i.e., properties keys not all in required).
    If so, Responses strict mode may reject the schema; return True to suggest relaxing.
    """
    def _walk(node: Any) -> bool:
        if isinstance(node, dict):
            t = node.get("type")
            if t == "object":
                props = node.get("properties", {})
                req = set(node.get("required", []))
                if isinstance(props, dict) and props:
                    keys = set(props.keys())
                    if not req or (keys - req):
                        return True
                    # Recurse into children
                    for v in props.values():
                        if _walk(v):
                            return True
            elif t == "array":
                if "items" in node and _walk(node["items"]):
                    return True
        return False
    try:
        data = json.loads(json.dumps(schema, ensure_ascii=False))
    except Exception:
        data = schema
    return _walk(data)


def _call_llm_json(
    messages: List[Dict[str, str]],
    schema: Dict[str, Any],
    temperature: float = 0.6,
    *,
    model: Optional[str] = None,
    max_output_tokens: int = 10000,
    use_responses_api: Optional[bool] = None,
    prefer_object: bool = True,
    use_chat_parse: bool = False,
    pydantic_model: Optional[Any] = None,
) -> Any:
    """
    Robust JSON caller with detailed debugging
    """
    client = _build_client()
    strict_schema = _ensure_schema_strict(schema)
    chosen_model = model or get_settings().models.paraphrase
    cfg = _ClientConfig.from_settings()
    attempts = cfg.max_retries + 1

    # ğŸ” ë””ë²„ê¹…: ëª¨ë¸ ì •ë³´ ì¶œë ¥
    print(f"\n[DEBUG] ========== LLM CALL DEBUG ==========")
    print(f"[DEBUG] Model requested: {chosen_model}")
    print(f"[DEBUG] API Key present: {bool(cfg.api_key)}")
    print(f"[DEBUG] API Key prefix: {cfg.api_key[:10] if cfg.api_key else 'None'}...")
    
    def log_header(api: str):
        print(f"[LLM] call start | api={api} | model={chosen_model} | temp={temperature} | max_out={max_output_tokens} | schema={strict_schema.get('title','(no-title)')} | messages={len(messages)}")
        for i, m in enumerate(messages[:3]):
            prev = (m.get("content") or "")[:240].replace("\n", " ")
            print(f"  - msg[{i}] role={m.get('role')} len={len(m.get('content') or '')} preview={prev}")

    # Responses API ì‚¬ìš© ì—¬ë¶€ ê²°ì •
    try:
        s = get_settings()
        if use_responses_api is None:
            use_responses_api = bool(
                isinstance(chosen_model, str)
                and chosen_model.startswith("gpt-5")
                and getattr(s.features, "use_responses_api_for_gpt5", True)
            )
        else:
            use_responses_api = bool(use_responses_api)
    except Exception as e:
        print(f"[DEBUG] Error checking settings: {e}")
        use_responses_api = bool(use_responses_api)

    print(f"[DEBUG] use_responses_api: {use_responses_api}")
    
    relax_needed = _schema_has_optional_props(schema)

    if use_responses_api:
        for attempt in range(1, attempts + 1):
            try:
                log_header("Responses")
                
                # messages ë³€í™˜
                if isinstance(messages, list):
                    input_text = "\n".join([
                        f"{msg.get('role', 'user')}: {msg.get('content', '')}"
                        for msg in messages
                    ])
                else:
                    input_text = messages
                
                # ğŸ” ë””ë²„ê¹…: ì…ë ¥ ë‚´ìš© ì¶œë ¥
                print(f"[DEBUG] Input text length: {len(input_text)}")
                print(f"[DEBUG] Input text preview: {input_text[:200]}...")
                
                # text íŒŒë¼ë¯¸í„° êµ¬ì„±
                text_config = {
                    "format": {
                        "type": "json_schema",
                        "name": strict_schema.get("title", "structured_output"),
                        "strict": False if relax_needed else True,
                        "schema": strict_schema,
                    }
                }
                
                # verbosity ì¶”ê°€
                try:
                    if chosen_model.startswith("gpt-5"):
                        text_config["verbosity"] = s.features.gpt5_text_verbosity
                except:
                    pass
                
                # reasoning íŒŒë¼ë¯¸í„°
                reasoning_config = {}
                try:
                    if chosen_model.startswith("gpt-5"):
                        reasoning_config = {"effort": s.features.gpt5_reasoning_effort}
                except:
                    reasoning_config = {"effort": "minimal"}
                
                # ğŸ” ë””ë²„ê¹…: API í˜¸ì¶œ íŒŒë¼ë¯¸í„° ì¶œë ¥
                print(f"[DEBUG] API Call Parameters:")
                print(f"  - model: {chosen_model}")
                print(f"  - input type: {type(input_text)}")
                print(f"  - text config: {text_config}")
                print(f"  - reasoning: {reasoning_config}")
                print(f"  - max_output_tokens: {max_output_tokens}")
                
                # API í˜¸ì¶œ
                resp = client.responses.create(
                    model=chosen_model,
                    input=input_text,
                    text=text_config,
                    reasoning=reasoning_config,
                    max_output_tokens=max_output_tokens,
                )
                
                # ğŸ” ë””ë²„ê¹…: ì‘ë‹µ ê°ì²´ ìƒì„¸ ë¶„ì„
                print(f"[DEBUG] Response type: {type(resp)}")
                print(f"[DEBUG] Response dir: {dir(resp)}")
                print(f"[DEBUG] Response attributes:")
                for attr in dir(resp):
                    if not attr.startswith('_'):
                        try:
                            val = getattr(resp, attr)
                            print(f"  - {attr}: {type(val)} = {str(val)[:100] if val else 'None'}")
                        except:
                            print(f"  - {attr}: <unable to access>")
                
                # ì‘ë‹µì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ ì‹œë„
                text = _extract_output_text(resp)
                
                # ğŸ” ë””ë²„ê¹…: ì¶”ì¶œëœ í…ìŠ¤íŠ¸ ì •ë³´
                print(f"[DEBUG] Extracted text type: {type(text)}")
                print(f"[DEBUG] Extracted text length: {len(text) if text else 0}")
                print(f"[DEBUG] Extracted text: {text[:200] if text else 'None'}")
                
                if not isinstance(text, str) or not text.strip():
                    raise ValueError(f"Empty output_text from model. Response object: {resp}")
                    
                print(f"[LLM] raw text len={len(text)}")
                return _safe_json_loads(_extract_json_from_codeblock(text) or text)
                
            except Exception as e:
                print(f"[LLM] error on attempt {attempt}: {e}")
                print(f"[DEBUG] Full error: {repr(e)}")
                import traceback
                print(f"[DEBUG] Traceback:\n{traceback.format_exc()}")
                
                if attempt >= attempts:
                    print("[LLM] fall back to Chat API")
                else:
                    time.sleep(_retry_backoff(attempt-1))

    # 2) Chat API
    for attempt in range(1, attempts + 1):
        try:
            log_header("Chat")
            messages_json = list(messages)
            
            # ğŸ” ë””ë²„ê¹…: Chat API ì‹œë„
            print(f"[DEBUG] Trying Chat API with model: {chosen_model}")
            
            # JSON í‚¤ì›Œë“œ ì¶”ê°€
            try:
                has_json_kw = any(
                    isinstance(m, dict) and isinstance(m.get("content"), str) and ("json" in m["content"].lower())
                    for m in messages_json
                )
            except Exception:
                has_json_kw = False
                
            if not has_json_kw:
                messages_json.append({
                    "role": "developer",
                    "content": (
                        "Return only JSON. Output a single valid JSON object or array that follows the intent of the schema. "
                        "Do not include prose, markdown, or code fences. json"
                    )
                })

            extra: Dict[str, Any] = {}

            # GPT-5ìš© íŠ¹ë³„ ì²˜ë¦¬
            if isinstance(chosen_model, str) and chosen_model.startswith("gpt-5"):
                # ğŸ” ë””ë²„ê¹…: GPT-5 Chat API í˜¸ì¶œ
                print(f"[DEBUG] GPT-5 Chat API call")
                print(f"[DEBUG] Using max_completion_tokens instead of max_tokens")
                
                try:
                    resp = client.chat.completions.create(
                        model=chosen_model,
                        messages=messages_json,
                        response_format={"type": "json_object"},
                        max_completion_tokens=max_output_tokens,
                        **extra,
                    )
                except Exception as api_error:
                    print(f"[DEBUG] GPT-5 Chat API error: {api_error}")
                    print(f"[DEBUG] Error type: {type(api_error)}")
                    print(f"[DEBUG] Error details: {repr(api_error)}")
                    raise
            else:
                resp = client.chat.completions.create(
                    model=chosen_model,
                    messages=messages_json,
                    response_format={"type": "json_object"},
                    max_tokens=max_output_tokens,
                    temperature=temperature,
                    **extra,
                )
            
            # ğŸ” ë””ë²„ê¹…: Chat API ì‘ë‹µ ë¶„ì„
            print(f"[DEBUG] Chat response type: {type(resp)}")
            print(f"[DEBUG] Chat response attributes: {dir(resp)}")
            if hasattr(resp, 'choices') and resp.choices:
                print(f"[DEBUG] Choices count: {len(resp.choices)}")
                if resp.choices[0].message:
                    print(f"[DEBUG] Message content present: {bool(resp.choices[0].message.content)}")
                    
            text = _extract_output_text(resp)
            
            if not isinstance(text, str) or not text.strip():
                raise ValueError(f"Empty output_text from model. Full response: {resp}")
                
            print(f"[LLM] raw text len={len(text)}")
            payload = _extract_json_from_codeblock(text) or text
            
            try:
                return _safe_json_loads(payload)
            except Exception as json_error:
                print(f"[DEBUG] JSON parsing error: {json_error}")
                # JSON ì¶”ì¶œ ì¬ì‹œë„
                import re
                m = re.search(r"\[\s*\{[\s\S]*\}\s*\]", payload)
                if m:
                    return _safe_json_loads(m.group(0))
                m = re.search(r"\{[\s\S]*\}", payload)
                if m:
                    return _safe_json_loads(m.group(0))
                raise
                
        except Exception as e:
            print(f"[LLM] error on attempt {attempt}: {e}")
            print(f"[DEBUG] Full Chat API error: {repr(e)}")
            if attempt >= attempts:
                print("[LLM] final fallback â†’ tools(function calling)")
            else:
                time.sleep(_retry_backoff(attempt-1))
    """
    Robust JSON caller:
      Responses(JSON schema) â†’ Chat(json_schema/json_object) â†’ Chat tools(function calling)
    """
    client = _build_client()
    strict_schema = _ensure_schema_strict(schema)
    chosen_model = model or get_settings().models.paraphrase
    cfg = _ClientConfig.from_settings()
    attempts = cfg.max_retries + 1

    def log_header(api: str):
        print(f"[LLM] call start | api={api} | model={chosen_model} | temp={temperature} | max_out={max_output_tokens} | schema={strict_schema.get('title','(no-title)')} | messages={len(messages)}")
        for i, m in enumerate(messages[:3]):
            prev = (m.get("content") or "")[:240].replace("\n", " ")
            print(f"  - msg[{i}] role={m.get('role')} len={len(m.get('content') or '')} preview={prev}")

    # Decide whether to use Responses API:
    # - If caller explicitly passed True/False, respect it.
    # - Otherwise, auto-enable for GPT-5 if configured in settings.
    try:
        s = get_settings()
        if use_responses_api is None:
            use_responses_api = bool(
                isinstance(chosen_model, str)
                and chosen_model.startswith("gpt-5")
                and getattr(s.features, "use_responses_api_for_gpt5", True)
            )
        else:
            use_responses_api = bool(use_responses_api)
    except Exception:
        use_responses_api = bool(use_responses_api)

    # Determine if schema contains optional properties; if so, relax strict mode
    relax_needed = _schema_has_optional_props(schema)

    if use_responses_api:
        for attempt in range(1, attempts + 1):
            try:
                log_header("Responses")
                
                # messagesë¥¼ ë‹¨ì¼ ë¬¸ìì—´ë¡œ ë³€í™˜
                if isinstance(messages, list):
                    input_text = "\n".join([
                        f"{msg.get('role', 'user')}: {msg.get('content', '')}"
                        for msg in messages
                    ])
                else:
                    input_text = messages
                
                # text íŒŒë¼ë¯¸í„° êµ¬ì„± (format + verbosity)
                text_config = {
                    "format": {
                        "type": "json_schema",
                        "name": strict_schema.get("title", "structured_output"),
                        "strict": False if relax_needed else True,
                        "schema": strict_schema,
                    }
                }
                
                # verbosity ì¶”ê°€ (ì˜µì…˜)
                try:
                    if chosen_model.startswith("gpt-5"):
                        text_config["verbosity"] = s.features.gpt5_text_verbosity
                except:
                    pass
                
                # reasoning íŒŒë¼ë¯¸í„° êµ¬ì„±
                reasoning_config = {}
                try:
                    if chosen_model.startswith("gpt-5"):
                        reasoning_config = {"effort": s.features.gpt5_reasoning_effort}
                except:
                    reasoning_config = {"effort": "medium"}
                
                # Responses API í˜¸ì¶œ
                resp = client.responses.create(
                    model=chosen_model,
                    input=input_text,
                    text=text_config,  # âœ… formatê³¼ verbosity ëª¨ë‘ í¬í•¨
                    reasoning=reasoning_config,  # âœ… effort í¬í•¨
                    max_output_tokens=max_output_tokens,
                )
                
                # ì‘ë‹µ ì²˜ë¦¬
                text = _extract_output_text(resp)
                if not isinstance(text, str) or not text.strip():
                    raise ValueError("Empty output_text from model")
                print(f"[LLM] raw text len={len(text)}")
                return _safe_json_loads(_extract_json_from_codeblock(text) or text)
            
            except Exception as e:
                print(f"[LLM] error on attempt {attempt}: {e}")
                if attempt >= attempts:
                    print("[LLM] final fallback â†’ chat(function calling)")
                else:
                    time.sleep(_retry_backoff(attempt-1))

    # 2) Chat API
    for attempt in range(1, attempts + 1):
        try:
            log_header("Chat")
            messages_json = list(messages)
            # response_format=json_object ì‚¬ìš© ì‹œ ë©”ì‹œì§€ì— json í‚¤ì›Œë“œ ë³´ê°•
            try:
                has_json_kw = any(
                    isinstance(m, dict) and isinstance(m.get("content"), str) and ("json" in m["content"].lower())
                    for m in messages_json
                )
            except Exception:
                has_json_kw = False
            if not has_json_kw:
                messages_json.append({
                    "role": "developer",
                    "content": (
                        "Return only JSON. Output a single valid JSON object or array that follows the intent of the schema. "
                        "Do not include prose, markdown, or code fences. json"
                    )
                })

            # Chat API extras: keep conservative for compatibility. Do NOT pass 'reasoning' or 'verbosity' here.
            extra: Dict[str, Any] = {}

            # Prefer Chat parse with Pydantic model if requested
            if use_chat_parse and pydantic_model is not None:
                try:
                    # gpt-5 Chat may require 'max_completion_tokens' instead of 'max_tokens'
                    if isinstance(chosen_model, str) and chosen_model.startswith("gpt-5"):
                        # omit temperature (unsupported except default)
                        resp = client.chat.completions.parse(
                            model=chosen_model,
                            messages=messages_json,
                            response_format=pydantic_model,
                            max_completion_tokens=max_output_tokens,
                            **extra,
                        )
                    else:
                        resp = client.chat.completions.parse(
                            model=chosen_model,
                            messages=messages_json,
                            response_format=pydantic_model,
                            max_tokens=max_output_tokens,
                            temperature=temperature,
                            **extra,
                        )
                    parsed = getattr(resp.choices[0].message, "parsed", None)
                    if parsed is not None:
                        # Pydantic model instance â†’ dict
                        return parsed.model_dump() if hasattr(parsed, "model_dump") else parsed
                except Exception as e:
                    print(f"[LLM] chat.parse failed: {e}")

            if prefer_object:
                if isinstance(chosen_model, str) and chosen_model.startswith("gpt-5"):
                    # omit temperature for gpt-5 chat
                    resp = client.chat.completions.create(
                        model=chosen_model,
                        messages=messages_json,
                        response_format={"type": "json_object"},
                        max_completion_tokens=max_output_tokens,
                        **extra,
                    )
                else:
                    resp = client.chat.completions.create(
                        model=chosen_model,
                        messages=messages_json,
                        response_format={"type": "json_object"},
                        max_tokens=max_output_tokens,
                        temperature=temperature,
                        **extra,
                    )
            else:
                # optional path: try json_schema when explicitly requested
                if isinstance(chosen_model, str) and chosen_model.startswith("gpt-5"):
                    # omit temperature for gpt-5 chat
                    resp = client.chat.completions.create(
                        model=chosen_model,
                        messages=messages_json,
                        response_format={
                            "type": "json_schema",
                            "json_schema": {
                                "name": strict_schema.get("title", "structured_output"),
                                "schema": strict_schema,
                                "strict": False if relax_needed else True,
                            },
                        },
                        max_completion_tokens=max_output_tokens,
                        **extra,
                    )
                else:
                    resp = client.chat.completions.create(
                        model=chosen_model,
                        messages=messages_json,
                        response_format={
                            "type": "json_schema",
                            "json_schema": {
                                "name": strict_schema.get("title", "structured_output"),
                                "schema": strict_schema,
                                "strict": False if relax_needed else True,
                            },
                        },
                        max_tokens=max_output_tokens,
                        temperature=temperature,
                        **extra,
                    )

            text = _extract_output_text(resp)
            if not isinstance(text, str) or not text.strip():
                raise ValueError("Empty output_text from model")
            print(f"[LLM] raw text len={len(text)}")
            payload = _extract_json_from_codeblock(text) or text
            try:
                return _safe_json_loads(payload)
            except Exception:
                # ê´„í˜¸ ë‚´ JSON ì¶”ì¶œ ì¬ì‹œë„
                m = re.search(r"\[\s*\{[\s\S]*\}\s*\]", payload)
                if m:
                    return _safe_json_loads(m.group(0))
                m = re.search(r"\{[\s\S]*\}", payload)
                if m:
                    return _safe_json_loads(m.group(0))
                raise
        except Exception as e:
            print(f"[LLM] error on attempt {attempt}: {e}")
            if attempt >= attempts:
                print("[LLM] final fallback â†’ tools(function calling)")
            else:
                time.sleep(_retry_backoff(attempt-1))

    # 3) Chat tools(function calling) â€” wrap schema as OBJECT
    try:
        tool_parameters = {
            "type": "object",
            "properties": {
                "data": strict_schema  # allow any top-level (array/object) via wrapper
            },
            "required": ["data"],
            "additionalProperties": False
        }
        # Use correct token param for GPT-5 chat
        token_kwargs = (
            {"max_completion_tokens": max_output_tokens}
            if isinstance(chosen_model, str) and chosen_model.startswith("gpt-5")
            else {"max_tokens": max_output_tokens, "temperature": temperature}
        )
        resp = client.chat.completions.create(
            model=chosen_model,
            messages=messages + [
                {"role": "system", "content": "Call function `return_json` with the JSON payload in the `data` field. Do not output anything else."}
            ],
            tools=[{"type": "function", "function": {
                "name": "return_json",
                "description": "Return the JSON strictly following the provided schema.",
                "parameters": tool_parameters
            }}],
            tool_choice={"type": "function", "function": {"name": "return_json"}},
            **token_kwargs,
        )
        tc = resp.choices[0].message.tool_calls
        if tc and len(tc) > 0:
            args_text = tc[0].function.arguments
            payload = json.loads(args_text)
            print("[LLM] tools fallback success")
            return payload.get("data", payload)
        raise RuntimeError("No tool_calls returned")
    except Exception as e_tools:
        print(f"[LLM] tools fallback failed: {e_tools}")
        raise

# -----------------------------
# High-level wrappers
# -----------------------------
def paraphrase_llm(pattern: str, ctx: Dict[str, Any]) -> List[str]:
    placeholders = re.findall(r"\{[a-zA-Z0-9_]+\}", pattern)

    schema = {
        "title": "ParaphraseVariants",
        "type": "object",
        "properties": {"variants": {"type": "array","items":{"type":"string","minLength":2,"maxLength":180},"minItems":1,"maxItems":20}},
        "required": ["variants"], "additionalProperties": False
    }

    system_msg = (
        "You are a Korean question rewriter. "
        "Return natural, concise Korean variants that keep placeholders unchanged."
    )
    developer_msg = (
        "HARD RULES:\n"
        f"- NEVER add/remove/rename placeholders. Exact set: {', '.join(placeholders) if placeholders else '(none)'}\n"
        "- Do NOT add bracketed scenario labels like [ìš´ì˜], [ê°ì‚¬], etc.\n"
        "- Do NOT add 5W1H prompts (ëˆ„ê°€/ì–¸ì œ/ì–´ë””/ì™œ/ì–´ë–»ê²Œ) unless they are directly answerable from data.\n"
        "- Do NOT leak metric identifiers or English tokens.\n"
        "- Keep each sentence <= 180 chars. Avoid near-duplicates.\n"
        "Return only JSON."
    )
    user_msg = json.dumps({"pattern": pattern, "max_variants": 12}, ensure_ascii=False)

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "developer", "content": developer_msg},
        {"role": "user", "content": user_msg},
    ]

    model = get_settings().models.paraphrase
    data = _call_llm_json(messages, schema, temperature=0.6, model=model, max_output_tokens=8000, use_responses_api=False)

    def same_placeholders(s: str) -> bool:
        return sorted(re.findall(r"\{[a-zA-Z0-9_]+\}", s)) == sorted(placeholders)

    variants_raw = data.get("variants", []) if isinstance(data, dict) else []
    out, seen = [], set()
    for v in variants_raw:
        if not isinstance(v, str): continue
        if "[" in v and "]" in v:  # ì•ˆì „ì¥ì¹˜: ì‹œë‚˜ë¦¬ì˜¤ ë¼ë²¨ ìœ ì… ì°¨ë‹¨
            continue
        if not same_placeholders(v): continue
        if v in seen: continue
        out.append(v); seen.add(v)
    return out[:12] or [pattern]



def synth_llm(payload: Dict[str, Any]) -> List[Dict[str, Any]]:
    """
    DSL synth/refine.
    - If 'draft' exists: refine (GROUNDING/CREATIVE/PRUNING)
    - else: synth within allowed_metrics
    """
    categories = payload.get("categories", [])
    min_per_cat = int(payload.get("min_per_cat", 4))
    draft = payload.get("draft")

    schema = {
        "title": "TestIntentDSLList",
        "type": "array",
        "items": {
            "type": "object",
            "properties": {
                "id": {"type": "string", "minLength": 3, "maxLength": 80},
                "category": {"type": "string"},
                "intent": {
                    "type": "object",
                    "properties": {
                        "metric": {"type": "string"},
                        "scope": {"type": "object"},
                        "aggregation": {"type": "string", "enum": ["boolean","numeric","set","map","text"]},
                        "placeholders": {"type": "array", "items": {"type":"string"}, "maxItems": 4}
                    },
                    "required": ["metric","scope","aggregation","placeholders"],
                    "additionalProperties": False
                },
                "pattern": {"type": "string", "minLength": 4, "maxLength": 180},
                "level": {"type": "number"},
                "goal": {"type": "string"},
                "policy_hints": {"type": "object"},
                "origin": {"type": "string"}
            },
            "required": ["id","category","intent","pattern"],
            "additionalProperties": False
        },
        "minItems": 1,
        "maxItems": 200,
        "additionalProperties": False
    }

    if draft:
        system_msg = "You are a network test synthesizer. Return ONLY JSON following the schema."
        developer_msg = (
            "REFINE THE DRAFT DSL ITEMS WITH TWO STEPS:\n"
            "1) GROUNDING: adjust 'scope' to fit CAPABILITIES. If only one AS exists, drop or simplify multi-AS comparisons. NEVER change 'metric'.\n"
            "2) CREATIVE: rewrite 'pattern' into concise, fluent Korean reflecting 'goal' and any 'policy_hints'. Keep placeholders like {host},{asn},{vrf}.\n"
            "RULES:\n"
            "REFINE THE DRAFT DSL ITEMS WITH TWO STEPS:\n"
            "1) GROUNDING: adjust 'scope' to fit CAPABILITIES. If only one AS exists, drop or simplify multi-AS comparisons. NEVER change 'metric'.\n"
            "2) CREATIVE: rewrite 'pattern' into concise, fluent Korean reflecting 'goal' and any 'policy_hints'. Keep placeholders like {host},{asn},{vrf}.\n"
            "STRICT RULES:\n"
            f"- Ensure ~{min_per_cat} items per category when possible.\n"
            "- Do NOT add bracketed scenario labels like [ìš´ì˜], [ê°ì‚¬], etc.\n"
            "- Do NOT include 5W1H fillers unless directly answerable by data.\n"
            "- Do NOT expose metric identifiers (e.g., ibgp_fullmesh_ok) in the pattern.\n"
            "- Remove duplicates or trivial items.\n"
            "- Keep 'level' and 'origin' as in draft.\n"
        )
    else:
        system_msg = "You are a network test template synthesizer. Return ONLY JSON following the schema."
        developer_msg = (
            "Synthesize DSL items using ONLY the provided allowed metrics. "
            f"Aim for at least {min_per_cat} items per category when supported by capabilities. "
            "Use placeholders {host},{asn},{vrf} appropriately. Keep Korean patterns concise."
        )

    messages = [
        {"role":"system","content":system_msg},
        {"role":"developer","content":developer_msg},
        {"role":"user","content":json.dumps(payload, ensure_ascii=False)},

    ]

    model = get_settings().models.enhanced_generation
    data = _call_llm_json(
        messages, schema, temperature=0.25, model=model,
        max_output_tokens=8000, use_responses_api=False  # Chat ìš°ì„ 
    )

    allowed = payload.get("allowed_metrics", {})
    out = []
    if isinstance(data, list):
        for x in data:
            cat = x.get("category")
            metric = (x.get("intent") or {}).get("metric")
            if not cat or cat not in categories:
                continue
            if metric not in (allowed.get(cat) or []):
                continue
            out.append(x)
    return out


def generate_questions_llm(
    network_facts: Dict[str, Any], 
    category: str, 
    sample_questions: List[Dict[str, Any]], 
    count: int = 3
) -> List[Dict[str, Any]]:
    """
    ë‹¨ìˆœí™”ëœ LLM ì§ˆë¬¸ ìƒì„± - 1íšŒ í˜¸ì¶œë¡œ ì™„ì„±ëœ ì§ˆë¬¸ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    schema = {
        "title": "NetworkQuestions",
        "type": "object", 
        "properties": {
            "questions": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "question": {"type": "string"},
                        "expected_answer": {"type": "string"},
                        "answer_type": {"type": "string", "enum": ["text", "numeric", "boolean", "list"]},
                        "notes": {"type": "string"}
                    },
                    "required": ["question", "expected_answer", "answer_type"],
                    "additionalProperties": False
                },
                "maxItems": count
            }
        },
        "required": ["questions"],
        "additionalProperties": False
    }
    
    system_msg = f"""ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ ìƒì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
{category} ì¹´í…Œê³ ë¦¬ì— ëŒ€í•´ ì£¼ì–´ì§„ ë„¤íŠ¸ì›Œí¬ í˜„í™©ì„ ë°”íƒ•ìœ¼ë¡œ ì‹¤ìš©ì ì¸ ì§ˆë¬¸ì„ ìƒì„±í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
1. ìƒ˜í”Œ ì§ˆë¬¸ë“¤ê³¼ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼ì´ì§€ë§Œ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ì§ˆë¬¸
2. ë„¤íŠ¸ì›Œí¬ ë°ì´í„°ë¡œë¶€í„° ì‹¤ì œ ê³„ì‚°/í™•ì¸ ê°€ëŠ¥í•œ ë‹µë³€
3. ëª…í™•í•˜ê³  êµ¬ì²´ì ì¸ í•œêµ­ì–´ ì§ˆë¬¸
4. ìš´ì˜ìê°€ ì‹¤ë¬´ì—ì„œ ë¬¼ì–´ë³¼ ë§Œí•œ ì‹¤ìš©ì  ë‚´ìš©"""

    user_msg = json.dumps({
        "category": category,
        "network_summary": _summarize_network_for_llm(network_facts),
        "sample_questions": sample_questions[:3],  # ì˜ˆì‹œë¡œ 3ê°œë§Œ
        "requested_count": count
    }, ensure_ascii=False, indent=2)

    messages = [
        {"role": "system", "content": system_msg},
        {"role": "user", "content": user_msg}
    ]

    try:
        data = _call_llm_json(
            messages, schema, temperature=0.7,
            model=get_settings().models.question_generation,
            max_output_tokens=8000, use_responses_api=False
        )
        
        if isinstance(data, dict) and "questions" in data:
            return data["questions"]
        return []
        
    except Exception as e:
        print(f"[QuestionGen] LLM í˜¸ì¶œ ì‹¤íŒ¨: {e}")
        return []


def generate_questions_llm_v2(network_summary: str, category: str, num_questions: int = 3, examples: str = "") -> List[Dict[str, Any]]:
    """
    ë‹¨ì¼ LLM í˜¸ì¶œë¡œ ì¹´í…Œê³ ë¦¬ë³„ ì¶”ê°€ ì§ˆë¬¸ ìƒì„±
    """
    schema = {
        "type": "object",
        "title": "QuestionList",
        "properties": {
            "questions": {
                "type": "array",
                "items": {
                    "type": "object",
                    "properties": {
                        "question": {"type": "string"},
                        "expected_answer": {"type": "string"},
                        "answer_type": {"type": "string", "enum": ["exact", "contains", "numeric", "boolean"]},
                        "difficulty": {"type": "string", "enum": ["easy", "medium", "hard"]},
                        "confidence": {"type": "number", "minimum": 0.0, "maximum": 1.0}
                    },
                    "required": ["question", "expected_answer"],
                    "additionalProperties": False
                }
            }
        },
        "required": ["questions"],
        "additionalProperties": False
    }
    
    prompt = f"""Generate {num_questions} additional network configuration questions for category '{category}'.

Network Context:
{network_summary}

Existing Examples:
{examples}

Requirements:
- Focus on {category} configuration and status
- Questions should be answerable from the network context
- Vary difficulty levels (easy/medium/hard)
- Provide specific, factual expected answers
- Use clear, professional language

Generate questions that complement but don't duplicate the examples."""

    messages = [{"role": "user", "content": prompt}]
    
    try:
        result = _call_llm_json(messages, schema, temperature=0.7)
        return result.get("questions", [])
    except Exception as e:
        print(f"[LLM] Question generation failed: {e}")
        return []


def review_questions_llm(questions: List[Any]) -> List[int]:
    """
    LLMì„ ì‚¬ìš©í•œ ì§ˆë¬¸ í’ˆì§ˆ ê²€í† 
    ìŠ¹ì¸ëœ ì§ˆë¬¸ì˜ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
    """
    schema = {
        "type": "object", 
        "title": "QuestionReview",
        "properties": {
            "approved_indices": {
                "type": "array",
                "items": {"type": "integer", "minimum": 0}
            },
            "review_notes": {"type": "string"}
        },
        "required": ["approved_indices"],
        "additionalProperties": False
    }
    
    # ì§ˆë¬¸ë“¤ì„ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
    question_texts = []
    for i, q in enumerate(questions):
        if hasattr(q, 'question'):
            question_texts.append(f"{i}: {q.question} -> {q.expected_answer}")
        else:
            question_texts.append(f"{i}: {q.get('question', '')} -> {q.get('expected_answer', '')}")
    
    prompt = f"""Review these network configuration questions for quality and appropriateness:

{chr(10).join(question_texts)}

Approve questions that are:
- Clear and well-formed
- Answerable from network configuration
- Technically accurate
- Not duplicated
- Have reasonable expected answers

Return the indices (0-based) of approved questions."""

    messages = [{"role": "user", "content": prompt}]
    
    try:
        result = _call_llm_json(messages, schema, temperature=0.3)
        approved = result.get("approved_indices", [])
        # ìœ íš¨í•œ ì¸ë±ìŠ¤ë§Œ ë°˜í™˜
        return [i for i in approved if 0 <= i < len(questions)]
    except Exception as e:
        print(f"[LLM] Question review failed: {e}")
        # ì‹¤íŒ¨ì‹œ ëª¨ë“  ì§ˆë¬¸ ìŠ¹ì¸
        return list(range(len(questions)))


def _summarize_network_for_llm(facts: Dict[str, Any]) -> Dict[str, Any]:
    """LLMìš© ë„¤íŠ¸ì›Œí¬ ìš”ì•½"""
    devices = facts.get("devices", [])
    summary = {
        "device_count": len(devices),
        "device_types": [],
        "as_numbers": set(),
        "technologies": set()
    }
    
    for device in devices:
        file_name = device.get("file", "")
        if "ce" in file_name.lower():
            summary["device_types"].append("CE")
        elif "sample" in file_name.lower():
            summary["device_types"].append("PE")
        
        # AS ë²ˆí˜¸
        bgp = device.get("routing", {}).get("bgp", {})
        if bgp.get("local_as"):
            summary["as_numbers"].add(str(bgp["local_as"]))
        
        # ê¸°ìˆ ìŠ¤íƒ
        if bgp:
            summary["technologies"].add("BGP")
        if device.get("routing", {}).get("ospf"):
            summary["technologies"].add("OSPF")
        if device.get("services", {}).get("l2vpn"):
            summary["technologies"].add("L2VPN")
        if device.get("services", {}).get("vrf"):
            summary["technologies"].add("VRF")
        if device.get("security", {}).get("ssh"):
            summary["technologies"].add("SSH")
    
    summary["as_numbers"] = list(summary["as_numbers"])
    summary["technologies"] = list(summary["technologies"])
    return summary


def _salvage_hypotheses(capabilities: Dict[str, Any], n: int, metrics: List[str]) -> List[Dict[str, Any]]:
    """LLM ì‹¤íŒ¨/ë¹ˆ ì‘ë‹µ ì‹œ, grounding/anomalies ê¸°ë°˜ ê¸°ë³¸ ê°€ì„¤ ìƒì„±"""
    out: List[Dict[str, Any]] = []
    anomalies = (capabilities or {}).get("anomalies", {}) or {}
    as_groups = (capabilities or {}).get("as_groups", {}) or {}
    ssh_missing_cnt = capabilities.get("ssh_missing_count") if isinstance(capabilities, dict) else None
    # í›„ë³´ ë©”íŠ¸ë¦­ í—¬í¼
    def pick(*cands: str) -> str:
        for c in cands:
            if c in metrics:
                return c
        return metrics[0] if metrics else ""
    # 1) SSH ë¯¸êµ¬í˜„
    if isinstance(ssh_missing_cnt, int) and ssh_missing_cnt > 0 and len(out) < n:
        out.append({
            "question": f"SSH ë¯¸êµ¬í˜„ ì¥ë¹„ {ssh_missing_cnt}ëŒ€ë¥¼ ëª¨ë‘ ì‹ë³„í•˜ê³  ë³´ì•ˆ ì •ì±… ìœ„ë°˜ ì—¬ë¶€ë¥¼ í™•ì¸í•  ìˆ˜ ìˆëŠ”ê°€?",
            "hypothesis_type": "ImpactAnalysis",
            "intent_hint": {"metric": pick("ssh_missing_count","ssh_missing_devices"), "scope": {}},
            "expected_condition": "ssh_missing_count == 0",
            "reasoning_steps": "grounding ì—ì„œ ssh_missing_count > 0 ë°œê²¬ â†’ ë³´ì•ˆ ì˜í–¥ í‰ê°€",
            "cited_values": {"ssh_missing_count": ssh_missing_cnt}
        })
    # 2) iBGP under-peered / missing
    for asn, meta in as_groups.items():
        if len(out) >= n: break
        miss = meta.get("ibgp_missing_pairs_count", 0)
        under = meta.get("ibgp_under_peered_count", 0)
        if miss:
            out.append({
                "question": f"AS {asn} ë‚´ iBGP í˜ì–´ {miss}ìŒ ëˆ„ë½ì´ ì „ì²´ ì „íŒŒì— ì˜í–¥ì„ ì£¼ëŠ”ê°€?",
                "hypothesis_type": "ImpactAnalysis",
                "intent_hint": {"metric": pick("ibgp_missing_pairs_count","ibgp_missing_pairs"), "scope": {"asn": asn}},
                "expected_condition": "ibgp_missing_pairs_count == 0",
                "reasoning_steps": "as_groups ì—ì„œ missing_pairs_count > 0",
                "cited_values": {"asn": asn, "ibgp_missing_pairs_count": miss}
            })
        if len(out) >= n: break
        if under:
            out.append({
                "question": f"AS {asn} ë‚´ í”¼ì–´ ìˆ˜ ë¶€ì¡±(under-peered) ì¥ë¹„ {under}ëŒ€ê°€ ë¼ìš°íŠ¸ ìˆ˜ë ´ì„ ì €í•´í•˜ëŠ”ê°€?",
                "hypothesis_type": "RootCauseAnalysis",
                "intent_hint": {"metric": pick("ibgp_under_peered_count","ibgp_under_peered_devices"), "scope": {"asn": asn}},
                "expected_condition": "ibgp_under_peered_count == 0",
                "reasoning_steps": "as_groups ë©”íƒ€ì—ì„œ under_peered_count > 0",
                "cited_values": {"asn": asn, "ibgp_under_peered_count": under}
            })
    # 3) VRF / L2VPN anomaly
    if len(out) < n and anomalies.get("vrf_without_rt_count"):
        c = anomalies["vrf_without_rt_count"]
        out.append({
            "question": f"Route-Target ë¯¸ì„¤ì • VRF {c}ê±´ì´ L3VPN ê²½ë¡œ ìœ í†µì„ ì €í•´í•˜ëŠ”ê°€?",
            "hypothesis_type": "ImpactAnalysis",
            "intent_hint": {"metric": pick("vrf_without_rt_count","vrf_rd_map","vrf_rt_list_per_device","vrf_without_rt_pairs"), "scope": {}},
            "expected_condition": "vrf_without_rt_count == 0",
            "reasoning_steps": "anomalies.vrf_without_rt_count > 0",
            "cited_values": {"vrf_without_rt_count": c}
        })
    if len(out) < n and anomalies.get("l2vpn_unidir_count"):
        c = anomalies["l2vpn_unidir_count"]
        out.append({
            "question": f"L2VPN ë‹¨ë°©í–¥(unidirectional) ì„¸ì…˜ {c}ê±´ì´ ì„œë¹„ìŠ¤ ì¥ì• ë¥¼ ìœ ë°œí•˜ëŠ”ê°€?",
            "hypothesis_type": "RootCauseAnalysis",
            "intent_hint": {"metric": pick("l2vpn_unidir_count","l2vpn_unidirectional_pairs"), "scope": {}},
            "expected_condition": "l2vpn_unidir_count == 0",
            "reasoning_steps": "anomalies.l2vpn_unidir_count > 0",
            "cited_values": {"l2vpn_unidir_count": c}
        })
    if len(out) < n and anomalies.get("l2vpn_mismatch_count"):
        c = anomalies["l2vpn_mismatch_count"]
        out.append({
            "question": f"L2VPN PW ID ë¶ˆì¼ì¹˜ {c}ê±´ì´ íŠ¸ë˜í”½ ë¸”ë™í™€ì„ ì´ˆë˜í•˜ëŠ”ê°€?",
            "hypothesis_type": "AdversarialCheck",
            "intent_hint": {"metric": pick("l2vpn_mismatch_count","l2vpn_pwid_mismatch_pairs"), "scope": {}},
            "expected_condition": "l2vpn_mismatch_count == 0",
            "reasoning_steps": "anomalies.l2vpn_mismatch_count > 0",
            "cited_values": {"l2vpn_mismatch_count": c}
        })
    # 4) ë¶€ì¡±í•˜ë©´ ì¼ë°˜ ë³´ì•ˆ/êµ¬ì„± ì •ìƒì„± ê°€ì„¤ ì±„ìš°ê¸°
    fillers = [
        ("SSH ê¸°ëŠ¥ì´ ëª¨ë“  ì¥ë¹„ì— ì¼ê´€ë˜ê²Œ ì ìš©ë˜ì–´ ìˆëŠ”ê°€?", pick("ssh_all_enabled_bool","ssh_missing_count"), "ssh_all_enabled_bool == true"),
        ("ê° ASì˜ iBGP í˜ì–´ë§ì´ ì™„ì „ ë©”ì‹œì¸ì§€ í™•ì¸í•  ìˆ˜ ìˆëŠ”ê°€?", pick("ibgp_fullmesh_ok","ibgp_missing_pairs_count"), "ibgp_fullmesh_ok == true"),
    ]
    for q, m, cond in fillers:
        if len(out) >= n: break
        out.append({
            "question": q,
            "hypothesis_type": "ImpactAnalysis",
            "intent_hint": {"metric": m, "scope": {}},
            "expected_condition": cond,
            "reasoning_steps": "fallback filler",
            "cited_values": {}
        })
    return out[:n]


# -----------------------------
# ì¶”ê°€: ì˜ë„ íŒŒì‹±(parse_intent_llm) ë° ê°€ì„¤ ë¦¬ë·°(review_hypotheses_llm)
# -----------------------------
from typing import Iterable

def parse_intent_llm(question: str, metrics: Iterable[str], hint_metric: str | None = None, hint_scope: dict | None = None, cited_values: dict | None = None) -> Dict[str, Any]:
    """ì§ˆë¬¸ì„ ê¸°ë°˜ìœ¼ë¡œ metric / scope íŒíŠ¸ë¥¼ ì‚°ì¶œ.
    - ìš°ì„ ìˆœìœ„: hint_metric â†’ ì§ˆë¬¸ í‚¤ì›Œë“œ íœ´ë¦¬ìŠ¤í‹± â†’ metrics[0]
    - scope ì¶”ì¶œ: AS ë²ˆí˜¸, hostname (cited_values ë˜ëŠ” ì§ˆë¬¸), VRF ì´ë¦„ íŒ¨í„´
    - í™˜ê²½ë³€ìˆ˜ GIA_USE_INTENT_LLM=1 ì´ë©´ LLM í˜¸ì¶œ (schema ê°•ì œ) ì‹œë„, ì‹¤íŒ¨ ì‹œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ í´ë°±
    """
    metrics_list = list(metrics or [])
    q = (question or "").strip()
    out_metric: str | None = None
    if hint_metric and hint_metric in metrics_list:
        out_metric = hint_metric
    else:
        low = q.lower()
        # ê°„ë‹¨ í‚¤ì›Œë“œ ë§¤í•‘
        KEYMAP = [
            ("ssh", ["ssh_missing_count","ssh_missing_devices","ssh_enabled_devices"]),
            ("bgp", ["ibgp_missing_pairs_count","ibgp_fullmesh_ok","ibgp_under_peered_count","neighbor_list_ibgp"]),
            ("vrf", ["vrf_without_rt_count","vrf_rd_map","vrf_rt_list_per_device","vrf_without_rt_pairs"]),
            ("l2vpn", ["l2vpn_unidir_count","l2vpn_mismatch_count","l2vpn_unidirectional_pairs"]),
            ("ospf", ["ospf_area0_if_count","ospf_proc_ids"]),
        ]
        for kw, cands in KEYMAP:
            if kw in low:
                for c in cands:
                    if c in metrics_list:
                        out_metric = c; break
            if out_metric: break
    if not out_metric and metrics_list:
        out_metric = metrics_list[0]

    # scope íœ´ë¦¬ìŠ¤í‹±
    scope: Dict[str, Any] = {}
    import re
    m_as = re.search(r"AS\s*(\d+)", q, flags=re.IGNORECASE)
    if m_as:
        scope["asn"] = m_as.group(1)
    # hostname ì¶”ì¶œ(ê°„ë‹¨): ëŒ€ë¬¸ì/ì†Œë¬¸ì/ìˆ«ì í¬í•¨ ë‹¨ì–´ ì¤‘ host, rtr ë“± íŒ¨í„´
    m_host = re.search(r"\b([A-Za-z][A-Za-z0-9_-]{2,})\b", q)
    if m_host and not scope.get("asn"):
        # ë„ˆë¬´ ì¼ë°˜ ë‹¨ì–´ ì œì™¸
        token = m_host.group(1)
        if token.lower() not in {"count","device","devices","vrf","bgp","ssh"}:
            scope["host"] = token
    # VRF ì´ë¦„ (vrf XXX)
    m_vrf = re.search(r"vrf\s+([A-Za-z0-9_-]{2,})", q, flags=re.IGNORECASE)
    if m_vrf:
        scope["vrf"] = m_vrf.group(1)

    # hint_scope ë³‘í•©(ìš°ì„ )
    if isinstance(hint_scope, dict):
        scope.update({k:v for k,v in hint_scope.items() if v not in (None,"")})

    intent = {"metric": out_metric or "", "scope": scope}

    # ì„ íƒì  LLM ë³´ê°• (í™˜ê²½ë³€ìˆ˜ ON)
    if get_settings().features.use_intent_llm:
        schema = {
            "title": "IntentParse",
            "type": "object",
            "properties": {
                "metric": {"type": "string"},
                "scope": {"type": "object"},
                "reasoning": {"type": "string"}
            },
            "required": ["metric","scope"],
            "additionalProperties": False
        }
        messages = [
            {"role": "system", "content": "ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ì˜ë„ ì¶”ì¶œê¸°. ì§ˆë¬¸ì—ì„œ metric ê³¼ scope ë¥¼ ì„ íƒ. ì œê³µëœ metrics í›„ë³´ ì™¸ ê°’ ìƒì„± ê¸ˆì§€."},
            {"role": "user", "content": json.dumps({
                "question": q,
                "candidates": metrics_list,
                "hint_metric": hint_metric,
                "hint_scope": hint_scope,
                "cited_values": cited_values
            }, ensure_ascii=False)}
        ]
        try:
            data = _call_llm_json(messages, schema, temperature=0.0, model=get_settings().models.intent_parsing, max_output_tokens=8000, use_responses_api=False)
            if isinstance(data, dict):
                msel = data.get("metric")
                if isinstance(msel, str) and msel in metrics_list:
                    intent["metric"] = msel
                if isinstance(data.get("scope"), dict):
                    intent["scope"].update({k:v for k,v in data["scope"].items() if v not in (None,"")})
        except Exception as e:
            print(f"[IntentParse] LLM ì‹¤íŒ¨ â†’ íœ´ë¦¬ìŠ¤í‹± ìœ ì§€: {e}")
    return intent


def review_hypotheses_llm(hypotheses: List[Dict[str, Any]], capabilities: Dict[str, Any], max_items: int = 20) -> List[Dict[str, Any]]:
    """ê°€ì„¤ í’ˆì§ˆì„ LLM ë˜ëŠ” íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì ìˆ˜í™”.
    ë°˜í™˜: [{hypothesis_index, total_score, is_recommended, justification, ...}]
    - íœ´ë¦¬ìŠ¤í‹±: ê¸¸ì´, intent_hint.metric ì¡´ì¬ ì—¬ë¶€, cited_values ì¡´ì¬ ì—¬ë¶€, ì •ëŸ‰/ì¡°ê±´ í‚¤ì›Œë“œ
    - í™˜ê²½ë³€ìˆ˜ GIA_DISABLE_HYPO_REVIEW=1 ì´ë©´ ì¦‰ì‹œ íŒ¨ìŠ¤
    """
    if not hypotheses:
        return []
    if get_settings().features.disable_hypo_review:
        return [
            {"hypothesis_index": i, "total_score": 100, "is_recommended": True, "justification": "disabled"}
            for i,_ in enumerate(hypotheses)
        ]

    schema = {
        "title": "HypothesisReviewList",
        "type": "array",
        "items": {
            "type": "object",
            "properties": {
                "hypothesis_index": {"type": "integer"},
                "clarity_score": {"type": "integer"},
                "evidential_score": {"type": "integer"},
                "risk_score": {"type": "integer"},
                "total_score": {"type": "integer"},
                "is_recommended": {"type": "boolean"},
                "justification": {"type": "string"}
            },
            "required": ["hypothesis_index","total_score","is_recommended"],
            "additionalProperties": False
        },
        "maxItems": min(max_items, len(hypotheses)),
        "additionalProperties": False
    }

    messages = [
        {"role": "system", "content": "ë„¤íŠ¸ì›Œí¬ í…ŒìŠ¤íŠ¸ ê°€ì„¤ í’ˆì§ˆ í‰ê°€ì. JSONë§Œ ì¶œë ¥."},
        {"role": "user", "content": json.dumps({
            "capabilities_summary": {k: (v if isinstance(v,(int,str,float)) else str(type(v).__name__)) for k,v in (capabilities or {}).items() if k in ("ssh_missing_count","anomalies","as_groups")},
            "hypotheses": hypotheses,
            "scoring_criteria": {
                "clarity": "ë¬¸ì¥ì´ ëª…í™•/ì¸¡ì •ê°€ëŠ¥ (0-4)",
                "evidence": "cited_values ë˜ëŠ” intent_hint ê·¼ê±° í™œìš© (0-4)",
                "risk": "ìš´ì˜/ë³´ì•ˆ/ê°€ìš©ì„± ìœ„í—˜ ê´€ë ¨ì„± (0-4)"
            }
        }, ensure_ascii=False)}
    ]

    try:
        data = _call_llm_json(messages, schema, temperature=0.0, model=get_settings().models.hypothesis_review, max_output_tokens=8000, use_responses_api=False)
        if isinstance(data, list):
            # total_score ëˆ„ë½ ì‹œ í•©ì‚°
            for it in data:
                if isinstance(it, dict):
                    if "total_score" not in it:
                        ts = (it.get("clarity_score",0)+it.get("evidential_score",0)+it.get("risk_score",0))
                        it["total_score"] = ts
            return data
    except Exception as e:
        print(f"[HypoReview] LLM ì‹¤íŒ¨ â†’ íœ´ë¦¬ìŠ¤í‹± ì „í™˜: {e}")

    # íœ´ë¦¬ìŠ¤í‹± ë°±ì—…
    reviews: List[Dict[str, Any]] = []
    import re
    kw_numeric = re.compile(r"\b(ê°œ|ìˆ˜|count|==|!=|>|<|>=|<=)\b")
    for i, h in enumerate(hypotheses):
        q = (h.get("question") or "").strip()
        length_score = 2 if 10 <= len(q) <= 150 else 1
        has_metric = 2 if (h.get("intent_hint") or {}).get("metric") else 0
        has_evidence = 2 if (h.get("cited_values") or {}) else 0
        numeric_hint = 2 if kw_numeric.search(q) else 0
        total = length_score + has_metric + has_evidence + numeric_hint
        reviews.append({
            "hypothesis_index": i,
            "total_score": total,
            "is_recommended": total >= 4,
            "justification": f"len={len(q)} metric={has_metric>0} evidence={has_evidence>0} numeric_kw={numeric_hint>0}"
        })
    return reviews

