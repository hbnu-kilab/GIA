from typing import Dict, Any, List, Union
import json

from utils.builder_core import BuilderCore
from utils.llm_adapter import _call_llm_json

class AnswerAgent:
    """Reasoning plan executor that synthesizes a descriptive answer."""

    def __init__(self, network_facts: Dict[str, Any]):
        self.network_facts = network_facts
        self.builder = BuilderCore(network_facts.get("devices", []))
        self.evidence: Dict[str, Any] = {}

    def execute_plan(self, question: str, plan: Union[List[Dict[str, Any]], str]) -> str:
        """Execute reasoning steps and return synthesized answer."""
        self.evidence = {}

        # Handle string-based plans
        if isinstance(plan, str):
            return self._synthesize_text_answer(question, plan)

        # Handle None or empty plans
        if not plan:
            return self._synthesize_text_answer(question, "No reasoning plan provided")

        # Handle list-based plans (original behavior)
        if isinstance(plan, list):
            for step in sorted(plan, key=lambda x: x.get("step", 0) if isinstance(x, dict) else 0):
                if not isinstance(step, dict):
                    continue
                metric = step.get("required_metric")
                if not metric:
                    continue
                params = step.get("metric_params") or {}
                try:
                    result = self.builder.calculate_metric(metric, params)
                except Exception as e:
                    result = f"error: {e}"
                self.evidence[f"step_{step.get('step')}_{metric}"] = result

            return self._synthesize_answer(question, plan)
        
        # Handle other types as text
        return self._synthesize_text_answer(question, str(plan))

    def _synthesize_text_answer(self, question: str, plan_text: str) -> str:
        """Handle text-based reasoning plans."""
        # Try to extract metrics from the plan text
        potential_metrics = [
            "ssh_missing_count", "ssh_all_enabled_bool", "ssh_enabled_devices",
            "ibgp_missing_pairs_count", "ibgp_fullmesh_ok", 
            "vrf_without_rt_count", "l2vpn_unidir_count",
            "bgp_inconsistent_as_count", "aaa_enabled_devices"
        ]
        
        relevant_metrics = []
        plan_lower = plan_text.lower()
        
        # Find relevant metrics based on question content
        question_lower = question.lower()
        if "ssh" in question_lower:
            relevant_metrics.extend(["ssh_missing_count", "ssh_enabled_devices", "ssh_all_enabled_bool"])
        if "bgp" in question_lower:
            relevant_metrics.extend(["ibgp_missing_pairs_count", "ibgp_fullmesh_ok", "bgp_inconsistent_as_count"])
        if "vrf" in question_lower:
            relevant_metrics.extend(["vrf_without_rt_count"])
        if "aaa" in question_lower:
            relevant_metrics.extend(["aaa_enabled_devices"])
        if "l2vpn" in question_lower:
            relevant_metrics.extend(["l2vpn_unidir_count"])
        
        # If no specific metrics found, use general metrics
        if not relevant_metrics:
            relevant_metrics = ["ssh_enabled_devices", "ibgp_missing_pairs_count"]
        
        # Calculate relevant metrics
        for metric in relevant_metrics:
            try:
                result = self.builder.calculate_metric(metric)
                self.evidence[metric] = result
            except Exception as e:
                self.evidence[metric] = f"error: {e}"
        
        return self._synthesize_answer(question, plan_text)

    def _synthesize_answer(self, question: str, plan: Union[List[Dict[str, Any]], str]) -> str:
        """Return a concise final answer based on collected evidence.

        LLMì„ ê³„ì‚°ê¸°ë¡œ í™œìš©í•˜ì—¬ ì¦ê±°ì—ì„œ ìµœì¢… ì •ë‹µ ê°’ë§Œ ë„ì¶œí•œë‹¤. ì‹¤íŒ¨ ì‹œ
        ì¦ê±° ìš”ì•½ JSONì„ ë°˜í™˜í•œë‹¤.
        """
        schema = {
            "title": "AnswerSynthesis",
            "type": "object",
            "properties": {
                "final_answer": {"type": ["string", "number", "array"]}
            },
            "required": ["final_answer"],
            "additionalProperties": False,
        }

        system_prompt = (
            "ë‹¹ì‹ ì€ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ìµœì¢… ê²°ë¡ ì„ ë„ì¶œí•˜ëŠ” ì •í™•í•œ ê³„ì‚°ê¸°ì…ë‹ˆë‹¤. "
            "ì£¼ì–´ì§„ ì§ˆë¬¸ê³¼ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ, ë‹¤ë¥¸ ì„¤ëª… ì—†ì´ ì˜¤ì§ ìµœì¢… ì •ë‹µ ê°’ë§Œì„ "
            "ê³„ì‚°í•˜ì—¬ ë°˜í™˜í•˜ì„¸ìš”."
        )

        evidence_text = json.dumps(self.evidence, ensure_ascii=False, indent=2)
        user_prompt = (
            f"[ì§ˆë¬¸]\n{question}\n\n[ìˆ˜ì§‘ëœ ë°ì´í„°]\n{evidence_text}\n\n"
            "ìœ„ ë°ì´í„°ë¥¼ ê·¼ê±°ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ìµœì¢… ì •ë‹µì„ ê³„ì‚°í•˜ì„¸ìš”.\n\n"
            "[ì‘ë‹µ í˜•ì‹ ê·œì¹™]\n"
            "- ë¦¬ìŠ¤íŠ¸: [\"A\", \"B\"]\n"
            "- ìˆ«ì: 4\n"
            "- ë¬¸ìì—´: \"192.168.1.1\"\n"
            "- ì¶”ê°€ ì„¤ëª… ê¸ˆì§€"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        try:
            data = _call_llm_json(
                messages,
                schema,
                temperature=0.0,
                model="gpt-4o-mini",
                max_output_tokens=300,
                use_responses_api=False,
            )
            ans = data.get("final_answer") if isinstance(data, dict) else None
            if ans is not None:
                if isinstance(ans, (list, dict)):
                    return json.dumps(ans, ensure_ascii=False)
                return str(ans).strip()
        except Exception:
            pass

        summary = {
            "question": question,
            "plan": plan,
            "evidence": self.evidence,
        """ìˆ˜ì§‘ëœ ì¦ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„± - í•µì‹¬ êµ¬í˜„!"""
        
        # Evidenceê°€ ë¹„ì–´ìˆìœ¼ë©´ ê¸°ë³¸ ë‹µë³€
        if not self.evidence:
            return self._generate_template_answer(question, "ìˆ˜ì§‘ëœ ì¦ê±°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # ì¦ê±° ìš”ì•½ ìƒì„±
        evidence_summary = self._format_evidence()
        
        # LLMì—ê²Œ ìµœì¢… ë‹µë³€ ìƒì„± ìš”ì²­ - ê°•í™”ëœ í”„ë¡¬í”„íŠ¸
        synthesis_prompt = f"""ë‹¹ì‹ ì€ ë„¤íŠ¸ì›Œí¬ ì„¤ì • ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ìˆ˜ì§‘ëœ ì¦ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ì •í™•í•˜ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ìˆ˜ì§‘ëœ ì¦ê±°:
{evidence_summary}

ì‹¤í–‰ëœ ê³„íš:
{json.dumps(plan, indent=2, ensure_ascii=False) if isinstance(plan, list) else plan}

ìš”êµ¬ì‚¬í•­:
1. ì¦ê±°ì— ê¸°ë°˜í•œ êµ¬ì²´ì ì´ê³  ì •í™•í•œ ë‹µë³€ ì‘ì„±
2. ìˆ˜ì¹˜ë‚˜ ì¥ë¹„ëª… ë“± êµ¬ì²´ì  ë°ì´í„° í¬í•¨
3. ë¬¸ì œê°€ ë°œê²¬ë˜ë©´ ì›ì¸ê³¼ í•´ê²°ì±… ì œì‹œ
4. ì „ë¬¸ì ì´ë©´ì„œë„ ì´í•´í•˜ê¸° ì‰¬ìš´ ì„œìˆ í˜• ë‹µë³€
5. ì¦ê±°ê°€ ë¶€ì¡±í•˜ë©´ ëª…ì‹œì ìœ¼ë¡œ ì–¸ê¸‰

ë‹µë³€:"""
        
        # Schema ì •ì˜ - ê°„ë‹¨í•˜ê²Œ ìˆ˜ì •
        schema = {
            "type": "object",
            "properties": {
                "final_answer": {
                    "type": "string"
                }
            },
            "required": ["final_answer"],
            "additionalProperties": False
        }

        messages = [
            {"role": "system", "content": "ë‹¹ì‹ ì€ ë„¤íŠ¸ì›Œí¬ ì„¤ì • ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì œê³µëœ ì¦ê±°ë¥¼ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë‹µë³€ì„ ìƒì„±í•˜ì„¸ìš”."},
            {"role": "user", "content": synthesis_prompt}
        ]

        try:
            # ì§ì ‘ OpenAI API í˜¸ì¶œë¡œ ë‹¨ìˆœí™”
            import openai
            
            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=messages,
                temperature=0.1,
                max_tokens=800
            )
            
            final_answer = response.choices[0].message.content.strip()
            
            # ë‹µë³€ì´ ìˆìœ¼ë©´ ë°˜í™˜
            if final_answer and len(final_answer) > 20:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                print(f"âœ… LLM ë‹µë³€ ìƒì„± ì„±ê³µ (ê¸¸ì´: {len(final_answer)}ì)")
                return final_answer
            else:
                print(f"âš ï¸ LLM ë‹µë³€ì´ ë„ˆë¬´ ì§§ìŒ: {len(final_answer)}ì")
                
        except Exception as e:
            print(f"ğŸš¨ OpenAI API í˜¸ì¶œ ì‹¤íŒ¨: {e}")
            
            # OpenAI ì§ì ‘ í˜¸ì¶œ ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ ì‹œë„
            try:
                data = _call_llm_json(
                    messages,
                    schema,
                    temperature=0.1,
                    model="gpt-4o-mini",
                    max_output_tokens=800,
                    use_responses_api=False,
                )
                
                if isinstance(data, dict) and data.get("final_answer"):
                    final_answer = data["final_answer"].strip()
                    print(f"âœ… JSON LLM ë‹µë³€ ìƒì„± ì„±ê³µ (ê¸¸ì´: {len(final_answer)}ì)")
                    return final_answer
                    
            except Exception as e2:
                print(f"ğŸš¨ JSON LLMë„ ì‹¤íŒ¨: {e2}")
                # ìµœí›„ì˜ ìˆ˜ë‹¨: ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ ì¬ì‹œë„
                return self._simple_llm_call(question, evidence_summary)

        # ëª¨ë“  ì‹œë„ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ë‹µë³€
        return self._generate_template_answer(question, evidence_summary)

    def _format_evidence(self) -> str:
        """Evidenceë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…"""
        if not self.evidence:
            return "ìˆ˜ì§‘ëœ ì¦ê±°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted = []
        for key, value in self.evidence.items():
            # step_1_bgp_peer_count -> "1ë‹¨ê³„: BGP í”¼ì–´ ìˆ˜"
            if key.startswith('step_'):
                parts = key.split('_')
                if len(parts) >= 3:
                    step_num = parts[1]
                    metric_name = '_'.join(parts[2:])
                    formatted.append(f"â€¢ {step_num}ë‹¨ê³„ ({metric_name}): {self._format_value(value)}")
                else:
                    formatted.append(f"â€¢ {key}: {self._format_value(value)}")
            else:
                # ë©”íŠ¸ë¦­ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë³€í™˜
                korean_name = self._translate_metric_name(key)
                formatted.append(f"â€¢ {korean_name}: {self._format_value(value)}")
        
        return '\n'.join(formatted)

    def _translate_metric_name(self, metric_name: str) -> str:
        """ë©”íŠ¸ë¦­ ì´ë¦„ì„ í•œêµ­ì–´ë¡œ ë²ˆì—­"""
        translations = {
            'ssh_enabled_devices': 'SSH í™œì„±í™”ëœ ì¥ë¹„',
            'ssh_missing_count': 'SSH ë¯¸ì„¤ì • ì¥ë¹„ ìˆ˜',
            'ssh_all_enabled_bool': 'SSH ì „ì²´ í™œì„±í™” ì—¬ë¶€',
            'ibgp_missing_pairs_count': 'iBGP ëˆ„ë½ í˜ì–´ ìˆ˜',
            'ibgp_fullmesh_ok': 'iBGP í’€ë©”ì‹œ ì •ìƒ ì—¬ë¶€',
            'bgp_inconsistent_as_count': 'BGP AS ë¶ˆì¼ì¹˜ ìˆ˜',
            'aaa_enabled_devices': 'AAA í™œì„±í™”ëœ ì¥ë¹„',
            'vrf_without_rt_count': 'RT ë¯¸ì„¤ì • VRF ìˆ˜',
            'l2vpn_unidir_count': 'ë‹¨ë°©í–¥ L2VPN ìˆ˜',
            'bgp_peer_count': 'BGP í”¼ì–´ ìˆ˜',
            'interface_count': 'ì¸í„°í˜ì´ìŠ¤ ìˆ˜',
            'ospf_area_count': 'OSPF ì˜ì—­ ìˆ˜'
        }
        return translations.get(metric_name, metric_name)

    def _format_value(self, value) -> str:
        """ê°’ì„ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ í¬ë§·íŒ…"""
        if isinstance(value, bool):
            return "âœ… ì •ìƒ" if value else "âŒ ë¬¸ì œ"
        elif isinstance(value, (int, float)) and value == 0:
            return "0 (ë¬¸ì œì—†ìŒ)"
        elif isinstance(value, list):
            if len(value) == 0:
                return "ì—†ìŒ"
            elif len(value) <= 3:
                return f"{', '.join(map(str, value))}"
            else:
                return f"{', '.join(map(str, value[:3]))}... (ì´ {len(value)}ê°œ)"
        elif isinstance(value, str) and value.startswith("error:"):
            return f"âš ï¸ {value}"
        else:
            return str(value)

    def _generate_template_answer(self, question: str, evidence_summary: str) -> str:
        """LLM ì‹¤íŒ¨ ì‹œ í…œí”Œë¦¿ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        if "ì¦ê±°ê°€ ì—†ìŠµë‹ˆë‹¤" in evidence_summary:
            return f"""
ì§ˆë¬¸ "{question}"ì— ëŒ€í•œ ë¶„ì„ì„ ì‹œë„í–ˆì§€ë§Œ, ê´€ë ¨ ì¦ê±°ë¥¼ ìˆ˜ì§‘í•  ìˆ˜ ì—†ì—ˆìŠµë‹ˆë‹¤.

ì´ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì›ì¸ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤:
â€¢ ë„¤íŠ¸ì›Œí¬ ì„¤ì • ë°ì´í„°ì—ì„œ ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ
â€¢ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë©”íŠ¸ë¦­ì´ ì•„ì§ êµ¬í˜„ë˜ì§€ ì•ŠìŒ
â€¢ ë°ì´í„° íŒŒì‹± ê³¼ì •ì—ì„œ ì˜¤ë¥˜ ë°œìƒ

ë” êµ¬ì²´ì ì¸ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” ë„¤íŠ¸ì›Œí¬ ì„¤ì • íŒŒì¼ê³¼ ì§ˆë¬¸ì˜ ì í•©ì„±ì„ í™•ì¸í•´ ì£¼ì„¸ìš”.
"""
        
        return f"""
ì§ˆë¬¸ "{question}"ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼:

ìˆ˜ì§‘ëœ ì¦ê±°:
{evidence_summary}

ìœ„ ì¦ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì˜ í˜„ì¬ ìƒíƒœë¥¼ íŒŒì•…í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. 
êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ì„¤ì • ìƒíƒœë¥¼ í†µí•´ í•´ë‹¹ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ì„ ë„ì¶œí•  ìˆ˜ ìˆìœ¼ë©°,
ë¬¸ì œê°€ ë°œê²¬ëœ ê²½ìš° ì ì ˆí•œ í•´ê²°ì±…ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤.

ğŸ’¡ ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ì„œëŠ” LLM ê¸°ë°˜ ë‹µë³€ ìƒì„± ê¸°ëŠ¥ì„ í™œìš©í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤.
"""

    def _simple_llm_call(self, question: str, evidence_summary: str) -> str:
        """ê°„ë‹¨í•œ í”„ë¡¬í”„íŠ¸ë¡œ LLM í˜¸ì¶œ - ìµœí›„ì˜ ìˆ˜ë‹¨"""
        try:
            import openai
            
            simple_prompt = f"""ë„¤íŠ¸ì›Œí¬ ì „ë¬¸ê°€ë¡œì„œ ë‹¤ìŒ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.

ì§ˆë¬¸: {question}

ì¦ê±°:
{evidence_summary}

ìœ„ ì¦ê±°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•´ ì „ë¬¸ì ì´ê³  êµ¬ì²´ì ì¸ ë‹µë³€ì„ í•œêµ­ì–´ë¡œ ì‘ì„±í•˜ì„¸ìš”. ì¦ê±°ì˜ ìˆ˜ì¹˜ì™€ ìƒíƒœë¥¼ ì–¸ê¸‰í•˜ë©° ì‹¤ë¬´ì ì¸ ê´€ì ì—ì„œ ì„¤ëª…í•´ì£¼ì„¸ìš”."""

            response = openai.chat.completions.create(
                model="gpt-4o-mini",
                messages=[{"role": "user", "content": simple_prompt}],
                temperature=0.1,
                max_tokens=600
            )
            
            answer = response.choices[0].message.content.strip()
            print(f"âœ… ê°„ë‹¨í•œ LLM í˜¸ì¶œ ì„±ê³µ (ê¸¸ì´: {len(answer)}ì)")
            return answer
            
        except Exception as e:
            print(f"ğŸš¨ ê°„ë‹¨í•œ LLM í˜¸ì¶œë„ ì‹¤íŒ¨: {e}")
            return self._generate_template_answer(question, evidence_summary)
