# pip install openai>=1.0.0 chromadb tiktoken langchain langchain-community
"""
ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ë§ LLM íŒŒì´í”„ë¼ì¸ - ì—°êµ¬ê¸‰ RAG vs Non-RAG ë¹„êµ ì‹¤í—˜
=====================================================

ì´ íŒŒì´í”„ë¼ì¸ì€ ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ë§ ì§ˆë¬¸ì— ëŒ€í•´ RAGì™€ Non-RAG ë°©ì‹ì„ ë¹„êµ í‰ê°€í•©ë‹ˆë‹¤.
ìë™ ì„ë² ë”© ê¸°ëŠ¥ì„ í¬í•¨í•˜ì—¬ ì‚¬ì „ ì„ë² ë”©ëœ ë°ì´í„°ê°€ ì—†ì–´ë„ ë°”ë¡œ ì‹¤í—˜ì„ ì‹œì‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
"""

# ============================================================================
# ì„¤ì • êµ¬ì„± - ì—¬ê¸°ì„œ ëª¨ë“  ê²½ë¡œ, API í‚¤, íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”
# ============================================================================

# ğŸ”‘ API í‚¤ ì„¤ì •
GOOGLE_CSE_ID = "API_key"  # Google Custom Search Engine ID
GOOGLE_API_KEY = "API_key"  # Google API Key
OPENAI_API_KEY = ""  # OpenAI API Key

# ğŸ“‚ íŒŒì¼ ê²½ë¡œ ì„¤ì •
CHROMADB_PATH = "/workspace/jke/chromadb_qwen"  # ChromaDB ì €ì¥ ê²½ë¡œ (ìë™ ìƒì„±ë¨)
XML_DIRECTORY = "c:/Users/yujin/CodeSpace/GIA-Re/docs/xml_ë¶„ì„"  # XML íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬
CSV_PATH = "c:/Users/yujin/CodeSpace/GIA-Re/Network-Management-System-main/dataset/test_fin.csv"  # í‰ê°€ ë°ì´í„°ì…‹

# ğŸ›ï¸ ì‹¤í—˜ íŒŒë¼ë¯¸í„° ì„¤ì •
COLLECTION_NAME = "network_devices"  # ChromaDB ì»¬ë ‰ì…˜ ì´ë¦„
MAX_ITERATIONS = 3  # RAG íŒŒì´í”„ë¼ì¸ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
TOP_K_VALUES = [5, 10, 20]  # RAGì—ì„œ í…ŒìŠ¤íŠ¸í•  Top-K ê°’ë“¤

# ğŸ¤– ëª¨ë¸ ì„¤ì •
EMBEDDING_MODEL = "Qwen/Qwen3-Embedding-8B"  # ì„ë² ë”© ëª¨ë¸
EMBEDDING_DEVICE = "cuda:1"  # ì„ë² ë”© ëª¨ë¸ ì‹¤í–‰ ë””ë°”ì´ìŠ¤
EMBEDDING_BATCH_SIZE = 8  # ì„ë² ë”© ë°°ì¹˜ í¬ê¸°
LLM_MODEL = "gpt-4o-mini"  # ë©”ì¸ LLM ëª¨ë¸
LLM_TEMPERATURE = 0.05  # LLM Temperature

# ğŸ“Š Non-RAG ì„¤ì •
NON_RAG_USE_EMBEDDING = True  # Non-RAGì—ì„œ ì„ë² ë”© ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ ì‚¬ìš© ì—¬ë¶€
NON_RAG_MAX_DOCS = 5  # Non-RAGì—ì„œ ì„ íƒí•  ìµœëŒ€ ë¬¸ì„œ ìˆ˜
NON_RAG_CHUNK_SIZE = 1500  # ì²­í¬ í¬ê¸° (í† í° ë‹¨ìœ„)

# ğŸ”§ ê¸°íƒ€ ì„¤ì •
OPENAI_EMBED_MODEL = "text-embedding-3-large"  # OpenAI ì„ë² ë”© ëª¨ë¸ (ì‚¬ìš© ì•ˆí•¨)
EMBED_DIMS = None  # ì„ë² ë”© ì°¨ì› (Noneì´ë©´ ëª¨ë¸ ê¸°ë³¸ê°’)
EXPERIMENT_BASE_DIR = "experiment_results"  # ì‹¤í—˜ ê²°ê³¼ ì €ì¥ ë””ë ‰í† ë¦¬

# ============================================================================
# í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ Import
# ============================================================================

from openai import OpenAI
from datetime import datetime
from multiprocessing import Process, Queue
from typing import List, Dict, Optional, Tuple
from langchain_core.tools import Tool
from langchain_google_community import GoogleSearchAPIWrapper
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers import Html2TextTransformer
from langchain_huggingface import HuggingFaceEmbeddings

# í‰ê°€ë¥¼ ìœ„í•œ ì¶”ê°€ ë¼ì´ë¸ŒëŸ¬ë¦¬
from bert_score import score as bert_score
from rouge import Rouge
from pathlib import Path
import json
import os
import time
import sys
import traceback
import pandas as pd
import re
import glob
import tiktoken
import chromadb
from io import StringIO
import contextlib

# ============================================================================
# í™˜ê²½ ë³€ìˆ˜ ì„¤ì • (ìœ„ì˜ ì„¤ì •ê°’ë“¤ì„ ì ìš©)
# ============================================================================

os.environ["GOOGLE_CSE_ID"] = GOOGLE_CSE_ID
os.environ["GOOGLE_API_KEY"] = GOOGLE_API_KEY  
os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY

# ============================================================================
# ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ë° ì „ì—­ ë³€ìˆ˜
# ============================================================================

# System prompt for network engineering assistant
chatgpt_system_prompt = """You are an expert network engineering assistant with deep knowledge of 
network configurations, troubleshooting, and security best practices. You have access to various 
network device configurations, XML schemas, and technical documentation."""

# ì „ì—­ ë³€ìˆ˜ë“¤
experiment_logger = None
tracked_openai_client = None

class ExperimentLogger:
    """ì‹¤í—˜ ë¡œê·¸ë¥¼ ì²´ê³„ì ìœ¼ë¡œ ê´€ë¦¬í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, experiment_name: str, base_dir: str = "experiment_results"):
        self.experiment_name = experiment_name
        self.timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.base_dir = Path(base_dir)
        
        # ì‹¤í—˜ë³„ ë””ë ‰í† ë¦¬ êµ¬ì¡° ìƒì„±
        self.exp_dir = self.base_dir / f"{experiment_name}_{self.timestamp}"
        self.exp_dir.mkdir(parents=True, exist_ok=True)
        
        # í•˜ìœ„ ë””ë ‰í† ë¦¬ë“¤
        self.logs_dir = self.exp_dir / "logs"
        self.results_dir = self.exp_dir / "results"
        self.llm_history_dir = self.exp_dir / "llm_history"
        self.console_dir = self.exp_dir / "console_output"
        
        for dir_path in [self.logs_dir, self.results_dir, self.llm_history_dir, self.console_dir]:
            dir_path.mkdir(exist_ok=True)
        
        # ì½˜ì†” ì¶œë ¥ ìº¡ì²˜ë¥¼ ìœ„í•œ ì„¤ì •
        self.console_buffer = StringIO()
        self.original_stdout = sys.stdout
        
        # LLM í˜¸ì¶œ íˆìŠ¤í† ë¦¬
        self.llm_calls = []
        
        print(f"[INFO] Experiment '{experiment_name}' initialized")
        print(f"[INFO] Results will be saved to: {self.exp_dir}")
    
    def start_console_capture(self):
        """ì½˜ì†” ì¶œë ¥ ìº¡ì²˜ ì‹œì‘"""
        sys.stdout = self.console_buffer
    
    def stop_console_capture(self):
        """ì½˜ì†” ì¶œë ¥ ìº¡ì²˜ ì¢…ë£Œ ë° íŒŒì¼ ì €ì¥"""
        sys.stdout = self.original_stdout
        console_content = self.console_buffer.getvalue()
        
        if console_content:
            console_file = self.console_dir / f"console_{self.timestamp}.txt"
            with open(console_file, 'w', encoding='utf-8') as f:
                f.write(console_content)
            print(f"[INFO] Console output saved to: {console_file}")
        
        self.console_buffer = StringIO()  # ë²„í¼ ë¦¬ì…‹
    
    def log_llm_call(self, call_type: str, prompt: str, response: str, model: str = "gpt-4o-mini", metadata: Dict = None):
        """LLM í˜¸ì¶œ ê¸°ë¡"""
        call_record = {
            "timestamp": datetime.now().isoformat(),
            "call_type": call_type,
            "model": model,
            "prompt": prompt,
            "response": response,
            "metadata": metadata or {}
        }
        self.llm_calls.append(call_record)
        
        # ê°œë³„ LLM í˜¸ì¶œ íŒŒì¼ë¡œë„ ì €ì¥
        call_file = self.llm_history_dir / f"llm_call_{len(self.llm_calls):03d}_{call_type}.json"
        with open(call_file, 'w', encoding='utf-8') as f:
            json.dump(call_record, f, ensure_ascii=False, indent=2)
    
    def save_detailed_log(self, log_data: List[Dict], filename: str = None):
        """ìƒì„¸ ë¡œê·¸ ì €ì¥"""
        if filename is None:
            filename = f"detailed_log_{self.timestamp}.json"
        
        log_file = self.logs_dir / filename
        with open(log_file, 'w', encoding='utf-8') as f:
            json.dump(log_data, f, ensure_ascii=False, indent=2)
        
        print(f"[INFO] Detailed log saved to: {log_file}")
    
    def save_results(self, results: Dict, filename: str = None):
        """ì‹¤í—˜ ê²°ê³¼ ì €ì¥"""
        if filename is None:
            filename = f"results_{self.timestamp}.json"
        
        results_file = self.results_dir / filename
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"[INFO] Results saved to: {results_file}")
    
    def save_evaluation_report(self, evaluation_data: Dict, filename: str = None):
        """í‰ê°€ ê²°ê³¼ ë¦¬í¬íŠ¸ ìƒì„± ë° ì €ì¥"""
        if filename is None:
            filename = f"evaluation_report_{self.timestamp}.md"
        
        report_file = self.results_dir / filename
        
        # Markdown í˜•íƒœì˜ ë¦¬í¬íŠ¸ ìƒì„±
        report_content = self._generate_evaluation_markdown(evaluation_data)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print(f"[INFO] Evaluation report saved to: {report_file}")
        return report_file
    
    def _generate_evaluation_markdown(self, evaluation_data: Dict) -> str:
        """í‰ê°€ ê²°ê³¼ë¥¼ Markdown í˜•íƒœë¡œ ìƒì„±"""
        content = []
        content.append(f"# ì‹¤í—˜ í‰ê°€ ë¦¬í¬íŠ¸")
        content.append(f"**ì‹¤í—˜ëª…**: {self.experiment_name}")
        content.append(f"**ì‹¤í–‰ ì‹œê°„**: {self.timestamp}")
        content.append(f"**ìƒì„± ì¼ì‹œ**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        content.append("")
        
        # ì‹¤í—˜ ê°œìš”
        if 'experiment_overview' in evaluation_data:
            overview = evaluation_data['experiment_overview']
            content.append("## ğŸ“Š ì‹¤í—˜ ê°œìš”")
            content.append(f"- **ì´ ì§ˆë¬¸ ìˆ˜**: {overview.get('total_questions', 'N/A')}")
            content.append(f"- **ì‹¤í—˜ ë°©ë²•**: {overview.get('methods', 'N/A')}")
            content.append(f"- **í‰ê°€ ë©”íŠ¸ë¦­**: {overview.get('metrics', 'N/A')}")
            content.append("")
        
        # Non-RAG ê²°ê³¼
        if 'non_rag' in evaluation_data:
            content.append("## ğŸ” Non-RAG íŒŒì´í”„ë¼ì¸ ê²°ê³¼")
            non_rag = evaluation_data['non_rag']['evaluation']
            
            content.append("### ì „ì²´ ì„±ëŠ¥")
            content.append(f"- **Exact Match**: {non_rag['overall']['exact_match']:.4f}")
            content.append(f"- **F1 Score**: {non_rag['overall']['f1_score']:.4f}")
            content.append(f"- **í‰ê·  ì²˜ë¦¬ ì‹œê°„**: {evaluation_data['non_rag']['avg_processing_time']:.2f}ì´ˆ")
            content.append("")
            
            if non_rag['rule_based']['question_count'] > 0:
                content.append("### Rule-based ì§ˆë¬¸ ì„±ëŠ¥")
                content.append(f"- **Exact Match**: {non_rag['rule_based']['exact_match']:.4f}")
                content.append(f"- **F1 Score**: {non_rag['rule_based']['f1_score']:.4f}")
                content.append(f"- **ì§ˆë¬¸ ìˆ˜**: {non_rag['rule_based']['question_count']}")
                content.append("")
            
            if non_rag['enhanced_llm']['question_count'] > 0:
                content.append("### Enhanced LLM ì§ˆë¬¸ ì„±ëŠ¥")
                content.append(f"- **Ground Truth EM**: {non_rag['enhanced_llm']['ground_truth']['exact_match']:.4f}")
                content.append(f"- **Ground Truth F1**: {non_rag['enhanced_llm']['ground_truth']['f1_score']:.4f}")
                if non_rag['enhanced_llm']['explanation']['valid_count'] > 0:
                    content.append(f"- **Explanation BERT F1**: {non_rag['enhanced_llm']['explanation']['bert_f1']:.4f}")
                    content.append(f"- **Explanation ROUGE-1**: {non_rag['enhanced_llm']['explanation']['rouge_1_f1']:.4f}")
                content.append(f"- **ì§ˆë¬¸ ìˆ˜**: {non_rag['enhanced_llm']['question_count']}")
                content.append("")
        
        # RAG ê²°ê³¼
        if 'rag' in evaluation_data:
            content.append("## ğŸ¯ RAG íŒŒì´í”„ë¼ì¸ ê²°ê³¼")
            
            for top_k, data in evaluation_data['rag'].items():
                if 'top_k_' in top_k:
                    k_value = top_k.split('_')[-1]
                    rag_eval = data['evaluation']
                    
                    content.append(f"### Top-K = {k_value}")
                    content.append(f"- **ì „ì²´ EM**: {rag_eval['overall']['exact_match']:.4f}")
                    content.append(f"- **ì „ì²´ F1**: {rag_eval['overall']['f1_score']:.4f}")
                    content.append(f"- **í‰ê·  ì²˜ë¦¬ ì‹œê°„**: {data['avg_processing_time']:.2f}ì´ˆ")
                    
                    if rag_eval['rule_based']['question_count'] > 0:
                        content.append(f"- **Rule-based EM**: {rag_eval['rule_based']['exact_match']:.4f}")
                        content.append(f"- **Rule-based F1**: {rag_eval['rule_based']['f1_score']:.4f}")
                    
                    if rag_eval['enhanced_llm']['question_count'] > 0:
                        content.append(f"- **Enhanced LLM GT EM**: {rag_eval['enhanced_llm']['ground_truth']['exact_match']:.4f}")
                        content.append(f"- **Enhanced LLM GT F1**: {rag_eval['enhanced_llm']['ground_truth']['f1_score']:.4f}")
                        if rag_eval['enhanced_llm']['explanation']['valid_count'] > 0:
                            content.append(f"- **Explanation BERT F1**: {rag_eval['enhanced_llm']['explanation']['bert_f1']:.4f}")
                    
                    content.append("")
        
        # ì„±ëŠ¥ ë¹„êµ í…Œì´ë¸”
        content.append("## ğŸ“ˆ ì„±ëŠ¥ ë¹„êµ ìš”ì•½")
        content.append("| ë°©ë²• | ì „ì²´ EM | ì „ì²´ F1 | Rule-based EM | Enhanced LLM EM | í‰ê·  ì²˜ë¦¬ì‹œê°„ |")
        content.append("|------|---------|---------|---------------|-----------------|---------------|")
        
        # Non-RAG í–‰
        if 'non_rag' in evaluation_data:
            nr = evaluation_data['non_rag']['evaluation']
            content.append(f"| Non-RAG | {nr['overall']['exact_match']:.4f} | {nr['overall']['f1_score']:.4f} | {nr['rule_based']['exact_match']:.4f} | {nr['enhanced_llm']['ground_truth']['exact_match']:.4f} | {evaluation_data['non_rag']['avg_processing_time']:.2f}ì´ˆ |")
        
        # RAG í–‰ë“¤
        if 'rag' in evaluation_data:
            for top_k, data in evaluation_data['rag'].items():
                if 'top_k_' in top_k:
                    k_value = top_k.split('_')[-1]
                    r = data['evaluation']
                    content.append(f"| RAG (k={k_value}) | {r['overall']['exact_match']:.4f} | {r['overall']['f1_score']:.4f} | {r['rule_based']['exact_match']:.4f} | {r['enhanced_llm']['ground_truth']['exact_match']:.4f} | {data['avg_processing_time']:.2f}ì´ˆ |")
        
        content.append("")
        content.append("## ğŸ“‹ ë¶„ì„ ê²°ê³¼")
        content.append("### ì£¼ìš” ë°œê²¬ì‚¬í•­")
        content.append("- TODO: ìë™ ë¶„ì„ ê²°ê³¼ ì¶”ê°€")
        content.append("")
        content.append("### ê¶Œì¥ì‚¬í•­")
        content.append("- TODO: ì„±ëŠ¥ ê°œì„  ê¶Œì¥ì‚¬í•­ ì¶”ê°€")
        content.append("")
        
        return "\n".join(content)
    
    def save_llm_history_summary(self):
        """LLM í˜¸ì¶œ íˆìŠ¤í† ë¦¬ ìš”ì•½ ì €ì¥"""
        summary_file = self.llm_history_dir / f"llm_history_summary_{self.timestamp}.json"
        
        # í˜¸ì¶œ ìœ í˜•ë³„ í†µê³„
        call_stats = {}
        for call in self.llm_calls:
            call_type = call['call_type']
            if call_type not in call_stats:
                call_stats[call_type] = {
                    'count': 0,
                    'total_prompt_length': 0,
                    'total_response_length': 0
                }
            
            call_stats[call_type]['count'] += 1
            call_stats[call_type]['total_prompt_length'] += len(call['prompt'])
            call_stats[call_type]['total_response_length'] += len(call['response'])
        
        summary = {
            "total_calls": len(self.llm_calls),
            "call_types": list(call_stats.keys()),
            "statistics": call_stats,
            "first_call": self.llm_calls[0]['timestamp'] if self.llm_calls else None,
            "last_call": self.llm_calls[-1]['timestamp'] if self.llm_calls else None
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
        
        print(f"[INFO] LLM history summary saved to: {summary_file}")
    
    def finalize_experiment(self):
        """ì‹¤í—˜ ì¢…ë£Œ ë° ìµœì¢… ì •ë¦¬"""
        self.stop_console_capture()
        self.save_llm_history_summary()
        
        # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            "experiment_name": self.experiment_name,
            "timestamp": self.timestamp,
            "total_llm_calls": len(self.llm_calls),
            "experiment_duration": "TODO: ì‹¤í—˜ ì‹œê°„ ê³„ì‚°",
            "directories": {
                "base": str(self.exp_dir),
                "logs": str(self.logs_dir),
                "results": str(self.results_dir),
                "llm_history": str(self.llm_history_dir),
                "console": str(self.console_dir)
            }
        }
        
        metadata_file = self.exp_dir / "experiment_metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, ensure_ascii=False, indent=2)
        
        print(f"\n{'='*70}")
        print(f"ì‹¤í—˜ '{self.experiment_name}' ì™„ë£Œ!")
        print(f"ê²°ê³¼ ë””ë ‰í† ë¦¬: {self.exp_dir}")
        print(f"ì´ LLM í˜¸ì¶œ: {len(self.llm_calls)}íšŒ")
        print(f"{'='*70}")

class TrackedOpenAIClient:
    """LLM í˜¸ì¶œì„ ì¶”ì í•˜ëŠ” OpenAI í´ë¼ì´ì–¸íŠ¸ ë˜í¼"""
    
    def __init__(self, logger: ExperimentLogger):
        self.client = OpenAI()
        self.logger = logger
    
    def chat_completions_create(self, call_type: str, **kwargs):
        """ì±„íŒ… ì™„ì„± í˜¸ì¶œ ë° ë¡œê¹…"""
        # í”„ë¡¬í”„íŠ¸ ì¶”ì¶œ
        messages = kwargs.get('messages', [])
        prompt = "\n".join([f"{msg['role']}: {msg['content']}" for msg in messages])
        
        # API í˜¸ì¶œ
        response = self.client.chat.completions.create(**kwargs)
        response_text = response.choices[0].message.content
        
        # ë¡œê¹…
        metadata = {
            "model": kwargs.get('model', 'unknown'),
            "temperature": kwargs.get('temperature', 'unknown'),
            "prompt_tokens": len(prompt.split()) if prompt else 0,
            "response_tokens": len(response_text.split()) if response_text else 0
        }
        
        self.logger.log_llm_call(
            call_type=call_type,
            prompt=prompt,
            response=response_text,
            model=kwargs.get('model', 'unknown'),
            metadata=metadata
        )
        
        return response

# ============================================================================
# ì‹¤í—˜ ë¡œê¹… ë° ì¶”ì  í´ë˜ìŠ¤ë“¤
# ============================================================================

class OpenAIEmbedder:
    """OpenAI ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤"""
    def __init__(self, model, dims: int | None = EMBED_DIMS):
        self.client = OpenAI()
        self.model = model
        self.dims = dims

    def embed(self, texts: list[str] | str) -> list[list[float]]:
        if isinstance(texts, str):
            texts = [texts]
        resp = self.client.embeddings.create(
            model=self.model,
            input=texts,
            **({"dimensions": self.dims} if self.dims else {})
        )
        return [d.embedding for d in resp.data]
    
class HuggingFaceEmbedder:
    def __init__(self, model_name=EMBEDDING_MODEL, device=EMBEDDING_DEVICE, batch_size=EMBEDDING_BATCH_SIZE):
        self.model_name = model_name
        self.device = device
        self.batch_size = batch_size
        
        self.embedder = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={"device": device},
            encode_kwargs={"batch_size": batch_size}
        )

        self.model_name = model_name
        self.device = device
        self.batch_size = batch_size

    def embed(self, texts: list[str] | str) -> list[list[float]]:
        if isinstance(texts, str):
            texts = [texts]
        return self.embedder.embed_documents(texts)

class ChromaDB:
    """ì‚¬ì „ ì„ë² ë”©ëœ XML íŒŒì¼ë“¤ì„ ìœ„í•œ ChromaDB ì¸í„°í˜ì´ìŠ¤"""
    def __init__(self, db_path: str, collection_name: str, embedder: object, xml_directory: str = None):
        self.db_path = db_path
        self.xml_directory = xml_directory
        self.client = chromadb.PersistentClient(path=db_path)
        self.embedder = embedder or OpenAIEmbedder()
        
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ (ì‚¬ì „ ì„ë² ë”©ëœ XML ë°ì´í„°)
            self.collection = self.client.get_collection(name=collection_name)
            print(f"[INFO] Loaded existing collection: {collection_name}")
            print(f"[INFO] Total documents in collection: {self.collection.count()}")
            
            # ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆë‹¤ë©´ ìë™ ì„ë² ë”© ìˆ˜í–‰
            if self.collection.count() == 0 and self.xml_directory:
                print(f"[INFO] Collection is empty. Auto-embedding XML files from: {self.xml_directory}")
                self._auto_embed_xml_files()
                
        except:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            self.collection = self.client.create_collection(name=collection_name)
            print(f"[INFO] Created new collection: {collection_name}")
            
            # XML ë””ë ‰í† ë¦¬ê°€ ì œê³µë˜ì—ˆë‹¤ë©´ ìë™ ì„ë² ë”© ìˆ˜í–‰
            if self.xml_directory:
                print(f"[INFO] Auto-embedding XML files from: {self.xml_directory}")
                self._auto_embed_xml_files()

    def add_docs(self, ids: list[str], docs: list[str], metadatas: list[dict] | None = None):
        """ìƒˆ ë¬¸ì„œ ì¶”ê°€ (í•„ìš”ì‹œ)"""
        embeddings = self.embedder.embed(docs)
        self.collection.add(ids=ids, documents=docs, embeddings=embeddings, metadatas=metadatas)

    def query(self, text: str, n_results: int = 5) -> Dict:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        q_emb = self.embedder.embed(text)
        return self.collection.query(query_embeddings=q_emb, n_results=n_results)
    
    def _auto_embed_xml_files(self):
        """XML ë””ë ‰í† ë¦¬ì˜ ëª¨ë“  XML íŒŒì¼ì„ ìë™ìœ¼ë¡œ ì„ë² ë”©"""
        if not self.xml_directory or not os.path.exists(self.xml_directory):
            print(f"[WARNING] XML directory not found: {self.xml_directory}")
            return
            
        xml_files = []
        for root, dirs, files in os.walk(self.xml_directory):
            for file in files:
                if file.lower().endswith('.xml'):
                    xml_files.append(os.path.join(root, file))
        
        if not xml_files:
            print(f"[WARNING] No XML files found in: {self.xml_directory}")
            return
            
        print(f"[INFO] Found {len(xml_files)} XML files. Starting auto-embedding...")
        print(f"[INFO] This may take several minutes depending on the number and size of files.")
        
        batch_size = 5  # ë” ì‘ì€ ë°°ì¹˜ í¬ê¸°ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê°ì†Œ
        total_chunks = 0
        failed_files = []
        
        for i, xml_file in enumerate(xml_files):
            try:
                print(f"[INFO] Processing file {i+1}/{len(xml_files)}: {os.path.basename(xml_file)}")
                
                # XML íŒŒì¼ ì½ê¸° (ë‹¤ì–‘í•œ ì¸ì½”ë”© ì‹œë„)
                content = None
                for encoding in ['utf-8', 'utf-8-sig', 'latin-1', 'cp1252']:
                    try:
                        with open(xml_file, 'r', encoding=encoding) as f:
                            content = f.read()
                        break
                    except UnicodeDecodeError:
                        continue
                
                if content is None:
                    print(f"[ERROR] Could not read file {xml_file} with any encoding")
                    failed_files.append(xml_file)
                    continue
                
                # í…ìŠ¤íŠ¸ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = chunk_texts(content, chunk_size=NON_RAG_CHUNK_SIZE)
                print(f"[INFO] Split into {len(chunks)} chunks")
                
                # ë°°ì¹˜ë³„ë¡œ ì²˜ë¦¬
                for batch_start in range(0, len(chunks), batch_size):
                    batch_chunks = chunks[batch_start:batch_start + batch_size]
                    
                    # IDì™€ ë©”íƒ€ë°ì´í„° ìƒì„±
                    batch_ids = []
                    batch_metadatas = []
                    
                    for j, chunk in enumerate(batch_chunks):
                        chunk_id = f"{os.path.basename(xml_file)}_chunk_{batch_start + j}"
                        batch_ids.append(chunk_id)
                        
                        metadata = {
                            "filename": os.path.basename(xml_file),
                            "file_path": xml_file,
                            "chunk_index": batch_start + j,
                            "total_chunks": len(chunks),
                            "source": "auto_embedded",
                            "source_directory": self.xml_directory
                        }
                        batch_metadatas.append(metadata)
                    
                    try:
                        # ì„ë² ë”© ë° ì €ì¥
                        self.add_docs(batch_ids, batch_chunks, batch_metadatas)
                        total_chunks += len(batch_chunks)
                        
                        if total_chunks % 25 == 0:  # ì§„í–‰ ìƒí™© í‘œì‹œ
                            print(f"[INFO] Embedded {total_chunks} chunks so far...")
                            
                    except Exception as e:
                        print(f"[ERROR] Failed to embed batch for {xml_file}: {e}")
                        continue
                        
            except Exception as e:
                print(f"[ERROR] Failed to process {xml_file}: {e}")
                failed_files.append(xml_file)
                continue
        
        print(f"[INFO] Auto-embedding complete!")
        print(f"[INFO] Successfully processed: {len(xml_files) - len(failed_files)}/{len(xml_files)} files")
        print(f"[INFO] Total chunks embedded: {total_chunks}")
        print(f"[INFO] Collection now contains: {self.collection.count()} documents")
        
        if failed_files:
            print(f"[WARNING] Failed to process {len(failed_files)} files:")
            for file in failed_files[:5]:  # ì²˜ìŒ 5ê°œë§Œ í‘œì‹œ
                print(f"  - {os.path.basename(file)}")
            if len(failed_files) > 5:
                print(f"  ... and {len(failed_files) - 5} more files")

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    encoding = tiktoken.get_encoding(encoding_name)
    return len(encoding.encode(string))

def chunk_texts(text: str, chunk_size: int = NON_RAG_CHUNK_SIZE) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ í† í° í¬ê¸°ì— ë§ê²Œ ë¶„í• """
    tokens = num_tokens_from_string(text)
    if tokens <= chunk_size:
        return [text]
    
    texts = []
    n = (tokens // chunk_size) + 1
    part_length = len(text) // n
    extra = len(text) % n
    
    parts = []
    start = 0
    for i in range(n):
        end = start + part_length + (1 if i < extra else 0)
        parts.append(text[start:end].replace('\n', " "))
        start = end
    
    return parts

def get_reranked_indices(question: str, documents: List[str], top_n: int = 5) -> List[int]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ 1ì°¨ ê²€ìƒ‰ ë¬¸ì„œë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ì¬ì •ë ¬í•˜ê³  ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        documents: 1ì°¨ ê²€ìƒ‰ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸ ëª©ë¡
        top_n: ìµœì¢… ì„ íƒí•  ë¬¸ì„œ ìˆ˜

    Returns:
        ì¬ì •ë ¬ëœ ìƒìœ„ ë¬¸ì„œì˜ 0-based ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not documents:
        return []

    # LLMì— ì „ë‹¬í•  ë¬¸ì„œ í¬ë§·íŒ…
    docs_with_indices = [f"Doc[{i+1}]:\n{doc}" for i, doc in enumerate(documents)]
    docs_str = "\n\n".join(docs_with_indices)

    rerank_prompt = f"""
    You are an expert document analyst. Re-rank the documents by relevance to the user question.

    User Question: "{question}"

    Provided Documents:
    {docs_str}

    Instructions:
    1. Read the question and all documents carefully.
    2. Choose the MOST relevant documents that directly help answer the question.
    3. Output a comma-separated list of document numbers only (e.g., Doc[5],Doc[2],Doc[8]) in descending importance.
    4. Return up to {top_n} documents. No explanations.
    """

    try:
        response = tracked_openai_client.chat_completions_create(
            call_type="document_reranking",
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": "You are a helpful assistant that ranks documents."},
                {"role": "user", "content": rerank_prompt}
            ],
            temperature=0.0
        )
        ranked_doc_indices_str = response.choices[0].message.content.strip()

        # ì˜ˆ: "Doc[5],Doc[2]" -> [4, 1]
        ranked_indices: List[int] = []
        matches = re.findall(r'Doc\[(\d+)\]', ranked_doc_indices_str)
        for m in matches:
            idx = int(m) - 1
            if 0 <= idx < len(documents):
                ranked_indices.append(idx)

        # ë¹„ì–´ìˆê±°ë‚˜ ì¼ë¶€ë§Œ ë°˜í™˜ ì‹œ ë³´ì™„
        if not ranked_indices:
            ranked_indices = list(range(min(top_n, len(documents))))

        return ranked_indices[:top_n]
    except Exception as e:
        print(f"[ERROR] Failed to re-rank documents: {e}")
        return list(range(min(top_n, len(documents))))

def get_reranked_documents(question: str, documents: List[str], top_n: int = 5) -> List[str]:
    """ë¬¸ì„œ ëª©ë¡ì„ LLMìœ¼ë¡œ ì¬ì •ë ¬í•˜ì—¬ ìƒìœ„ top_n ë¬¸ì„œë¥¼ ë°˜í™˜."""
    idx = get_reranked_indices(question, documents, top_n=top_n)
    return [documents[i] for i in idx]

def get_classification_result(question: str) -> str:
    """ì‚¬ìš©ì ì…ë ¥ì„ 2ê°€ì§€ ì‘ì—… ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜"""
    classification_prompt = '''
    You are an excellent network engineering assistant. Classify the question into ONE of these categories:

    1. **Simple Lookup Tasks** 
        - Tasks that can be solved by referring to or retrieving information from network configuration XML files
        - Simple lookup tasks: Executing commands for the purpose of retrieving device status, routing tables, interfaces, sessions, logs, etc

    2. **Other Tasks** 
        - All other cases that do not rely on network configuration XML files
        - System control commands: performing configuration changes, applying policies, and executing device control commands
        - Technical errors: conducting fault analysis, identifying root causes, executing recovery procedures, and resolving issues based on logs
        - Configuration/Policy review and optimization: reviewing network configurations and optimizing routing/security policies
        - Security/Audit response: analyzing security events, performing compliance checks, and handling audit responses

    IMPORTANT:
    - Return ONLY the exact category name, nothing else.
    ex) Simple Lookup Tasks
    ex) Other Tasks
    '''
    
    response = tracked_openai_client.chat_completions_create(
        call_type="task_classification",
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nInstruction: {classification_prompt}"}
        ],
        temperature=LLM_TEMPERATURE
    )
    
    task_type = response.choices[0].message.content.strip()
    print(f"[INFO] Task classified as: {task_type}")
    return task_type

def get_draft(question: str, task_type: str) -> str:
    """ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ë§ì¶¤í˜• ì´ˆì•ˆ ìƒì„±"""

    if task_type == "Simple Lookup Tasks":
        prompt = '''
            This is a simple lookup query. Provide a direct answer based on your knowledge.

            FORMAT REQUIREMENTS:
            [GROUND_TRUTH]
            {{EXACT_VALUE_ONLY - no labels or explanations}}

            [EXPLANATION]  
            {{Brief technical explanation in Korean}}

            EXAMPLES:
            - IP addresses: "192.168.1.1"
            - Device names: "CE1, sample7"
            - Port numbers: "22, 80, 443"
            - Boolean: "Yes" or "No"
        '''
    else:
        prompt = '''
            Provide a comprehensive technical answer following this structure:

            [GROUND_TRUTH]
            {{Exact technical values only}}

            [EXPLANATION]
            {{Detailed technical implementation in Korean}}
            1. Direct Answer: State the solution clearly
            2. Technical Implementation: Provide specific steps or configurations  
            3. Considerations: Note any important factors or best practices

            Use proper network engineering terminology and be specific with commands/configurations.
        '''
        
    response = tracked_openai_client.chat_completions_create(
        call_type="initial_draft",
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\n{prompt}"}
        ],
        temperature=0.2
    )
    
    return response.choices[0].message.content

def get_xml_query(question: str, answer: str) -> str:
    """ChromaDBì˜ XML íŒŒì¼ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    query_prompt = '''
        You are responsible for generating search queries in ChromaDB to find relevant XML network configuration files and related documents.

        CONTEXT:
            User Question: {question}
            Search History (information found or concluded so far): {answer}

        OBJECTIVE:
            Generate more effective search queries that expand beyond what has already been attempted or concluded in the history.

        OUTPUT INSTRUCTIONS:
            - Output ONLY the search query string.
            - Do not include explanations, reasoning, or extra text.
        '''
    
    response = tracked_openai_client.chat_completions_create(
        call_type="xml_query_generation",
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nContent: {answer}\n\nInstruction: {query_prompt}"}
        ],
        temperature=LLM_TEMPERATURE
    )
    
    return response.choices[0].message.content.strip()

def get_internet_query(question: str, answer: str) -> str:
    """ì¸í„°ë„· ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    query_prompt = '''
        You are responsible for generating a Google search query to validate and reinforce the technical accuracy of the given network engineering answer.

        CONTEXT:
            User Question: {question}
            Proposed Answer: {answer}

        OBJECTIVE:
            - Find authoritative and reliable references that can support, confirm, or refine the proposed answer.
            - Prioritize high-trust sources such as:
                * Official vendor documentation (Cisco, Juniper, Arista, Huawei, etc.)
                * Standards bodies (IETF RFCs, IEEE standards)
                * Well-known security advisories (NVD, CERT, vendor advisories)
                * Recognized industry knowledge bases (Cisco Community, Juniper TechLibrary, etc.)
            - Ensure the query is precise, technical, and aimed at verifying correctness.
            - If possible, emphasize recency to capture the latest best practices, features, or advisories.

        OUTPUT INSTRUCTIONS:
            - Output ONLY the search query string.
            - Do not include explanations, reasoning, or extra text.
        '''
    
    response = tracked_openai_client.chat_completions_create(
        call_type="internet_query_generation",
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nContent: {answer}\n\nInstruction: {query_prompt}"}
        ],
        temperature=0.1
    )
    
    return response.choices[0].message.content.strip()


def get_google_search(query: str = "", k: int = 3) -> Optional[List[Dict]]:
    """ìˆ˜ì •ëœ Google Search API í•¨ìˆ˜"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print(f"[DEBUG] Google search query: '{query}' (k={k})")
        
        # API í‚¤ í™•ì¸
        api_key = os.environ.get("GOOGLE_API_KEY")
        cse_id = os.environ.get("GOOGLE_CSE_ID")
        
        if not api_key:
            print("[ERROR] GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
            
        if not cse_id:
            print("[ERROR] GOOGLE_CSE_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # GoogleSearchAPIWrapper ìƒì„±
        search = GoogleSearchAPIWrapper(
            k=k,
            google_api_key=api_key,
            google_cse_id=cse_id
        )
        
        # ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
        def search_results(query):
            try:
                results = search.results(query, k)
                print(f"[DEBUG] Search results count: {len(results) if results else 0}")
                return results
            except Exception as e:
                print(f"[ERROR] search.results() í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                return None
        
        # Tool ìƒì„±
        tool = Tool(
            name="Google Search Snippets",
            description="Search Google for recent results.",
            func=search_results,
        )
        
        # ê²€ìƒ‰ ì‹¤í–‰
        ref_text = tool.run(query)
        
        if len(ref_text) > 0:
            # ê²°ê³¼ ê²€ì¦ - ì²« ë²ˆì§¸ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            first_item = ref_text[0]
            if isinstance(first_item, dict):
                # 'Result' í‚¤ê°€ ì—†ìœ¼ë©´ ìœ íš¨í•œ ê²°ê³¼ë¡œ íŒë‹¨ (ì›ë˜ ì¡°ê±´ ìˆ˜ì •)
                if 'Result' not in first_item:
                    print(f"[SUCCESS] Valid search results found: {len(ref_text)} items")
                    return ref_text
                else:
                    print("[WARNING] Results contain 'Result' key - treating as invalid")
                    return None
            else:
                # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆì–´ë„ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë°˜í™˜
                print(f"[INFO] Non-dict results found: {len(ref_text)} items")
                return ref_text
        else:
            print("[INFO] No search results returned")
            return None
            
    except ImportError as e:
        print(f"[ERROR] í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install langchain-google-community")
        return None
    except Exception as e:
        print(f"[ERROR] Google search ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def get_page_content(link: str) -> Optional[str]:
    """ì›¹ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ"""
    try:
        loader = AsyncHtmlLoader([link])
        docs = loader.load()
        html2text = Html2TextTransformer()
        docs_transformed = html2text.transform_documents(docs)
        if len(docs_transformed) > 0:
            return docs_transformed[0].page_content
        else:
            return None
    except Exception as e:
        print(f"[ERROR] Failed to fetch page content: {e}")
        return None
    
def get_internet_content(query: str) -> Optional[List[str]]:
    """Google ê²€ìƒ‰ì„ í†µí•´ ì¸í„°ë„· ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
    search_results = get_google_search(query, k=3)
    
    if not search_results:
        print("[INFO] No Google search results found")
        return None
    
    all_content = []
    for result in search_results[:2]:  # ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ ì²˜ë¦¬
        link = result.get('link')
        if link:
            print(f"[INFO] Fetching content from: {link}")
            page_content = get_page_content(link)
            if page_content:
                # ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = chunk_texts(page_content, NON_RAG_CHUNK_SIZE)
                all_content.extend(chunks[:2])  # ê° í˜ì´ì§€ì—ì„œ ìµœëŒ€ 2ê°œ ì²­í¬
    
    return all_content if all_content else None

def get_revise_answer(question: str, answer: str, content: str, task_type: str) -> str:
    """ì°¸ì¡° ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìˆ˜ì •"""
    if task_type == "Simple Lookup Tasks":
        prompt = '''
            You are revising a network engineering answer using reference content.

            CONTEXT:
                User Question: {question}
                Current Answer: {answer}
                Reference Content: {content}

            INSTRUCTIONS:
                - Use reference content to verify and refine the answer
                - For ground_truth sections: Ensure EXACT values only (no labels)
                - For explanation sections: Add technical details from reference
                - Maintain the [GROUND_TRUTH] and [EXPLANATION] format
                - Be extremely precise with technical values
                - Extract exact device names, IPs, ports from reference content

            OUTPUT: Return the complete revised answer in the same format.
        '''
    else:
        prompt = '''
            Revise the network engineering answer using reference content for technical accuracy.

            Guidelines:
            - Verify all technical details against the reference
            - Ensure [GROUND_TRUTH] contains only exact values
            - Enhance [EXPLANATION] with comprehensive technical details
            - Correct any errors or outdated practices
            - Maintain precise technical formatting
            - Focus on actionable network configuration information

            OUTPUT: Return the complete revised answer maintaining the format.
        '''
        
    response = tracked_openai_client.chat_completions_create(
        call_type="answer_revision",
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", 
             "content": f"Reference Content: {content}\n\nQuestion: {question}\n\nOriginal Answer: {answer}\n\nInstruction: {prompt}"}
        ],
        temperature=LLM_TEMPERATURE
    )
    
    return response.choices[0].message.content

def determine_reference_source(task_type: str, iteration: int) -> str:
    """ì‘ì—… ìœ í˜•ê³¼ ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¼ ì°¸ì¡° ì†ŒìŠ¤ ê²°ì •"""
    if task_type == "Simple Lookup Tasks":
        # Simple Lookupì€ ì£¼ë¡œ ë‚´ë¶€ XML DB ì‚¬ìš©
        return "chromadb"
    else:
        return "chromadb" if iteration % 2 == 0 else "internet"

def run_with_timeout(func, timeout, *args, **kwargs):
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” í•¨ìˆ˜ ì‹¤í–‰"""
    q = Queue()
    
    def wrapper(q, *args, **kwargs):
        try:
            result = func(*args, **kwargs)
            q.put(result)
        except Exception as e:
            print(f"[ERROR] Function execution failed: {e}")
            q.put(None)
    
    p = Process(target=wrapper, args=(q, *args), kwargs=kwargs)
    p.start()
    p.join(timeout)
    
    if p.is_alive():
        print(f"[WARNING] Function timed out after {timeout}s")
        p.terminate()
        p.join()
        return None
    else:
        return q.get() if not q.empty() else None

def get_final_response(question: str, refined_answer: str, task_type: str) -> str:
    """ìµœì¢… ì‘ë‹µ ìƒì„± - CSV í˜•ì‹ì— ë§ì¶˜ ë‹µë³€ ê°•í™”"""
    final_prompt = f"""
    You are a network engineering expert. Analyze the question and provide a precise answer in the specified format.

    CRITICAL GROUND_TRUTH FORMATTING RULES:
    1. **Single Value Questions**: Return ONLY the exact value
       - IP addresses: "192.168.1.1" (no labels)
       - Device names: "CE1" or "sample7" (exact name only)
       - Port numbers: "22" or "443" (number only)
       - Boolean answers: "Yes" or "No"
       
    2. **Multiple Value Questions**: Use comma-separated format
       - Device lists: "CE1, CE2, sample10"
       - IP ranges: "192.168.1.1, 192.168.1.2"
       - Port lists: "22, 80, 443"
       
    3. **Configuration Values**: Return exact configuration strings
       - Interface names: "GigabitEthernet0/0"
       - Protocol names: "OSPF" or "BGP"
       - VLAN IDs: "100" or "200"

    EXAMPLES FROM CSV PATTERN:
    Q: "CE1ì˜ IP ì£¼ì†ŒëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ?"
    â†’ Ground Truth: "192.168.1.100"
    
    Q: "SSHê°€ í™œì„±í™”ëœ ì¥ì¹˜ë“¤ì€?"
    â†’ Ground Truth: "CE1, sample7, sample10"
    
    Q: "sample7ì—ì„œ ì‚¬ìš© ì¤‘ì¸ í¬íŠ¸ëŠ”?"
    â†’ Ground Truth: "22, 80, 443"

    OUTPUT FORMAT:
    [GROUND_TRUTH]
    {{EXACT_VALUE_ONLY}}

    [EXPLANATION]
    {{ìƒì„¸í•œ ê¸°ìˆ ì  ì„¤ëª…ì„ í•œêµ­ì–´ë¡œ ì œê³µ}}

    STRICT RULES:
    - Ground Truth: NO extra words, labels, or formatting
    - Explanation: Comprehensive Korean explanation
    - Be extremely precise with values
    - Match the exact format expected in evaluation
    """
    
    response = tracked_openai_client.chat_completions_create(
        call_type="final_optimization",
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", 
             "content": f"Question: {question}\n\nCurrent Answer: {refined_answer}\n\nInstruction: {final_prompt}"}
        ],
        temperature=LLM_TEMPERATURE
    )
    
    return response.choices[0].message.content.strip()

class NetworkEngineeringPipeline:
    """ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ë§ LLM íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, chromadb_path: str, 
                 collection_name: str,
                 max_iterations: int,
                 xml_directory: str = None):
        """
        ì´ˆê¸°í™”
        Args:
            chromadb_path: ì‚¬ì „ ì„ë² ë”©ëœ XML íŒŒì¼ì´ ìˆëŠ” ChromaDB ê²½ë¡œ
            collection_name: XML ì„¤ì • íŒŒì¼ ì»¬ë ‰ì…˜
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
            xml_directory: XML íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ (ìë™ ì„ë² ë”©ìš©)
        """
        self.db = ChromaDB(chromadb_path, collection_name, 
            embedder=HuggingFaceEmbedder(),  # ê¸°ë³¸ ì„¤ì •ê°’ ì‚¬ìš©
            xml_directory=xml_directory
        )
        self.max_iterations = max_iterations
        print(f"[INFO] Pipeline initialized with {max_iterations} max iterations")
        
    def get_chromadb_content(self, question: str, answer: str, n_results=5, top_n_after_rerank: int = 5) -> Optional[str]:
        """ChromaDBì—ì„œ ê´€ë ¨ XML ì„¤ì • íŒŒì¼ ê²€ìƒ‰, LLMìœ¼ë¡œ Re-ranking, ê·¸ë¦¬ê³  ë¡œê¹…

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            answer: í˜„ì¬ê¹Œì§€ì˜ ì´ˆì•ˆ/ë‹µë³€ (ë¯¸ì‚¬ìš© ê°€ëŠ¥)
            n_results: 1ì°¨ í›„ë³´êµ° ê°œìˆ˜ (ë²¡í„° ê²€ìƒ‰ Top-K)
            top_n_after_rerank: Re-ranking í›„ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ë¬¸ì„œ ìˆ˜
        """
        query = f"ë‹¨ìˆœ ì¡°íšŒ, {question}"
        print(f"[INFO] ChromaDB query: {query}")
        
        results = self.db.query(query, n_results=n_results)
        
        if results and results['documents'] and results['documents'][0]:
            documents = results['documents'][0]
            metadatas = results['metadatas'][0] if results.get('metadatas') else None

            # Re-ranking ì ìš© (í›„ë³´êµ°ì´ ì¶©ë¶„íˆ í´ ë•Œë§Œ)
            if len(documents) > 1:
                print(f"  â”œâ”€ Re-ranking {len(documents)} candidates â†’ top {min(top_n_after_rerank, len(documents))}")
                ranked_indices = get_reranked_indices(question, documents, top_n=top_n_after_rerank)

                # ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ/ë©”íƒ€ë°ì´í„° ì •ë ¬
                documents = [documents[i] for i in ranked_indices]
                if metadatas:
                    metadatas = [metadatas[i] for i in ranked_indices]
                print(f"  â””â”€ âœ“ Re-ranking complete. Selected {len(documents)} docs")
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶œë ¥
            if metadatas:
                for i, meta in enumerate(metadatas):
                    print(f"  - Document {i+1}: {meta}")
            
            # ì œì™¸í•  í‚¤
            exclude_keys = {"file_path", "source", "filename", "source_directory"}
            
            # ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ join
            if metadatas:
                combined_content = "\n\n".join(
                    f"[METADATA]\n" +
                    "\n".join(f"{k}: {v}" for k, v in meta.items() if k not in exclude_keys) +
                    f"\n\n[CONTENT]\n{doc}"
                    for doc, meta in zip(documents, metadatas)
                )
            else:
                combined_content = "\n\n".join(documents)
            
            # ë””ë²„ê¹… ë¡œê·¸ ì €ì¥
            log_path = "input_docs.log"
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(f"\n[QUESTION]\n{question}\n")
                f.write(f"\n[COMBINED_CONTENT]\n{combined_content}\n")
                f.write("=" * 80 + "\n")
            
            return combined_content
        
        return None
    
    def process_query(self, user_question: str, top_k_chroma) -> Dict:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
        start_time = time.time()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ë¡œê¹…ì„ ìœ„í•œ ë°ì´í„° ì €ì¥
        log_data = []
        
        print(f"\n{'='*70}")
        print(f"NETWORK ENGINEERING LLM PIPELINE")
        print(f"{'='*70}")
        print(f"Query: {user_question[:100]}...")
        print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}\n")
        
        # 1. ì‘ì—… ë¶„ë¥˜
        print("[STEP 1/6] Classifying task type...")
        task_type = get_classification_result(user_question)
        print(f"  â””â”€ Classified as: {task_type}")
        
        # JSON ë¡œê¹…: ì‘ì—… ë¶„ë¥˜
        log_data.append({
            "step": "task_classification",
            "content": task_type,
            "timestamp": timestamp
        })
        # 2. ì´ˆì•ˆ ì‘ì„±
        print("[WARNNING] Task Type : ", task_type)
        print("\n[STEP 2/6] Generating initial draft...")
            
        current_answer = get_draft(user_question, task_type)

        # JSON ë¡œê¹…: ì´ˆì•ˆ
        log_data.append({
            "step": "initial_draft",
            "content": current_answer,
            "timestamp": timestamp
        })
        
        # ê²°ê³¼ ì €ì¥
        results = {
            "question": user_question,
            "task_type": task_type,
            "initial_draft": current_answer,
            "iterations": [],
            "total_revisions": 0
        }
        
        # 3-5. ë°˜ë³µì  ê°œì„ 
        print(f"\n[STEP 3-5/6] Iterative refinement ({self.max_iterations} iterations)")
        print("â”€" * 50)
        
        for iteration in range(self.max_iterations):
            print(f"\n[ITERATION {iteration + 1}/{self.max_iterations}]")
            
            # ì°¸ì¡° ì†ŒìŠ¤ ê²°ì •
            ref_source = determine_reference_source(task_type, iteration)
            print(f"  â”œâ”€ Reference source: {ref_source.upper()}")
            
            reference_content = None
            source_details = {}
            
            if ref_source == "chromadb":
                # ChromaDBì—ì„œ XML ì„¤ì • íŒŒì¼ ê²€ìƒ‰
                print("  â”œâ”€ Searching ChromaDB for XML configurations...")
                reference_content = self.get_chromadb_content(user_question, current_answer, n_results=top_k_chroma)
                
                
                if reference_content:
                    print(f"  â”œâ”€ Found relevant XML configurations")
                    source_details = {"type": "xml_config", "source": "chromadb"}
                else:
                    print("  â”œâ”€ No relevant XML found in ChromaDB")
                    
            else:  # internet
                # ì¸í„°ë„·ì—ì„œ ìµœì‹  ì •ë³´ ê²€ìƒ‰
                print("  â”œâ”€ Searching internet for latest information...")
                query = get_internet_query(user_question, current_answer)
                print(f"  â”œâ”€ Search query: {query}")
                
                content_list = get_internet_content(query)
                
                if content_list:
                    reference_content = "\n\n".join(content_list)
                    print(f"  â”œâ”€ Retrieved {len(content_list)} content chunks")
                    source_details = {"type": "web_content", "source": "google"}
                else:
                    print("  â”œâ”€ No relevant content found online")
            
            # ì°¸ì¡° ìë£Œê°€ ìˆìœ¼ë©´ ë‹µë³€ ìˆ˜ì •
            if reference_content:
                print("  â”œâ”€ Revising answer with references...")
                revised_answer = run_with_timeout(
                    get_revise_answer, 10, user_question, current_answer, 
                    reference_content, task_type  # í† í° ì œí•œ
                )
                
                if revised_answer and revised_answer != current_answer:
                    print("  â””â”€ âœ“ Answer improved")
                    current_answer = revised_answer
                    results["total_revisions"] += 1
                    
                    # JSON ë¡œê¹…: ìˆ˜ì •ëœ ë‹µë³€
                    log_data.append({
                        "step": f"iteration_{iteration + 1}_revised",
                        "content": current_answer,
                        "timestamp": timestamp
                    })
                    
                    iteration_result = {
                        "iteration": iteration + 1,
                        "source": ref_source,
                        "reference_found": True,
                        "answer_revised": True,
                        "source_details": source_details
                    }
                else:
                    print("  â””â”€ â—‹ No changes needed")
                    iteration_result = {
                        "iteration": iteration + 1,
                        "source": ref_source,
                        "reference_found": True,
                        "answer_revised": False,
                        "source_details": source_details
                    }
            else:
                print("  â””â”€ â—‹ No references found")
                iteration_result = {
                    "iteration": iteration + 1,
                    "source": ref_source,
                    "reference_found": False,
                    "answer_revised": False,
                    "source_details": {}
                }
            
            results["iterations"].append(iteration_result)
        
        # 6. ìµœì¢… ì‘ë‹µ ìƒì„± (NEW STEP)
        print(f"\n[STEP 6/6] Generating final optimized response...")
        print("  â”œâ”€ Optimizing for Exact Match and BERT-F1 Score...")
        
        final_response = run_with_timeout(
            get_final_response, 10, user_question, current_answer, task_type
        )
        
        if final_response and final_response != current_answer:
            print("  â””â”€ âœ“ Final response optimized for evaluation metrics")
            results["final_optimization"] = True
        else:
            print("  â””â”€ â—‹ Current answer already optimal")
            final_response = current_answer
            results["final_optimization"] = False
        
        # JSON ë¡œê¹…: ìµœì¢… ì‘ë‹µ
        log_data.append({
            "step": "final_response",
            "content": final_response,
            "timestamp": timestamp
        })
        
        # experiment_loggerë¥¼ í†µí•´ ìƒì„¸ ë¡œê·¸ ì €ì¥
        if experiment_logger:
            experiment_logger.save_detailed_log(log_data, f"pipeline_detailed_{timestamp}.json")
        
        # ìµœì¢… ê²°ê³¼
        results["final_answer"] = final_response
        results["processing_time"] = round(time.time() - start_time, 2)
        
        print(f"\n{'='*70}")
        print(f"PIPELINE COMPLETE")
        print(f"  - Total time: {results['processing_time']}s")
        print(f"  - Total revisions: {results['total_revisions']}")
        print(f"  - Final optimization: {'YES' if results['final_optimization'] else 'NO'}")
        print(f"  - Task type: {task_type}")
        print(f"{'='*70}\n")
        
        return results
            ref_source = determine_reference_source(task_type, iteration)
            print(f"  â”œâ”€ Reference source: {ref_source.upper()}")
            
            reference_content = None
            source_details = {}
            
            if ref_source == "chromadb":
                # ChromaDBì—ì„œ XML ì„¤ì • íŒŒì¼ ê²€ìƒ‰
                print("  â”œâ”€ Searching ChromaDB for XML configurations...")
                reference_content = self.get_chromadb_content(user_question, current_answer, n_results=top_k_chroma)
                
                
                if reference_content:
                    print(f"  â”œâ”€ Found relevant XML configurations")
                    source_details = {"type": "xml_config", "source": "chromadb"}
                else:
                    print("  â”œâ”€ No relevant XML found in ChromaDB")
                    
            else:  # internet
                # ì¸í„°ë„·ì—ì„œ ìµœì‹  ì •ë³´ ê²€ìƒ‰
                print("  â”œâ”€ Searching internet for latest information...")
                query = get_internet_query(user_question, current_answer)
                print(f"  â”œâ”€ Search query: {query}")
                
                content_list = get_internet_content(query)
                
                if content_list:
                    reference_content = "\n\n".join(content_list)
                    print(f"  â”œâ”€ Retrieved {len(content_list)} content chunks")
                    source_details = {"type": "web_content", "source": "google"}
                else:
                    print("  â”œâ”€ No relevant content found online")
            
            # ì°¸ì¡° ìë£Œê°€ ìˆìœ¼ë©´ ë‹µë³€ ìˆ˜ì •
            if reference_content:
                print("  â”œâ”€ Revising answer with references...")
                revised_answer = run_with_timeout(
                    get_revise_answer, 10, user_question, current_answer, 
                    reference_content, task_type  # í† í° ì œí•œ
                )
                
                if revised_answer and revised_answer != current_answer:
                    print("  â””â”€ âœ“ Answer improved")
                    current_answer = revised_answer
                    results["total_revisions"] += 1
                    
                    # JSON ë¡œê¹…: ìˆ˜ì •ëœ ë‹µë³€
                    log_data.append({
                        "step": f"iteration_{iteration + 1}_revised",
                        "content": current_answer,
                        "timestamp": timestamp
                    })
                    
                    iteration_result = {
                        "iteration": iteration + 1,
                        "source": ref_source,
                        "reference_found": True,
                        "answer_revised": True,
                        "source_details": source_details
                    }
                else:
                    print("  â””â”€ â—‹ No changes needed")
                    iteration_result = {
                        "iteration": iteration + 1,
                        "source": ref_source,
                        "reference_found": True,
                        "answer_revised": False,
                        "source_details": source_details
                    }
            else:
                print("  â””â”€ â—‹ No references found")
                iteration_result = {
                    "iteration": iteration + 1,
                    "source": ref_source,
                    "reference_found": False,
                    "answer_revised": False,
                    "source_details": {}
                }
            
            results["iterations"].append(iteration_result)
        
        # 6. ìµœì¢… ì‘ë‹µ ìƒì„± (NEW STEP)
        print(f"\n[STEP 6/6] Generating final optimized response...")
        print("  â”œâ”€ Optimizing for Exact Match and BERT-F1 Score...")
        
        final_response = run_with_timeout(
            get_final_response, 10, user_question, current_answer, task_type
        )
        
        if final_response and final_response != current_answer:
            print("  â””â”€ âœ“ Final response optimized for evaluation metrics")
            results["final_optimization"] = True
        else:
            print("  â””â”€ â—‹ Current answer already optimal")
            final_response = current_answer
            results["final_optimization"] = False
        
        # JSON ë¡œê¹…: ìµœì¢… ì‘ë‹µ
        log_data.append({
            "step": "final_response",
            "content": final_response,
            "timestamp": timestamp
        })
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        import json
        import os
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = "logs2"
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
        
        # JSON íŒŒì¼ ê²½ë¡œ
        json_filename = f"{log_dir}/pipeline_log_{timestamp}.json"
        
        # JSON í˜•íƒœë¡œ ì „ì²´ ë¡œê·¸ êµ¬ì„±
        complete_log = {
            "question": user_question,
            "task_type": task_type,
            "timestamp": timestamp,
            "pipeline_steps": log_data
        }
        
        # JSON íŒŒì¼ ì €ì¥
        with open(json_filename, 'w', encoding='utf-8') as jsonfile:
            json.dump(complete_log, jsonfile, ensure_ascii=False, indent=2)
        
        print(f"  â”œâ”€ Logged to: {json_filename}")
        results["log_file"] = json_filename
        
        # ìµœì¢… ê²°ê³¼
        results["final_answer"] = final_response
        results["processing_time"] = round(time.time() - start_time, 2)
        
        print(f"\n{'='*70}")
        print(f"PIPELINE COMPLETE")
        print(f"  - Total time: {results['processing_time']}s")
        print(f"  - Total revisions: {results['total_revisions']}")
        print(f"  - Final optimization: {'YES' if results['final_optimization'] else 'NO'}")
        print(f"  - Task type: {task_type}")
        print(f"{'='*70}\n")
        
        return results

def main():
    """ë‘ ê°€ì§€ ì‹¤í—˜ì„ ìœ„í•œ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
    global experiment_logger, tracked_openai_client
    
    # ì‹¤í—˜ ë¡œê±° ì´ˆê¸°í™”
    experiment_logger = ExperimentLogger("network_pipeline_comparison", EXPERIMENT_BASE_DIR)
    tracked_openai_client = TrackedOpenAIClient(experiment_logger)
    
    # ì½˜ì†” ì¶œë ¥ ìº¡ì²˜ ì‹œì‘
    experiment_logger.start_console_capture()
    
    try:
        # ì„¤ì •ê°’ ì¶œë ¥ (í™•ì¸ìš©)
        print("="*80)
        print("ì‹¤í—˜ ì„¤ì • í™•ì¸")
        print("="*80)
        print(f"ChromaDB ê²½ë¡œ: {CHROMADB_PATH}")
        print(f"XML ë””ë ‰í† ë¦¬: {XML_DIRECTORY}")
        print(f"CSV ë°ì´í„°ì…‹: {CSV_PATH}")
        print(f"ì„ë² ë”© ëª¨ë¸: {EMBEDDING_MODEL}")
        print(f"GPU ë””ë°”ì´ìŠ¤: {EMBEDDING_DEVICE}")
        print(f"ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜: {MAX_ITERATIONS}")
        print(f"Top-K ê°’ë“¤: {TOP_K_VALUES}")
        print("="*80)
        
        # ì‹¤í—˜ ê°œìš” ì €ì¥
        experiment_overview = {
            "total_questions": 0,
            "methods": ["Improved Non-RAG (Intelligent Chunking)", "RAG (ChromaDB with top_k variations)"],
            "metrics": ["Exact Match", "F1 Score", "BERT F1", "ROUGE-1", "Processing Time"],
            "chromadb_path": CHROMADB_PATH,
            "xml_directory": XML_DIRECTORY,
            "max_iterations": MAX_ITERATIONS,
            "top_k_values": TOP_K_VALUES,
            "embedding_model": EMBEDDING_MODEL
        }
        }
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë“œ
        print("Loading test data...")
        df = pd.read_csv(CSV_PATH)
        test_queries = df["question"].dropna().tolist()
        experiment_overview["total_questions"] = len(test_queries)
        print(f"Loaded {len(test_queries)} test queries")
        
        # ===============================
        # ì‹¤í—˜ 1: ê°œì„ ëœ Non-RAG (ì§€ëŠ¥í˜• ì²­í‚¹)
        # ===============================
        print("\n" + "="*80)
        print("EXPERIMENT 1: IMPROVED NON-RAG PIPELINE (Intelligent XML Chunking)")
        print("="*80)
        
        # ê¸°ì¡´ NonRAGPipeline ëŒ€ì‹  ImprovedNonRAGPipeline ì‚¬ìš©
        non_rag_pipeline = ImprovedNonRAGPipeline(XML_DIRECTORY)
        non_rag_results = []
        
        for i, query in enumerate(test_queries):
            print(f"\nProcessing query {i+1}/{len(test_queries)}: {query[:50]}...")
            result = non_rag_pipeline.process_query(query)
            non_rag_results.append(result)
        
        # ì‹¤í—˜ 1 ê²°ê³¼ ì €ì¥
        experiment_logger.save_results(non_rag_results, "improved_non_rag_results.json")
        
        # ì‹¤í—˜ 1 í‰ê°€
        print("\n" + "="*80)
        print("EXPERIMENT 1 EVALUATION")
        print("="*80)
        non_rag_eval = evaluate_predictions(non_rag_results, df)
        
        # ===============================
        # ì‹¤í—˜ 2: RAG ì ìš©
        # ===============================
        print("\n" + "="*80)
        print("EXPERIMENT 2: RAG PIPELINE (ChromaDB)")
        print("="*80)
        
        rag_pipeline = NetworkEngineeringPipeline(
            chromadb_path=CHROMADB_PATH,
            collection_name=COLLECTION_NAME,
            max_iterations=MAX_ITERATIONS,
            xml_directory=XML_DIRECTORY
        )
        
        # ë‹¤ì–‘í•œ top_k ê°’ìœ¼ë¡œ ì‹¤í—˜
        top_k_values = TOP_K_VALUES
        rag_results_by_k = {}
        
        for top_k in top_k_values:
            print(f"\n--- Testing with top_k={top_k} ---")
            rag_results = []
            
            for i, query in enumerate(test_queries):
                print(f"\nProcessing query {i+1}/{len(test_queries)} (top_k={top_k}): {query[:50]}...")
                result = rag_pipeline.process_query(query, top_k_chroma=top_k)
                rag_results.append(result)
            
            rag_results_by_k[top_k] = rag_results
            
            # ê° top_kë³„ ê²°ê³¼ ì €ì¥
            experiment_logger.save_results(rag_results, f"rag_results_k{top_k}.json")
            
            # ì‹¤í—˜ 2 í‰ê°€ (ê° top_kë³„)
            print(f"\n--- EXPERIMENT 2 EVALUATION (top_k={top_k}) ---")
            rag_eval = evaluate_predictions(rag_results, df)
            rag_results_by_k[f"{top_k}_eval"] = rag_eval
        
        # ===============================
        # ì¢…í•© ê²°ê³¼ ë¹„êµ
        # ===============================
        print("\n" + "="*80)
        print("COMPREHENSIVE COMPARISON")
        print("="*80)
        
        comparison_results = {
            "experiment_date": datetime.now().strftime('%Y-%m-%d %H:%M:%S'),
            "experiment_overview": experiment_overview,
            "test_queries_count": len(test_queries),
            "improved_non_rag": {
                "evaluation": non_rag_eval,
                "avg_processing_time": sum(r['processing_time'] for r in non_rag_results) / len(non_rag_results)
            },
            "rag": {}
        }
        
        print("\n[IMPROVED NON-RAG RESULTS (Intelligent Chunking)]")
        print(f"Overall - EM: {non_rag_eval['overall']['exact_match']:.4f}, F1: {non_rag_eval['overall']['f1_score']:.4f}")
        if 'origin' in df.columns:
            print(f"Rule-based - EM: {non_rag_eval['rule_based']['exact_match']:.4f}, F1: {non_rag_eval['rule_based']['f1_score']:.4f}")
            print(f"Enhanced LLM GT - EM: {non_rag_eval['enhanced_llm']['ground_truth']['exact_match']:.4f}, F1: {non_rag_eval['enhanced_llm']['ground_truth']['f1_score']:.4f}")
            if non_rag_eval['enhanced_llm']['explanation']['valid_count'] > 0:
                print(f"Enhanced LLM Exp - BERT F1: {non_rag_eval['enhanced_llm']['explanation']['bert_f1']:.4f}, ROUGE-1: {non_rag_eval['enhanced_llm']['explanation']['rouge_1_f1']:.4f}")
        
        for top_k in top_k_values:
            rag_eval = rag_results_by_k[f"{top_k}_eval"]
            rag_results = rag_results_by_k[top_k]
            
            comparison_results["rag"][f"top_k_{top_k}"] = {
                "evaluation": rag_eval,
                "avg_processing_time": sum(r['processing_time'] for r in rag_results) / len(rag_results)
            }
            
            print(f"\n[RAG RESULTS - top_k={top_k}]")
            print(f"Overall - EM: {rag_eval['overall']['exact_match']:.4f}, F1: {rag_eval['overall']['f1_score']:.4f}")
            if 'origin' in df.columns:
                print(f"Rule-based - EM: {rag_eval['rule_based']['exact_match']:.4f}, F1: {rag_eval['rule_based']['f1_score']:.4f}")
                print(f"Enhanced LLM GT - EM: {rag_eval['enhanced_llm']['ground_truth']['exact_match']:.4f}, F1: {rag_eval['enhanced_llm']['ground_truth']['f1_score']:.4f}")
                if rag_eval['enhanced_llm']['explanation']['valid_count'] > 0:
                    print(f"Enhanced LLM Exp - BERT F1: {rag_eval['enhanced_llm']['explanation']['bert_f1']:.4f}, ROUGE-1: {rag_eval['enhanced_llm']['explanation']['rouge_1_f1']:.4f}")
        
        # ì¢…í•© ê²°ê³¼ë¥¼ experiment_loggerë¡œ ì €ì¥
        experiment_logger.save_results(comparison_results, "comprehensive_comparison_results.json")
        
        # í‰ê°€ ë¦¬í¬íŠ¸ ìƒì„±
        experiment_logger.save_evaluation_report(comparison_results)
        
        print(f"\nì‹¤í—˜ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
        print(f"ê²°ê³¼ëŠ” {experiment_logger.exp_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print("="*80)
        
    except Exception as e:
        print(f"[ERROR] ì‹¤í—˜ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        traceback.print_exc()
    finally:
        # ì‹¤í—˜ ì¢…ë£Œ ì²˜ë¦¬
        experiment_logger.finalize_experiment()

    # for query in test_queries:
    #     results = pipeline.process_query(query, top_k_chroma=20)
        
    #     # ê²°ê³¼ ì¶œë ¥
    #     print("\n" + "="*70)
    #     print("FINAL RESULTS")
    #     print("="*70)
    #     print(f"\nQuestion: {results['question']}")
    #     print(f"Task Type: {results['task_type']}")
    #     print(f"Processing Time: {results['processing_time']}s")
    #     print(f"Total Revisions: {results['total_revisions']}")
        
    #     print("\nIteration Summary:")
    #     for iter_info in results['iterations']:
    #         status = "âœ“" if iter_info['answer_revised'] else "â—‹"
    #         print(f"  {status} Iteration {iter_info['iteration']}: "
    #             f"{iter_info['source']} - "
    #             f"{'Found' if iter_info['reference_found'] else 'Not found'}")
        
    #     print(f"\n{'â”€'*70}")
    #     print("FINAL ANSWER:")
    #     print("â”€"*70)
    #     print(results['final_answer'])
    #     print("="*70)

    #     # ğŸ”¹ ë¡œê·¸ íŒŒì¼ ì €ì¥ ì¶”ê°€
    #     with open(f"pipeline_results_qwen_improved_prompt{20}.log", "a", encoding="utf-8") as f:
    #         f.write("="*70 + "\n")
    #         f.write(f"Question: {results['question']}\n")
    #         f.write(f"Final Answer: {results['final_answer']}\n")
    #         f.write("="*70 + "\n\n")

def calculate_exact_match(predictions: List[str], ground_truths: List[str]) -> float:
    """Exact Match ê³„ì‚°"""
    if len(predictions) != len(ground_truths):
        raise ValueError("Predictions and ground truths must have the same length")
    
    correct = 0
    for pred, gt in zip(predictions, ground_truths):
        if pred.strip().lower() == gt.strip().lower():
            correct += 1
    
    return correct / len(predictions)

def calculate_f1_score(predictions: List[str], ground_truths: List[str]) -> float:
    """F1 Score ê³„ì‚° (í† í° ê¸°ë°˜)"""
    if len(predictions) != len(ground_truths):
        raise ValueError("Predictions and ground truths must have the same length")
    
    f1_scores = []
    for pred, gt in zip(predictions, ground_truths):
        pred_tokens = set(pred.strip().lower().split())
        gt_tokens = set(gt.strip().lower().split())
        
        if len(gt_tokens) == 0:
            f1_scores.append(1.0 if len(pred_tokens) == 0 else 0.0)
            continue
            
        intersection = pred_tokens.intersection(gt_tokens)
        precision = len(intersection) / len(pred_tokens) if len(pred_tokens) > 0 else 0
        recall = len(intersection) / len(gt_tokens)
        
        if precision + recall == 0:
            f1_scores.append(0.0)
        else:
            f1_scores.append(2 * precision * recall / (precision + recall))
    
    return sum(f1_scores) / len(f1_scores)

def calculate_bert_score(predictions: List[str], ground_truths: List[str]) -> Dict[str, float]:
    """BERT Score ê³„ì‚°"""
    if len(predictions) != len(ground_truths):
        raise ValueError("Predictions and ground truths must have the same length")
    
    # BERT Score ê³„ì‚°
    P, R, F1 = bert_score(predictions, ground_truths, lang="en", verbose=False)
    
    return {
        "precision": P.mean().item(),
        "recall": R.mean().item(), 
        "f1": F1.mean().item()
    }

def calculate_rouge_scores(predictions: List[str], ground_truths: List[str]) -> Dict[str, float]:
    """ROUGE Score ê³„ì‚°"""
    if len(predictions) != len(ground_truths):
        raise ValueError("Predictions and ground truths must have the same length")
    
    rouge = Rouge()
    scores = rouge.get_scores(predictions, ground_truths, avg=True)
    
    return {
        "rouge-1": scores['rouge-1']['f'],
        "rouge-2": scores['rouge-2']['f'],
        "rouge-l": scores['rouge-l']['f']
    }

def load_xml_files(xml_directory: str) -> str:
    """XML íŒŒì¼ë“¤ì„ ë¡œë“œí•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ ê²°í•©"""
    xml_files = glob.glob(os.path.join(xml_directory, "*.xml"))
    combined_xml = ""
    
    for xml_file in xml_files:
        try:
            with open(xml_file, 'r', encoding='utf-8') as f:
                content = f.read()
                combined_xml += f"\n\n=== {os.path.basename(xml_file)} ===\n{content}\n"
        except Exception as e:
            print(f"[WARNING] Failed to load {xml_file}: {e}")
    
    return combined_xml


class ImprovedNonRAGPipeline:
    """ê°œì„ ëœ Non-RAG íŒŒì´í”„ë¼ì¸ - ì§€ëŠ¥í˜• XML ì²­í‚¹ ë° ì„ íƒ"""
    
    def __init__(self, xml_directory: str):
        """
        ì´ˆê¸°í™”
        Args:
            xml_directory: ì›ë³¸ XML íŒŒì¼ë“¤ì´ ìˆëŠ” ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        self.xml_files = self._load_xml_files_with_chunking(xml_directory)
        self.embedder = HuggingFaceEmbedder()  # ê¸°ë³¸ ì„¤ì •ê°’ ì‚¬ìš©
        print(f"[INFO] Loaded {len(self.xml_files)} XML files with intelligent chunking")
        total_chunks = sum(len(xml_file['chunks']) for xml_file in self.xml_files)
        print(f"[INFO] Total chunks created: {total_chunks}")
    
    def _load_xml_files_with_chunking(self, xml_directory: str) -> List[Dict]:
        """XML íŒŒì¼ë“¤ì„ ê°œë³„ì ìœ¼ë¡œ ë¡œë“œí•˜ì—¬ ì§€ëŠ¥í˜• ì²­í‚¹ ì ìš©"""
        xml_files = []
        
        for xml_file in glob.glob(os.path.join(xml_directory, "*.xml")):
            try:
                with open(xml_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                    # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì²­í‚¹ (XML íƒœê·¸ ê¸°ë°˜)
                    chunks = self._intelligent_xml_chunking(content)
                    
                    xml_files.append({
                        "filename": os.path.basename(xml_file),
                        "original_content": content,
                        "chunks": chunks,
                        "chunk_count": len(chunks)
                    })
                    
                print(f"[INFO] Processed {os.path.basename(xml_file)}: {len(chunks)} chunks")
                    
            except Exception as e:
                print(f"[WARNING] Failed to load {xml_file}: {e}")
        
        return xml_files
    
    def _intelligent_xml_chunking(self, xml_content: str, max_chunk_size: int = 1500) -> List[Dict]:
        """XML ë‚´ìš©ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ì§€ëŠ¥í˜• ì²­í‚¹"""
        chunks = []
        
        # XML êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë¶„í•  íŒ¨í„´ë“¤
        xml_patterns = [
            r'<device[^>]*>.*?</device>',  # ë””ë°”ì´ìŠ¤ ë‹¨ìœ„
            r'<interface[^>]*>.*?</interface>',  # ì¸í„°í˜ì´ìŠ¤ ë‹¨ìœ„
            r'<vlan[^>]*>.*?</vlan>',  # VLAN ë‹¨ìœ„
            r'<routing[^>]*>.*?</routing>',  # ë¼ìš°íŒ… ë‹¨ìœ„
            r'<security[^>]*>.*?</security>',  # ë³´ì•ˆ ë‹¨ìœ„
            r'<configuration[^>]*>.*?</configuration>',  # ì„¤ì • ë‹¨ìœ„
        ]
        
        # íŒ¨í„´ë³„ë¡œ ë§¤ì¹­ëœ ì²­í¬ë“¤ ìˆ˜ì§‘
        matched_chunks = []
        remaining_content = xml_content
        
        for pattern in xml_patterns:
            matches = re.findall(pattern, xml_content, re.DOTALL | re.IGNORECASE)
            for match in matches:
                chunk_info = self._create_chunk_info(match, pattern)
                if chunk_info and chunk_info not in matched_chunks:
                    matched_chunks.append(chunk_info)
                    # ë§¤ì¹­ëœ ë¶€ë¶„ì„ ì œê±°
                    remaining_content = remaining_content.replace(match, '', 1)
        
        # ë§¤ì¹­ë˜ì§€ ì•Šì€ ë‚˜ë¨¸ì§€ ë‚´ìš©ë„ ì²­í‚¹
        if remaining_content.strip():
            remaining_chunks = chunk_texts(remaining_content, max_chunk_size)
            for i, chunk_text in enumerate(remaining_chunks):
                if chunk_text.strip():
                    matched_chunks.append({
                        "content": chunk_text,
                        "type": "miscellaneous",
                        "chunk_id": f"misc_{i}",
                        "tokens": num_tokens_from_string(chunk_text)
                    })
        
        # í† í° í¬ê¸°ê°€ í° ì²­í¬ë“¤ì„ ë‹¤ì‹œ ë¶„í• 
        final_chunks = []
        for chunk in matched_chunks:
            if chunk["tokens"] > max_chunk_size:
                sub_chunks = chunk_texts(chunk["content"], max_chunk_size)
                for j, sub_chunk in enumerate(sub_chunks):
                    final_chunks.append({
                        "content": sub_chunk,
                        "type": f"{chunk['type']}_sub",
                        "chunk_id": f"{chunk['chunk_id']}_{j}",
                        "tokens": num_tokens_from_string(sub_chunk)
                    })
            else:
                final_chunks.append(chunk)
        
        return final_chunks
    
    def _create_chunk_info(self, content: str, pattern: str) -> Dict:
        """ì²­í¬ ì •ë³´ ìƒì„±"""
        # íŒ¨í„´ì—ì„œ íƒ€ì… ì¶”ì¶œ
        type_mapping = {
            'device': 'device_config',
            'interface': 'interface_config', 
            'vlan': 'vlan_config',
            'routing': 'routing_config',
            'security': 'security_config',
            'configuration': 'general_config'
        }
        
        chunk_type = 'unknown'
        for key, value in type_mapping.items():
            if key in pattern:
                chunk_type = value
                break
        
        return {
            "content": content,
            "type": chunk_type,
            "chunk_id": f"{chunk_type}_{hash(content) % 10000}",
            "tokens": num_tokens_from_string(content)
        }
    
    def _select_relevant_chunks_with_embedding(self, question: str, max_tokens: int = 15000) -> str:
        """ì„ë² ë”© ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ XML ì²­í¬ë“¤ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ì„ íƒ"""
        
        print(f"[INFO] Selecting relevant chunks for question: {question[:50]}...")
        
        # ëª¨ë“  ì²­í¬ë“¤ ìˆ˜ì§‘
        all_chunks = []
        for xml_file in self.xml_files:
            for chunk in xml_file["chunks"]:
                chunk_with_metadata = chunk.copy()
                chunk_with_metadata["filename"] = xml_file["filename"]
                all_chunks.append(chunk_with_metadata)
        
        print(f"[INFO] Total chunks to analyze: {len(all_chunks)}")
        
        # ì§ˆë¬¸ ì„ë² ë”©
        question_embedding = self.embedder.embed([question])[0]
        
        # ì²­í¬ë“¤ ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°
        chunk_texts = [chunk["content"] for chunk in all_chunks]
        chunk_embeddings = self.embedder.embed(chunk_texts)
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i, chunk_emb in enumerate(chunk_embeddings):
            similarity = self._cosine_similarity(question_embedding, chunk_emb)
            similarities.append((similarity, i))
        
        # ìœ ì‚¬ë„ìˆœìœ¼ë¡œ ì •ë ¬
        similarities.sort(reverse=True)
        
        # ìœ ì‚¬ë„ ê¸°ë°˜ + ë‹¤ì–‘ì„±ì„ ê³ ë ¤í•œ ì„ íƒ
        selected_chunks = []
        total_tokens = 0
        type_counts = {}
        
        for similarity, idx in similarities:
            chunk = all_chunks[idx]
            chunk_type = chunk["type"]
            
            # í† í° ì œí•œ í™•ì¸
            if total_tokens + chunk["tokens"] > max_tokens:
                continue
            
            # íƒ€ì… ë‹¤ì–‘ì„± ê³ ë ¤ (ê° íƒ€ì…ë³„ ìµœëŒ€ ì œí•œ)
            type_limit = max_tokens // 8  # ê° íƒ€ì…ë³„ ëŒ€ëµì  ì œí•œ
            if type_counts.get(chunk_type, 0) * 500 > type_limit:  # ëŒ€ëµì  ê³„ì‚°
                continue
            
            # ìµœì†Œ ìœ ì‚¬ë„ ì„ê³„ê°’
            if similarity < 0.1:  # ë„ˆë¬´ ë‚®ì€ ìœ ì‚¬ë„ëŠ” ì œì™¸
                break
            
            selected_chunks.append({
                "content": chunk["content"],
                "filename": chunk["filename"],
                "type": chunk["type"],
                "similarity": similarity,
                "tokens": chunk["tokens"]
            })
            
            total_tokens += chunk["tokens"]
            type_counts[chunk_type] = type_counts.get(chunk_type, 0) + 1
            
            # ì¶©ë¶„í•œ ì •ë³´ê°€ ëª¨ì´ë©´ ì¢…ë£Œ
            if len(selected_chunks) >= 20 or total_tokens >= max_tokens * 0.9:
                break
        
        print(f"[INFO] Selected {len(selected_chunks)} relevant chunks ({total_tokens} tokens)")
        print(f"[INFO] Type distribution: {type_counts}")
        
        # ì„ íƒëœ ì²­í¬ë“¤ì„ í¬ë§·íŒ…
        formatted_content = []
        for i, chunk in enumerate(selected_chunks):
            header = f"=== {chunk['filename']} | {chunk['type']} | Similarity: {chunk['similarity']:.3f} ==="
            formatted_content.append(f"{header}\n{chunk['content']}")
        
        return "\n\n".join(formatted_content)
    
    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°"""
        import numpy as np
        
        vec1 = np.array(vec1)
        vec2 = np.array(vec2)
        
        dot_product = np.dot(vec1, vec2)
        norm1 = np.linalg.norm(vec1)
        norm2 = np.linalg.norm(vec2)
        
        if norm1 == 0 or norm2 == 0:
            return 0.0
        
        return dot_product / (norm1 * norm2)
    
    def _fallback_keyword_selection(self, question: str, max_tokens: int = 15000) -> str:
        """ì„ë² ë”© ì‹¤íŒ¨ì‹œ í‚¤ì›Œë“œ ê¸°ë°˜ í´ë°± ì„ íƒ"""
        print("[WARNING] Using fallback keyword-based selection")
        
        # í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
        question_keywords = set(question.lower().split())
        scored_chunks = []
        
        for xml_file in self.xml_files:
            for chunk in xml_file["chunks"]:
                # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜
                chunk_words = set(chunk["content"].lower().split())
                overlap = len(question_keywords.intersection(chunk_words))
                
                scored_chunks.append({
                    "content": chunk["content"],
                    "filename": xml_file["filename"],
                    "type": chunk["type"],
                    "score": overlap,
                    "tokens": chunk["tokens"]
                })
        
        # ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬
        scored_chunks.sort(key=lambda x: x["score"], reverse=True)
        
        # í† í° ì œí•œ ë‚´ì—ì„œ ìƒìœ„ ì²­í¬ë“¤ ì„ íƒ
        selected_content = []
        total_tokens = 0
        
        for chunk in scored_chunks:
            if total_tokens + chunk["tokens"] <= max_tokens and chunk["score"] > 0:
                header = f"=== {chunk['filename']} | {chunk['type']} | Score: {chunk['score']} ==="
                selected_content.append(f"{header}\n{chunk['content']}")
                total_tokens += chunk["tokens"]
            else:
                if total_tokens >= max_tokens:
                    break
        
        print(f"[INFO] Fallback selected {len(selected_content)} chunks ({total_tokens} tokens)")
        return "\n\n".join(selected_content)
    
    def process_query(self, user_question: str) -> Dict:
        """ê°œì„ ëœ Non-RAG ì¿¼ë¦¬ ì²˜ë¦¬"""
        start_time = time.time()
        
        print(f"\n{'='*70}")
        print(f"IMPROVED NON-RAG PIPELINE (Intelligent Chunking)")
        print(f"{'='*70}")
        print(f"Query: {user_question[:100]}...")
        print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}\n")
        
        # ì‘ì—… ë¶„ë¥˜
        print("[STEP 1/3] Classifying task type...")
        task_type = get_classification_result(user_question)
        print(f"  â””â”€ Classified as: {task_type}")
        
        # ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ XML ì²­í¬ë“¤ì„ ì§€ëŠ¥ì ìœ¼ë¡œ ì„ íƒ
        print("\n[STEP 2/3] Selecting relevant XML chunks...")
        
        if NON_RAG_USE_EMBEDDING:
            try:
                relevant_xml = self._select_relevant_chunks_with_embedding(user_question, max_tokens=15000)
                print(f"  â””â”€ Using embedding-based selection")
            except Exception as e:
                print(f"[WARNING] Embedding-based selection failed: {e}")
                print(f"  â””â”€ Falling back to keyword-based selection")
                relevant_xml = self._fallback_keyword_selection(user_question, max_tokens=15000)
        else:
            print(f"  â””â”€ Using keyword-based selection (embedding disabled)")
            relevant_xml = self._fallback_keyword_selection(user_question, max_tokens=15000)
        
        # LLMìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±
        print("\n[STEP 3/3] Generating answer with selected XML chunks...")
        
        if task_type == "Simple Lookup Tasks":
            prompt_template = f"""
            You are a network engineering expert analyzing pre-selected relevant XML configuration chunks.

            RELEVANT XML Configuration Data (intelligently selected for this question):
            {relevant_xml}

            User Question: {user_question}

            CRITICAL GROUND_TRUTH FORMATTING RULES:
            1. **Single Value Questions**: Return ONLY the exact value
               - IP addresses: "192.168.1.1" (no labels)
               - Device names: "CE1" or "sample7" (exact name only)
               - Port numbers: "22" or "443" (number only)
               - Boolean answers: "Yes" or "No"
               
            2. **Multiple Value Questions**: Use comma-separated format
               - Device lists: "CE1, CE2, sample10"
               - IP ranges: "192.168.1.1, 192.168.1.2"
               - Port lists: "22, 80, 443"
               
            3. **Configuration Values**: Return exact configuration strings
               - Interface names: "GigabitEthernet0/0"
               - Protocol names: "OSPF" or "BGP"
               - VLAN IDs: "100" or "200"

            OUTPUT FORMAT:
            [GROUND_TRUTH]
            {{EXACT_VALUE_ONLY}}

            [EXPLANATION]
            {{ìƒì„¸í•œ ê¸°ìˆ ì  ì„¤ëª…ì„ í•œêµ­ì–´ë¡œ ì œê³µ}}

            STRICT RULES:
            - Ground Truth: NO extra words, labels, or formatting
            - Explanation: Comprehensive Korean explanation
            - Be extremely precise with values
            - Match the exact format expected in evaluation
            """
        else:
            prompt_template = f"""
            You are a network engineering expert analyzing pre-selected relevant XML configuration chunks.

            RELEVANT XML Configuration Data (intelligently selected for this question):
            {relevant_xml}

            User Question: {user_question}

            INSTRUCTIONS:
            - Analyze the provided XML chunks thoroughly
            - Extract exact technical values for ground truth
            - Provide comprehensive technical explanation in Korean
            - Use specific device names, IPs, configurations from the XML data

            OUTPUT FORMAT:
            [GROUND_TRUTH]
            {{ì •í™•í•œ ê¸°ìˆ ì  ê°’ë§Œ}}

            [EXPLANATION]
            {{ìƒì„¸í•œ í•œêµ­ì–´ ê¸°ìˆ  ì„¤ëª…}}
            1. ì§ì ‘ì  ë‹µë³€: ì§ˆë¬¸ì— ëŒ€í•œ ëª…í™•í•œ í•´ë‹µ ì œì‹œ
            2. ê¸°ìˆ ì  êµ¬í˜„: êµ¬ì²´ì ì¸ ì„¤ì • ë‹¨ê³„ë‚˜ ëª…ë ¹ì–´ ì œê³µ
            3. ê³ ë ¤ì‚¬í•­: ì¤‘ìš”í•œ ìš”ì†Œë‚˜ ëª¨ë²” ì‚¬ë¡€ ì–¸ê¸‰
            """
        
        response = tracked_openai_client.chat_completions_create(
            call_type="improved_non_rag_answer",
            model=LLM_MODEL,
            messages=[
                {"role": "system", "content": chatgpt_system_prompt},
                {"role": "user", "content": prompt_template}
            ],
            temperature=0.1
        )
        
        final_answer = response.choices[0].message.content.strip()
        processing_time = round(time.time() - start_time, 2)
        
        print(f"\n{'='*70}")
        print(f"IMPROVED NON-RAG PIPELINE COMPLETE")
        print(f"  - Total time: {processing_time}s")
        print(f"  - Task type: {task_type}")
        print(f"  - XML chunks analyzed: {len(relevant_xml.split('===')) - 1}")
        print(f"{'='*70}\n")
        
        return {
            "question": user_question,
            "task_type": task_type,
            "final_answer": final_answer,
            "processing_time": processing_time,
            "method": "improved_non_rag_with_chunking",
            "xml_chunks_used": len(relevant_xml.split("===")) - 1
        }


def parse_answer_sections(answer: str) -> Tuple[str, str]:
    """ë‹µë³€ì—ì„œ ground_truthì™€ explanation ì„¹ì…˜ì„ ë¶„ë¦¬"""
    ground_truth = ""
    explanation = ""
    
    # [GROUND_TRUTH]ì™€ [EXPLANATION] ì„¹ì…˜ ì°¾ê¸°
    gt_match = re.search(r'\[GROUND_TRUTH\](.*?)(?:\[EXPLANATION\]|$)', answer, re.DOTALL | re.IGNORECASE)
    exp_match = re.search(r'\[EXPLANATION\](.*?)$', answer, re.DOTALL | re.IGNORECASE)
    
    if gt_match:
        ground_truth = gt_match.group(1).strip()
    
    if exp_match:
        explanation = exp_match.group(1).strip()
    
    # ì„¹ì…˜ì´ ì—†ìœ¼ë©´ ì „ì²´ ë‹µë³€ì„ explanationìœ¼ë¡œ ì‚¬ìš©
    if not ground_truth and not explanation:
        explanation = answer.strip()
        # ê°„ë‹¨í•œ ë‹µë³€ì¸ ê²½ìš° ground_truthë¡œë„ ì‚¬ìš©
        if len(answer.strip().split()) <= 10:
            ground_truth = answer.strip()
    
    return ground_truth, explanation

def evaluate_predictions(predictions: List[Dict], test_data: pd.DataFrame) -> Dict:
    """ì˜ˆì¸¡ ê²°ê³¼ë¥¼ í‰ê°€ - originì— ë”°ë¼ ë‹¤ë¥¸ í‰ê°€ ì ìš©"""
    print("\n" + "="*70)
    print("EVALUATION RESULTS")
    print("="*70)
    
    # ì „ì²´ ë°ì´í„° ì¤€ë¹„
    pred_ground_truths = []
    pred_explanations = []
    gt_ground_truths = test_data['ground_truth'].tolist()
    gt_explanations = test_data['explanation'].tolist()
    
    # origin ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ ì‚¬ìš©, ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¡œ ì²˜ë¦¬
    if 'origin' in test_data.columns:
        origins = test_data['origin'].tolist()
    else:
        print("[INFO] No 'origin' column found. Treating all as general evaluation.")
        origins = ['general'] * len(predictions)
    
    for pred in predictions:
        gt, exp = parse_answer_sections(pred['final_answer'])
        pred_ground_truths.append(gt)
        pred_explanations.append(exp)
    
    # Originë³„ ë°ì´í„° ë¶„ë¦¬
    if 'origin' in test_data.columns:
        rule_based_indices = [i for i, origin in enumerate(origins) if origin == 'rule-based']
        enhanced_llm_indices = [i for i, origin in enumerate(origins) if origin == 'enhanced_llm_with_agent']
    else:
        # origin ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ëª¨ë“  ë°ì´í„°ë¥¼ generalë¡œ ì²˜ë¦¬
        rule_based_indices = []
        enhanced_llm_indices = []
        general_indices = list(range(len(predictions)))
    
    print(f"\n[DATA DISTRIBUTION]")
    if 'origin' in test_data.columns:
        print(f"  - Rule-based questions: {len(rule_based_indices)}")
        print(f"  - Enhanced LLM questions: {len(enhanced_llm_indices)}")
    else:
        print(f"  - General questions: {len(predictions)}")
    print(f"  - Total questions: {len(predictions)}")
    
    # ì „ì²´ Ground Truth í‰ê°€ (EM, F1)
    print("\n[OVERALL GROUND_TRUTH EVALUATION]")
    gt_em_overall = calculate_exact_match(pred_ground_truths, gt_ground_truths)
    gt_f1_overall = calculate_f1_score(pred_ground_truths, gt_ground_truths)
    
    print(f"  - Overall Exact Match: {gt_em_overall:.4f}")
    print(f"  - Overall F1 Score: {gt_f1_overall:.4f}")
    
    # Rule-basedë§Œ Ground Truth í‰ê°€
    if rule_based_indices:
        print("\n[RULE-BASED GROUND_TRUTH EVALUATION]")
        rule_pred_gt = [pred_ground_truths[i] for i in rule_based_indices]
        rule_true_gt = [gt_ground_truths[i] for i in rule_based_indices]
        
        rule_gt_em = calculate_exact_match(rule_pred_gt, rule_true_gt)
        rule_gt_f1 = calculate_f1_score(rule_pred_gt, rule_true_gt)
        
        print(f"  - Rule-based Exact Match: {rule_gt_em:.4f}")
        print(f"  - Rule-based F1 Score: {rule_gt_f1:.4f}")
    else:
        rule_gt_em = rule_gt_f1 = 0.0
        print("\n[WARNING] No rule-based questions found!")
    
    # Enhanced LLM Ground Truth + Explanation í‰ê°€
    explanation_results = {}
    if enhanced_llm_indices:
        print("\n[ENHANCED_LLM GROUND_TRUTH EVALUATION]")
        enhanced_pred_gt = [pred_ground_truths[i] for i in enhanced_llm_indices]
        enhanced_true_gt = [gt_ground_truths[i] for i in enhanced_llm_indices]
        
        enhanced_gt_em = calculate_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_gt_f1 = calculate_f1_score(enhanced_pred_gt, enhanced_true_gt)
        
        print(f"  - Enhanced LLM Exact Match: {enhanced_gt_em:.4f}")
        print(f"  - Enhanced LLM F1 Score: {enhanced_gt_f1:.4f}")
        
        # Enhanced LLM Explanation í‰ê°€
        print("\n[ENHANCED_LLM EXPLANATION EVALUATION]")
        enhanced_pred_exp = [pred_explanations[i] for i in enhanced_llm_indices]
        enhanced_true_exp = [gt_explanations[i] for i in enhanced_llm_indices]
        
        # explanationì´ ë¹„ì–´ìˆì§€ ì•Šì€ ê²½ìš°ë§Œ í‰ê°€
        valid_exp_indices = [i for i, exp in enumerate(enhanced_true_exp) if exp and str(exp).strip()]
        
        if valid_exp_indices:
            valid_pred_exp = [enhanced_pred_exp[i] for i in valid_exp_indices]
            valid_true_exp = [enhanced_true_exp[i] for i in valid_exp_indices]
            
            bert_scores = calculate_bert_score(valid_pred_exp, valid_true_exp)
            rouge_scores = calculate_rouge_scores(valid_pred_exp, valid_true_exp)
            
            print(f"  - BERT F1: {bert_scores['f1']:.4f}")
            print(f"  - ROUGE-1 F1: {rouge_scores['rouge-1']:.4f}")
            print(f"  - ROUGE-2 F1: {rouge_scores['rouge-2']:.4f}")
            print(f"  - ROUGE-L F1: {rouge_scores['rouge-l']:.4f}")
            print(f"  - Valid explanations evaluated: {len(valid_exp_indices)}/{len(enhanced_llm_indices)}")
            
            explanation_results = {
                "bert_f1": bert_scores['f1'],
                "bert_precision": bert_scores['precision'],
                "bert_recall": bert_scores['recall'],
                "rouge_1_f1": rouge_scores['rouge-1'],
                "rouge_2_f1": rouge_scores['rouge-2'],
                "rouge_l_f1": rouge_scores['rouge-l'],
                "valid_count": len(valid_exp_indices),
                "total_count": len(enhanced_llm_indices)
            }
        else:
            print("  - No valid explanations found for evaluation")
            explanation_results = {
                "bert_f1": 0.0, "bert_precision": 0.0, "bert_recall": 0.0,
                "rouge_1_f1": 0.0, "rouge_2_f1": 0.0, "rouge_l_f1": 0.0,
                "valid_count": 0, "total_count": len(enhanced_llm_indices)
            }
            print("  - No valid explanations found for evaluation!")
            explanation_results = {
                "bert_f1": 0.0, "bert_precision": 0.0, "bert_recall": 0.0,
                "rouge_1_f1": 0.0, "rouge_2_f1": 0.0, "rouge_l_f1": 0.0,
                "valid_count": 0, "total_count": len(enhanced_llm_indices)
            }
    else:
        enhanced_gt_em = enhanced_gt_f1 = 0.0
        explanation_results = {
            "bert_f1": 0.0, "bert_precision": 0.0, "bert_recall": 0.0,
            "rouge_1_f1": 0.0, "rouge_2_f1": 0.0, "rouge_l_f1": 0.0,
            "valid_count": 0, "total_count": 0
        }
        print("\n[WARNING] No enhanced LLM questions found!")
    
    return {
        "overall": {
            "exact_match": gt_em_overall,
            "f1_score": gt_f1_overall,
            "total_questions": len(predictions)
        },
        "rule_based": {
            "exact_match": rule_gt_em,
            "f1_score": rule_gt_f1,
            "question_count": len(rule_based_indices)
        },
        "enhanced_llm": {
            "ground_truth": {
                "exact_match": enhanced_gt_em,
                "f1_score": enhanced_gt_f1
            },
            "explanation": explanation_results,
            "question_count": len(enhanced_llm_indices)
        }
    }
