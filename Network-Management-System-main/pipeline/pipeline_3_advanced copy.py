# pip install openai>=1.0.0 chromadb tiktoken langchain langchain-community
import os
import chromadb
import tiktoken
import pandas as pd 
from openai import OpenAI
from datetime import datetime
from multiprocessing import Process, Queue
from typing import List, Dict, Optional, Tuple
import time
import torch
from langchain_core.tools import Tool
from langchain_google_community import GoogleSearchAPIWrapper
from langchain_community.document_loaders import AsyncHtmlLoader
from langchain_community.document_transformers import Html2TextTransformer
from langchain_huggingface import HuggingFaceEmbeddings


# API Keys Configuration
# wlsruddms@gmail.com
os.environ["GOOGLE_CSE_ID"] = "API_key"  # Your Google CSE ID
os.environ["GOOGLE_API_KEY"] = "API_key"  # Your Google API Key
os.environ["OPENAI_API_KEY"] = ""

# Constants
OPENAI_EMBED_MODEL = "text-embedding-3-large"
EMBED_DIMS = None

# System prompt for network engineering assistant
chatgpt_system_prompt = """You are an expert network engineering assistant with deep knowledge of 
network configurations, troubleshooting, and security best practices. You have access to various 
network device configurations, XML schemas, and technical documentation."""

# Initialize OpenAI client
openai_client = OpenAI()  # í™˜ê²½ë³€ìˆ˜ OPENAI_API_KEY í•„ìš”

class OpenAIEmbedder:
    """OpenAI ì„ë² ë”© ìƒì„± í´ë˜ìŠ¤"""
    def __init__(self, model, dims: int | None = EMBED_DIMS):
        self.client = OpenAI()
        self.model = model
        self.dims = dims

    def embed(self, texts: list[str] | str) -> list[list[float]]:
        if isinstance(texts, str):
            texts = [texts]
        resp = self.client.embeddings.create(
            model=self.model,
            input=texts,
            **({"dimensions": self.dims} if self.dims else {})
        )
        return [d.embedding for d in resp.data]
    
class HuggingFaceEmbedder:
    def __init__(self, model_name, device, batch_size):
        self.model_name = model_name
        self.device = device
        self.batch_size = batch_size
        
        self.embedder = HuggingFaceEmbeddings(
            model_name=model_name,
            model_kwargs={"device": device},
            encode_kwargs={"batch_size": batch_size}
        )

        self.model_name = model_name
        self.device = device
        self.batch_size = batch_size

    def embed(self, texts: list[str] | str) -> list[list[float]]:
        if isinstance(texts, str):
            texts = [texts]
        return self.embedder.embed_documents(texts)

class ChromaDB:
    """ì‚¬ì „ ì„ë² ë”©ëœ XML íŒŒì¼ë“¤ì„ ìœ„í•œ ChromaDB ì¸í„°í˜ì´ìŠ¤"""
    def __init__(self, db_path: str, collection_name: str, embedder: object):
        self.db_path = db_path
        self.client = chromadb.PersistentClient(path=db_path)
        self.embedder = embedder or OpenAIEmbedder()
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ë¡œë“œ (ì‚¬ì „ ì„ë² ë”©ëœ XML ë°ì´í„°)
            self.collection = self.client.get_collection(name=collection_name)
            print(f"[INFO] Loaded existing collection: {collection_name}")
            print(f"[INFO] Total documents in collection: {self.collection.count()}")
        except:
            # ì»¬ë ‰ì…˜ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
            self.collection = self.client.create_collection(name=collection_name)
            print(f"[INFO] Created new collection: {collection_name}")

    def add_docs(self, ids: list[str], docs: list[str], metadatas: list[dict] | None = None):
        """ìƒˆ ë¬¸ì„œ ì¶”ê°€ (í•„ìš”ì‹œ)"""
        embeddings = self.embedder.embed(docs)
        self.collection.add(ids=ids, documents=docs, embeddings=embeddings, metadatas=metadatas)

    def query(self, text: str, n_results: int = 5) -> Dict:
        """ë²¡í„° ìœ ì‚¬ë„ ê²€ìƒ‰"""
        q_emb = self.embedder.embed(text)
        return self.collection.query(query_embeddings=q_emb, n_results=n_results)

def num_tokens_from_string(string: str, encoding_name: str = "cl100k_base") -> int:
    """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ê³„ì‚°"""
    encoding = tiktoken.get_encoding(encoding_name)
    return len(encoding.encode(string))

def chunk_texts(text: str, chunk_size: int = 1500) -> List[str]:
    """í…ìŠ¤íŠ¸ë¥¼ í† í° í¬ê¸°ì— ë§ê²Œ ë¶„í• """
    tokens = num_tokens_from_string(text)
    if tokens <= chunk_size:
        return [text]
    
    texts = []
    n = (tokens // chunk_size) + 1
    part_length = len(text) // n
    extra = len(text) % n
    
    parts = []
    start = 0
    for i in range(n):
        end = start + part_length + (1 if i < extra else 0)
        parts.append(text[start:end].replace('\n', " "))
        start = end
    
    return parts

def get_reranked_indices(question: str, documents: List[str], top_n: int = 5) -> List[int]:
    """LLMì„ ì‚¬ìš©í•˜ì—¬ 1ì°¨ ê²€ìƒ‰ ë¬¸ì„œë“¤ì˜ ì¤‘ìš”ë„ë¥¼ ì¬ì •ë ¬í•˜ê³  ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜.

    Args:
        question: ì‚¬ìš©ì ì§ˆë¬¸
        documents: 1ì°¨ ê²€ìƒ‰ëœ ë¬¸ì„œ í…ìŠ¤íŠ¸ ëª©ë¡
        top_n: ìµœì¢… ì„ íƒí•  ë¬¸ì„œ ìˆ˜

    Returns:
        ì¬ì •ë ¬ëœ ìƒìœ„ ë¬¸ì„œì˜ 0-based ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
    """
    if not documents:
        return []

    # LLMì— ì „ë‹¬í•  ë¬¸ì„œ í¬ë§·íŒ…
    docs_with_indices = [f"Doc[{i+1}]:\n{doc}" for i, doc in enumerate(documents)]
    docs_str = "\n\n".join(docs_with_indices)

    rerank_prompt = f"""
    You are an expert document analyst. Re-rank the documents by relevance to the user question.

    User Question: "{question}"

    Provided Documents:
    {docs_str}

    Instructions:
    1. Read the question and all documents carefully.
    2. Choose the MOST relevant documents that directly help answer the question.
    3. Output a comma-separated list of document numbers only (e.g., Doc[5],Doc[2],Doc[8]) in descending importance.
    4. Return up to {top_n} documents. No explanations.
    """

    try:
        response = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "You are a helpful assistant that ranks documents."},
                {"role": "user", "content": rerank_prompt}
            ],
            temperature=0.0
        )
        ranked_doc_indices_str = response.choices[0].message.content.strip()

        # ì˜ˆ: "Doc[5],Doc[2]" -> [4, 1]
        import re
        ranked_indices: List[int] = []
        matches = re.findall(r'Doc\[(\d+)\]', ranked_doc_indices_str)
        for m in matches:
            idx = int(m) - 1
            if 0 <= idx < len(documents):
                ranked_indices.append(idx)

        # ë¹„ì–´ìˆê±°ë‚˜ ì¼ë¶€ë§Œ ë°˜í™˜ ì‹œ ë³´ì™„
        if not ranked_indices:
            ranked_indices = list(range(min(top_n, len(documents))))

        return ranked_indices[:top_n]
    except Exception as e:
        print(f"[ERROR] Failed to re-rank documents: {e}")
        return list(range(min(top_n, len(documents))))

def get_reranked_documents(question: str, documents: List[str], top_n: int = 5) -> List[str]:
    """ë¬¸ì„œ ëª©ë¡ì„ LLMìœ¼ë¡œ ì¬ì •ë ¬í•˜ì—¬ ìƒìœ„ top_n ë¬¸ì„œë¥¼ ë°˜í™˜."""
    idx = get_reranked_indices(question, documents, top_n=top_n)
    return [documents[i] for i in idx]

def get_classification_result(question: str) -> str:
    """ì‚¬ìš©ì ì…ë ¥ì„ 2ê°€ì§€ ì‘ì—… ì¹´í…Œê³ ë¦¬ ì¤‘ í•˜ë‚˜ë¡œ ë¶„ë¥˜"""
    classification_prompt = '''
    You are an excellent network engineering assistant. Classify the question into ONE of these categories:

    1. **Simple Lookup Tasks** 
        - Tasks that can be solved by referring to or retrieving information from network configuration XML files
        - Simple lookup tasks: Executing commands for the purpose of retrieving device status, routing tables, interfaces, sessions, logs, etc

    2. **Other Tasks** 
        - All other cases that do not rely on network configuration XML files
        - System control commands: performing configuration changes, applying policies, and executing device control commands
        - Technical errors: conducting fault analysis, identifying root causes, executing recovery procedures, and resolving issues based on logs
        - Configuration/Policy review and optimization: reviewing network configurations and optimizing routing/security policies
        - Security/Audit response: analyzing security events, performing compliance checks, and handling audit responses

    IMPORTANT:
    - Return ONLY the exact category name, nothing else.
    ex) Simple Lookup Tasks
    ex) Other Tasks
    '''
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nInstruction: {classification_prompt}"}
        ],
        temperature=0.05
    )
    
    task_type = response.choices[0].message.content.strip()
    print(f"[INFO] Task classified as: {task_type}")
    return task_type

def get_draft(question: str, task_type: str) -> str:
    """ì‘ì—… ìœ í˜•ì— ë”°ë¥¸ ë§ì¶¤í˜• ì´ˆì•ˆ ìƒì„±"""

    if task_type == "Simple Lookup Tasks":
        prompt = '''
            This is a simple lookup query. Provide a direct answer based on your knowledge.

            Be extremely concise. No explanations needed.
        '''
    else:
        prompt = '''
            Provide a comprehensive technical answer following this structure:

            1. Direct Answer: State the solution clearly
            2. Technical Implementation: Provide specific steps or configurations
            3. Considerations: Note any important factors or best practices

            Use proper network engineering terminology and be specific with commands/configurations.
        '''
    # ì´ˆì•ˆì€ Temperature ë†’ê²Œ.
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\n{prompt}"}
        ],
        temperature=0.3
    )
    
    return response.choices[0].message.content

def get_xml_query(question: str, answer: str) -> str:
    """ChromaDBì˜ XML íŒŒì¼ ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    query_prompt = '''
        You are responsible for generating search queries in ChromaDB to find relevant XML network configuration files and related documents.

        CONTEXT:
            User Question: {question}
            Search History (information found or concluded so far): {answer}

        OBJECTIVE:
            Generate more effective search queries that expand beyond what has already been attempted or concluded in the history.

        OUTPUT INSTRUCTIONS:
            - Output ONLY the search query string.
            - Do not include explanations, reasoning, or extra text.
        '''
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nContent: {answer}\n\nInstruction: {query_prompt}"}
        ],
        temperature=0.05
    )
    
    return response.choices[0].message.content.strip()

def get_internet_query(question: str, answer: str) -> str:
    """ì¸í„°ë„· ê²€ìƒ‰ì„ ìœ„í•œ ì¿¼ë¦¬ ìƒì„±"""
    query_prompt = '''
        You are responsible for generating a Google search query to validate and reinforce the technical accuracy of the given network engineering answer.

        CONTEXT:
            User Question: {question}
            Proposed Answer: {answer}

        OBJECTIVE:
            - Find authoritative and reliable references that can support, confirm, or refine the proposed answer.
            - Prioritize high-trust sources such as:
                * Official vendor documentation (Cisco, Juniper, Arista, Huawei, etc.)
                * Standards bodies (IETF RFCs, IEEE standards)
                * Well-known security advisories (NVD, CERT, vendor advisories)
                * Recognized industry knowledge bases (Cisco Community, Juniper TechLibrary, etc.)
            - Ensure the query is precise, technical, and aimed at verifying correctness.
            - If possible, emphasize recency to capture the latest best practices, features, or advisories.

        OUTPUT INSTRUCTIONS:
            - Output ONLY the search query string.
            - Do not include explanations, reasoning, or extra text.
        '''
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", "content": f"Question: {question}\n\nContent: {answer}\n\nInstruction: {query_prompt}"}
        ],
        temperature=0.1
    )
    
    return response.choices[0].message.content.strip()


def get_google_search(query: str = "", k: int = 3) -> Optional[List[Dict]]:
    """ìˆ˜ì •ëœ Google Search API í•¨ìˆ˜"""
    try:
        # ë””ë²„ê¹… ì •ë³´ ì¶œë ¥
        print(f"[DEBUG] Google search query: '{query}' (k={k})")
        
        # API í‚¤ í™•ì¸
        api_key = os.environ.get("GOOGLE_API_KEY")
        cse_id = os.environ.get("GOOGLE_CSE_ID")
        
        if not api_key:
            print("[ERROR] GOOGLE_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
            
        if not cse_id:
            print("[ERROR] GOOGLE_CSE_ID í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            return None
        
        # GoogleSearchAPIWrapper ìƒì„±
        search = GoogleSearchAPIWrapper(
            k=k,
            google_api_key=api_key,
            google_cse_id=cse_id
        )
        
        # ê²€ìƒ‰ í•¨ìˆ˜ ì •ì˜
        def search_results(query):
            try:
                results = search.results(query, k)
                print(f"[DEBUG] Search results count: {len(results) if results else 0}")
                return results
            except Exception as e:
                print(f"[ERROR] search.results() í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
                return None
        
        # Tool ìƒì„±
        tool = Tool(
            name="Google Search Snippets",
            description="Search Google for recent results.",
            func=search_results,
        )
        
        # ê²€ìƒ‰ ì‹¤í–‰
        ref_text = tool.run(query)
        
        if len(ref_text) > 0:
            # ê²°ê³¼ ê²€ì¦ - ì²« ë²ˆì§¸ í•­ëª©ì´ ë”•ì…”ë„ˆë¦¬ì¸ì§€ í™•ì¸
            first_item = ref_text[0]
            if isinstance(first_item, dict):
                # 'Result' í‚¤ê°€ ì—†ìœ¼ë©´ ìœ íš¨í•œ ê²°ê³¼ë¡œ íŒë‹¨ (ì›ë˜ ì¡°ê±´ ìˆ˜ì •)
                if 'Result' not in first_item:
                    print(f"[SUCCESS] Valid search results found: {len(ref_text)} items")
                    return ref_text
                else:
                    print("[WARNING] Results contain 'Result' key - treating as invalid")
                    return None
            else:
                # ë”•ì…”ë„ˆë¦¬ê°€ ì•„ë‹ˆì–´ë„ ê²°ê³¼ê°€ ìˆë‹¤ë©´ ë°˜í™˜
                print(f"[INFO] Non-dict results found: {len(ref_text)} items")
                return ref_text
        else:
            print("[INFO] No search results returned")
            return None
            
    except ImportError as e:
        print(f"[ERROR] í•„ìš”í•œ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì‹¤íŒ¨: {e}")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”: pip install langchain-google-community")
        return None
    except Exception as e:
        print(f"[ERROR] Google search ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        return None

def get_page_content(link: str) -> Optional[str]:
    """ì›¹ í˜ì´ì§€ ë‚´ìš© ì¶”ì¶œ"""
    try:
        loader = AsyncHtmlLoader([link])
        docs = loader.load()
        html2text = Html2TextTransformer()
        docs_transformed = html2text.transform_documents(docs)
        if len(docs_transformed) > 0:
            return docs_transformed[0].page_content
        else:
            return None
    except Exception as e:
        print(f"[ERROR] Failed to fetch page content: {e}")
        return None
    
def get_internet_content(query: str) -> Optional[List[str]]:
    """Google ê²€ìƒ‰ì„ í†µí•´ ì¸í„°ë„· ì½˜í…ì¸  ê°€ì ¸ì˜¤ê¸°"""
    search_results = get_google_search(query, k=3)
    
    if not search_results:
        print("[INFO] No Google search results found")
        return None
    
    all_content = []
    for result in search_results[:2]:  # ìƒìœ„ 2ê°œ ê²°ê³¼ë§Œ ì²˜ë¦¬
        link = result.get('link')
        if link:
            print(f"[INFO] Fetching content from: {link}")
            page_content = get_page_content(link)
            if page_content:
                # ì½˜í…ì¸ ë¥¼ ì²­í¬ë¡œ ë¶„í• 
                chunks = chunk_texts(page_content, 1500)
                all_content.extend(chunks[:2])  # ê° í˜ì´ì§€ì—ì„œ ìµœëŒ€ 2ê°œ ì²­í¬
    
    return all_content if all_content else None

def get_revise_answer(question: str, answer: str, content: str, task_type: str) -> str:
    """ì°¸ì¡° ìë£Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ ìˆ˜ì •"""
    if task_type == "Simple Lookup Tasks":
        prompt = '''
            You are revising the given answer using the provided reference content.

            CONTEXT:
                User Question: {question}
                Trusted Answer: {answer}
                Reference Content: {content}

            INSTRUCTIONS:
                - Treat the given answer as reliable and relevant to the question.
                - Use the reference content only to confirm or slightly refine the answer.
                - Keep the revised answer short, direct, and concise.
                - Do not add extra explanations or unrelated details.
                - Output ONLY the revised answer text.
        '''
    else:
        prompt = '''
            Revise the answer using the reference content to ensure technical accuracy.

            Guidelines:
            - Verify all technical details against the reference
            - Add any missing critical information
            - Correct any errors or outdated practices
            - Maintain professional technical language
            - Focus on actionable information

            Output the revised answer directly without any meta-commentary.
        '''
        
    # # ë¶„ë¥˜ ê²°ê³¼ ì—†ìœ¼ë©´ Simple Lookup Tasksë¡œ    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt},
            {"role": "user", 
             "content": f"Reference Content: {content}\n\nQuestion: {question}\n\nOriginal Answer: {answer}\n\nInstruction: {prompt}"}
        ],
        temperature=0.1
    )
    
    return response.choices[0].message.content

def determine_reference_source(task_type: str, iteration: int) -> str:
    """ì‘ì—… ìœ í˜•ê³¼ ë°˜ë³µ íšŸìˆ˜ì— ë”°ë¼ ì°¸ì¡° ì†ŒìŠ¤ ê²°ì •"""
    if task_type == "Simple Lookup Tasks":
        # Simple Lookupì€ ì£¼ë¡œ ë‚´ë¶€ XML DB ì‚¬ìš©
        return "chromadb"
    else:
        return "chromadb" if iteration % 2 == 0 else "internet"

def run_with_timeout(func, timeout, *args, **kwargs):
    """íƒ€ì„ì•„ì›ƒì´ ìˆëŠ” í•¨ìˆ˜ ì‹¤í–‰"""
    q = Queue()
    
    def wrapper(q, *args, **kwargs):
        try:
            result = func(*args, **kwargs)
            q.put(result)
        except Exception as e:
            print(f"[ERROR] Function execution failed: {e}")
            q.put(None)
    
    p = Process(target=wrapper, args=(q, *args), kwargs=kwargs)
    p.start()
    p.join(timeout)
    
    if p.is_alive():
        print(f"[WARNING] Function timed out after {timeout}s")
        p.terminate()
        p.join()
        return None
    else:
        return q.get() if not q.empty() else None

def get_final_response(question: str, refined_answer: str, task_type:str) -> str:
    """ìµœì¢… ì‘ë‹µ ìƒì„± - Exact Match ë° BERT-F1 Score ìµœì í™”"""
    if task_type == "Simple Lookup Tasks":
        final_prompt = """
            You are providing a final answer for a network lookup query.

            CRITICAL INSTRUCTIONS:
            1. Output ONLY the device names or values requested
            2. DO NOT include labels like "Final Answer:" or "Explanation:"
            3. Format as a simple comma-separated list or single value
            4. Be extremely concise and factual

            Example Input: "Which devices have SSH enabled?"
            Example Output: CE1, CE2, sample10

            Now provide the answer for the given question.
        """
    else:  # Other Tasks
        final_prompt = """
            You are providing a comprehensive network engineering solution.

            OUTPUT FORMAT (use exactly this structure):
            [ANSWER]
            {Your concise direct answer here}

            [TECHNICAL DETAILS]
            {Detailed technical explanation with:
            - Step-by-step instructions if applicable
            - Configuration commands if relevant
            - Security considerations
            - Best practices}

            IMPORTANT:
            - Start directly with [ANSWER] section
            - Keep the answer section brief and direct
            - Provide thorough technical details in the second section

            LANGUAEG:
            - ë°˜ë“œì‹œ í•œêµ­ì–´ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”.
        """
    
    response = openai_client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": chatgpt_system_prompt + "\n\nYou are optimizing responses for exact match and BERT-F1 evaluation metrics."},
            {"role": "user", 
             "content": f"Question: {question}\n\nRefined Answer: {refined_answer}\n\nInstruction: {final_prompt}"}
        ],
        temperature=0.1
    )
    
    return response.choices[0].message.content.strip()

class NetworkEngineeringPipeline:
    """ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ë§ LLM íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, chromadb_path: str, 
                 collection_name: str,
                 max_iterations: int):
        """
        ì´ˆê¸°í™”
        Args:
            chromadb_path: ì‚¬ì „ ì„ë² ë”©ëœ XML íŒŒì¼ì´ ìˆëŠ” ChromaDB ê²½ë¡œ
            collection_name: XML ì„¤ì • íŒŒì¼ ì»¬ë ‰ì…˜ ì´ë¦„
            max_iterations: ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜
        """
        self.db = ChromaDB(chromadb_path, collection_name, 
            embedder=HuggingFaceEmbedder(
                model_name="Qwen/Qwen3-Embedding-8B",
                device="cuda:1",
                batch_size=8,
            )
        )
        self.max_iterations = max_iterations
        print(f"[INFO] Pipeline initialized with {max_iterations} max iterations")
        
    def get_chromadb_content(self, question: str, answer: str, n_results=5, top_n_after_rerank: int = 5) -> Optional[str]:
        """ChromaDBì—ì„œ ê´€ë ¨ XML ì„¤ì • íŒŒì¼ ê²€ìƒ‰, LLMìœ¼ë¡œ Re-ranking, ê·¸ë¦¬ê³  ë¡œê¹…

        Args:
            question: ì‚¬ìš©ì ì§ˆë¬¸
            answer: í˜„ì¬ê¹Œì§€ì˜ ì´ˆì•ˆ/ë‹µë³€ (ë¯¸ì‚¬ìš© ê°€ëŠ¥)
            n_results: 1ì°¨ í›„ë³´êµ° ê°œìˆ˜ (ë²¡í„° ê²€ìƒ‰ Top-K)
            top_n_after_rerank: Re-ranking í›„ ìµœì¢… ì»¨í…ìŠ¤íŠ¸ë¡œ ì‚¬ìš©í•  ë¬¸ì„œ ìˆ˜
        """
        query = f"ë‹¨ìˆœ ì¡°íšŒ, {question}"
        print(f"[INFO] ChromaDB query: {query}")
        
        results = self.db.query(query, n_results=n_results)
        
        if results and results['documents'] and results['documents'][0]:
            documents = results['documents'][0]
            metadatas = results['metadatas'][0] if results.get('metadatas') else None

            # Re-ranking ì ìš© (í›„ë³´êµ°ì´ ì¶©ë¶„íˆ í´ ë•Œë§Œ)
            if len(documents) > 1:
                print(f"  â”œâ”€ Re-ranking {len(documents)} candidates â†’ top {min(top_n_after_rerank, len(documents))}")
                ranked_indices = get_reranked_indices(question, documents, top_n=top_n_after_rerank)

                # ì¸ë±ìŠ¤ ê¸°ë°˜ìœ¼ë¡œ ë¬¸ì„œ/ë©”íƒ€ë°ì´í„° ì •ë ¬
                documents = [documents[i] for i in ranked_indices]
                if metadatas:
                    metadatas = [metadatas[i] for i in ranked_indices]
                print(f"  â””â”€ âœ“ Re-ranking complete. Selected {len(documents)} docs")
            
            # ë©”íƒ€ë°ì´í„° ì •ë³´ ì¶œë ¥
            if metadatas:
                for i, meta in enumerate(metadatas):
                    print(f"  - Document {i+1}: {meta}")
            
            # ì œì™¸í•  í‚¤
            exclude_keys = {"file_path", "source", "filename", "source_directory"}
            
            # ë¬¸ì„œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ í•¨ê»˜ join
            if metadatas:
                combined_content = "\n\n".join(
                    f"[METADATA]\n" +
                    "\n".join(f"{k}: {v}" for k, v in meta.items() if k not in exclude_keys) +
                    f"\n\n[CONTENT]\n{doc}"
                    for doc, meta in zip(documents, metadatas)
                )
            else:
                combined_content = "\n\n".join(documents)
            
            # ë””ë²„ê¹… ë¡œê·¸ ì €ì¥
            log_path = "input_docs.log"
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(f"\n[QUESTION]\n{question}\n")
                f.write(f"\n[COMBINED_CONTENT]\n{combined_content}\n")
                f.write("=" * 80 + "\n")
            
            return combined_content
        
        return None
    
    def process_query(self, user_question: str, top_k_chroma) -> Dict:
        """ì‚¬ìš©ì ì¿¼ë¦¬ ì²˜ë¦¬ ë©”ì¸ íŒŒì´í”„ë¼ì¸"""
        start_time = time.time()
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        
        # JSON ë¡œê¹…ì„ ìœ„í•œ ë°ì´í„° ì €ì¥
        log_data = []
        
        print(f"\n{'='*70}")
        print(f"NETWORK ENGINEERING LLM PIPELINE")
        print(f"{'='*70}")
        print(f"Query: {user_question[:100]}...")
        print(f"Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"{'='*70}\n")
        
        # 1. ì‘ì—… ë¶„ë¥˜
        print("[STEP 1/6] Classifying task type...")
        task_type = get_classification_result(user_question)
        print(f"  â””â”€ Classified as: {task_type}")
        
        # JSON ë¡œê¹…: ì‘ì—… ë¶„ë¥˜
        log_data.append({
            "step": "task_classification",
            "content": task_type,
            "timestamp": timestamp
        })
        # 2. ì´ˆì•ˆ ì‘ì„±
        print("[WARNNING] Task Type : ", task_type)
        print("\n[STEP 2/6] Generating initial draft...")
            
        current_answer = get_draft(user_question, task_type)

        # JSON ë¡œê¹…: ì´ˆì•ˆ
        log_data.append({
            "step": "initial_draft",
            "content": current_answer,
            "timestamp": timestamp
        })
        
        # ê²°ê³¼ ì €ì¥
        results = {
            "question": user_question,
            "task_type": task_type,
            "initial_draft": current_answer,
            "iterations": [],
            "total_revisions": 0
        }
        
        # 3-5. ë°˜ë³µì  ê°œì„ 
        print(f"\n[STEP 3-5/6] Iterative refinement ({self.max_iterations} iterations)")
        print("â”€" * 50)
        
        for iteration in range(self.max_iterations):
            print(f"\n[ITERATION {iteration + 1}/{self.max_iterations}]")
            
            # ì°¸ì¡° ì†ŒìŠ¤ ê²°ì •
            ref_source = determine_reference_source(task_type, iteration)
            print(f"  â”œâ”€ Reference source: {ref_source.upper()}")
            
            reference_content = None
            source_details = {}
            
            if ref_source == "chromadb":
                # ChromaDBì—ì„œ XML ì„¤ì • íŒŒì¼ ê²€ìƒ‰
                print("  â”œâ”€ Searching ChromaDB for XML configurations...")
                reference_content = self.get_chromadb_content(user_question, current_answer, n_results=top_k_chroma)
                
                
                if reference_content:
                    print(f"  â”œâ”€ Found relevant XML configurations")
                    source_details = {"type": "xml_config", "source": "chromadb"}
                else:
                    print("  â”œâ”€ No relevant XML found in ChromaDB")
                    
            else:  # internet
                # ì¸í„°ë„·ì—ì„œ ìµœì‹  ì •ë³´ ê²€ìƒ‰
                print("  â”œâ”€ Searching internet for latest information...")
                query = get_internet_query(user_question, current_answer)
                print(f"  â”œâ”€ Search query: {query}")
                
                content_list = get_internet_content(query)
                
                if content_list:
                    reference_content = "\n\n".join(content_list)
                    print(f"  â”œâ”€ Retrieved {len(content_list)} content chunks")
                    source_details = {"type": "web_content", "source": "google"}
                else:
                    print("  â”œâ”€ No relevant content found online")
            
            # ì°¸ì¡° ìë£Œê°€ ìˆìœ¼ë©´ ë‹µë³€ ìˆ˜ì •
            if reference_content:
                print("  â”œâ”€ Revising answer with references...")
                revised_answer = run_with_timeout(
                    get_revise_answer, 10, user_question, current_answer, 
                    reference_content, task_type  # í† í° ì œí•œ
                )
                
                if revised_answer and revised_answer != current_answer:
                    print("  â””â”€ âœ“ Answer improved")
                    current_answer = revised_answer
                    results["total_revisions"] += 1
                    
                    # JSON ë¡œê¹…: ìˆ˜ì •ëœ ë‹µë³€
                    log_data.append({
                        "step": f"iteration_{iteration + 1}_revised",
                        "content": current_answer,
                        "timestamp": timestamp
                    })
                    
                    iteration_result = {
                        "iteration": iteration + 1,
                        "source": ref_source,
                        "reference_found": True,
                        "answer_revised": True,
                        "source_details": source_details
                    }
                else:
                    print("  â””â”€ â—‹ No changes needed")
                    iteration_result = {
                        "iteration": iteration + 1,
                        "source": ref_source,
                        "reference_found": True,
                        "answer_revised": False,
                        "source_details": source_details
                    }
            else:
                print("  â””â”€ â—‹ No references found")
                iteration_result = {
                    "iteration": iteration + 1,
                    "source": ref_source,
                    "reference_found": False,
                    "answer_revised": False,
                    "source_details": {}
                }
            
            results["iterations"].append(iteration_result)
        
        # 6. ìµœì¢… ì‘ë‹µ ìƒì„± (NEW STEP)
        print(f"\n[STEP 6/6] Generating final optimized response...")
        print("  â”œâ”€ Optimizing for Exact Match and BERT-F1 Score...")
        
        final_response = run_with_timeout(
            get_final_response, 10, user_question, current_answer, task_type
        )
        
        if final_response and final_response != current_answer:
            print("  â””â”€ âœ“ Final response optimized for evaluation metrics")
            results["final_optimization"] = True
        else:
            print("  â””â”€ â—‹ Current answer already optimal")
            final_response = current_answer
            results["final_optimization"] = False
        
        # JSON ë¡œê¹…: ìµœì¢… ì‘ë‹µ
        log_data.append({
            "step": "final_response",
            "content": final_response,
            "timestamp": timestamp
        })
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        import json
        import os
        
        # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
        log_dir = "logs"
        if not os.path.exists(log_dir):
            os.makedirs(log_dir)
        
        # JSON íŒŒì¼ ê²½ë¡œ
        json_filename = f"{log_dir}/pipeline_log_{timestamp}.json"
        
        # JSON í˜•íƒœë¡œ ì „ì²´ ë¡œê·¸ êµ¬ì„±
        complete_log = {
            "question": user_question,
            "task_type": task_type,
            "timestamp": timestamp,
            "pipeline_steps": log_data
        }
        
        # JSON íŒŒì¼ ì €ì¥
        with open(json_filename, 'w', encoding='utf-8') as jsonfile:
            json.dump(complete_log, jsonfile, ensure_ascii=False, indent=2)
        
        print(f"  â”œâ”€ Logged to: {json_filename}")
        results["log_file"] = json_filename
        
        # ìµœì¢… ê²°ê³¼
        results["final_answer"] = final_response
        results["processing_time"] = round(time.time() - start_time, 2)
        
        print(f"\n{'='*70}")
        print(f"PIPELINE COMPLETE")
        print(f"  - Total time: {results['processing_time']}s")
        print(f"  - Total revisions: {results['total_revisions']}")
        print(f"  - Final optimization: {'YES' if results['final_optimization'] else 'NO'}")
        print(f"  - Task type: {task_type}")
        print(f"{'='*70}\n")
        
        return results

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
def main():
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì˜ˆì œ"""
    
    CHROMADB_PATH = "/workspace/jke/chromadb_qwen"  # ì‚¬ì „ ì„ë² ë”©ëœ XML íŒŒì¼ë“¤ì´ ì €ì¥ëœ ê²½ë¡œ
    COLLECTION_NAME = "network_devices"  # XML ì„¤ì • íŒŒì¼ ì»¬ë ‰ì…˜
    MAX_ITERATIONS = 3  # ë°˜ë³µ íšŸìˆ˜ ì„¤ì •

    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = NetworkEngineeringPipeline(
        chromadb_path=CHROMADB_PATH,  # ì‚¬ì „ ì„ë² ë”©ëœ XML DB ê²½ë¡œ
        collection_name=COLLECTION_NAME,
        max_iterations=MAX_ITERATIONS
    )
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    # "CE1 IP address.",
    csv_path = "/workspace/Yujin/GIA/Network-Management-System-main/dataset/test_fin.csv"

    # question ì»¬ëŸ¼ë§Œ ì½ì–´ì„œ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜  
    df = pd.read_csv(csv_path)
    test_queries = df["question"].dropna().tolist()
    
    top_k_chromas = [1, 5, 10, 20, 50]
    # ì²« ë²ˆì§¸ ì¿¼ë¦¬ ì‹¤í–‰
    for top_k_chroma in top_k_chromas:
        for query in test_queries:
            results = pipeline.process_query(query, top_k_chroma=top_k_chroma)
            
            # ê²°ê³¼ ì¶œë ¥
            print("\n" + "="*70)
            print("FINAL RESULTS")
            print("="*70)
            print(f"\nQuestion: {results['question']}")
            print(f"Task Type: {results['task_type']}")
            print(f"Processing Time: {results['processing_time']}s")
            print(f"Total Revisions: {results['total_revisions']}")
            
            print("\nIteration Summary:")
            for iter_info in results['iterations']:
                status = "âœ“" if iter_info['answer_revised'] else "â—‹"
                print(f"  {status} Iteration {iter_info['iteration']}: "
                    f"{iter_info['source']} - "
                    f"{'Found' if iter_info['reference_found'] else 'Not found'}")
            
            print(f"\n{'â”€'*70}")
            print("FINAL ANSWER:")
            print("â”€"*70)
            print(results['final_answer'])
            print("="*70)

            # ğŸ”¹ ë¡œê·¸ íŒŒì¼ ì €ì¥ ì¶”ê°€
            with open(f"pipeline_results_qwen_final_exprement_{top_k_chroma}.log", "a", encoding="utf-8") as f:
                f.write("="*70 + "\n")
                f.write(f"Question: {results['question']}\n")
                f.write(f"Final Answer: {results['final_answer']}\n")
                f.write("="*70 + "\n\n")

    # for query in test_queries:
    #     results = pipeline.process_query(query, top_k_chroma=20)
        
    #     # ê²°ê³¼ ì¶œë ¥
    #     print("\n" + "="*70)
    #     print("FINAL RESULTS")
    #     print("="*70)
    #     print(f"\nQuestion: {results['question']}")
    #     print(f"Task Type: {results['task_type']}")
    #     print(f"Processing Time: {results['processing_time']}s")
    #     print(f"Total Revisions: {results['total_revisions']}")
        
    #     print("\nIteration Summary:")
    #     for iter_info in results['iterations']:
    #         status = "âœ“" if iter_info['answer_revised'] else "â—‹"
    #         print(f"  {status} Iteration {iter_info['iteration']}: "
    #             f"{iter_info['source']} - "
    #             f"{'Found' if iter_info['reference_found'] else 'Not found'}")
        
    #     print(f"\n{'â”€'*70}")
    #     print("FINAL ANSWER:")
    #     print("â”€"*70)
    #     print(results['final_answer'])
    #     print("="*70)

    #     # ğŸ”¹ ë¡œê·¸ íŒŒì¼ ì €ì¥ ì¶”ê°€
    #     with open(f"pipeline_results_qwen_improved_prompt{20}.log", "a", encoding="utf-8") as f:
    #         f.write("="*70 + "\n")
    #         f.write(f"Question: {results['question']}\n")
    #         f.write(f"Final Answer: {results['final_answer']}\n")
    #         f.write("="*70 + "\n\n")

if __name__ == "__main__":
    # API í‚¤ í™•ì¸
    if not os.environ.get("OPENAI_API_KEY"):
        print("[ERROR] Please set OPENAI_API_KEY environment variable")
        exit(1)
    
    if not os.environ.get("GOOGLE_API_KEY") or not os.environ.get("GOOGLE_CSE_ID"):
        print("[WARNING] Google Search API keys not set. Internet search will be disabled.")
    
    main()
