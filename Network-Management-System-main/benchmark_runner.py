"""
í†µí•© ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ ì‹œìŠ¤í…œ
ë‹¤ì–‘í•œ LLM ëª¨ë¸ë“¤ì„ í†µí•© í‰ê°€í•˜ëŠ” ë©”ì¸ ì‹œìŠ¤í…œ
"""

import os
import json
import time
import pandas as pd
from typing import Dict, List, Any, Optional
from datetime import datetime
import argparse

# ë¡œì»¬ ëª¨ë“ˆë“¤
from llm_manager import LLMManager
from experiment_logger import ExperimentLogger, ExperimentResult, ExperimentVisualizer
from pipeline.pipeline_3_advanced import NetworkEngineeringPipeline

# í‰ê°€ ë©”íŠ¸ë¦­
from bert_score import score as bert_score
from rouge_score import rouge_scorer
import re

class BenchmarkEvaluator:
    """ë²¤ì¹˜ë§ˆí¬ í‰ê°€ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.rouge_scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)
    
    def normalize_text(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ ì •ê·œí™”"""
        text = re.sub(r'\s+', ' ', text.strip())
        return text.lower()
    
    def exact_match(self, prediction: str, ground_truth: str) -> float:
        """ì™„ì „ ì¼ì¹˜ ì ìˆ˜"""
        pred_norm = self.normalize_text(prediction)
        gt_norm = self.normalize_text(ground_truth)
        return 1.0 if pred_norm == gt_norm else 0.0
    
    def f1_score(self, prediction: str, ground_truth: str) -> float:
        """F1 ì ìˆ˜ ê³„ì‚°"""
        pred_tokens = set(self.normalize_text(prediction).split())
        gt_tokens = set(self.normalize_text(ground_truth).split())
        
        if not gt_tokens:
            return 1.0 if not pred_tokens else 0.0
        
        intersection = pred_tokens & gt_tokens
        if not intersection:
            return 0.0
        
        precision = len(intersection) / len(pred_tokens) if pred_tokens else 0.0
        recall = len(intersection) / len(gt_tokens)
        
        if precision + recall == 0:
            return 0.0
        
        return 2 * (precision * recall) / (precision + recall)
    
    def bert_score_single(self, prediction: str, ground_truth: str) -> float:
        """ë‹¨ì¼ BERT ì ìˆ˜ ê³„ì‚°"""
        try:
            P, R, F1 = bert_score([prediction], [ground_truth], lang="en", verbose=False)
            return F1.item()
        except:
            return 0.0
    
    def rouge_l_score(self, prediction: str, ground_truth: str) -> float:
        """ROUGE-L ì ìˆ˜ ê³„ì‚°"""
        try:
            scores = self.rouge_scorer.score(ground_truth, prediction)
            return scores['rougeL'].fmeasure
        except:
            return 0.0
    
    def evaluate_prediction(self, prediction: str, ground_truth: str) -> Dict[str, float]:
        """ë‹¨ì¼ ì˜ˆì¸¡ì— ëŒ€í•œ ì¢…í•© í‰ê°€"""
        return {
            'exact_match': self.exact_match(prediction, ground_truth),
            'f1_score': self.f1_score(prediction, ground_truth),
            'bert_score': self.bert_score_single(prediction, ground_truth),
            'rouge_l': self.rouge_l_score(prediction, ground_truth)
        }

class BenchmarkRunner:
    """ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰ê¸°"""
    
    def __init__(self, config_file: str = "benchmark_config.json"):
        self.config_file = config_file
        self.config = self.load_config()
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.llm_manager = LLMManager()
        self.logger = ExperimentLogger()
        self.evaluator = BenchmarkEvaluator()
        self.visualizer = ExperimentVisualizer(self.logger)
        
        # RAG íŒŒì´í”„ë¼ì¸ (í•„ìš”ì‹œ)
        self.rag_pipeline = None
        if self.config.get('use_rag', False):
            self.rag_pipeline = NetworkEngineeringPipeline(
                chromadb_path=self.config.get('chromadb_path', '/workspace/jke/chromadb_qwen'),
                collection_name=self.config.get('collection_name', 'network_devices'),
                max_iterations=self.config.get('max_iterations', 3)
            )
    
    def load_config(self) -> Dict[str, Any]:
        """ì„¤ì • íŒŒì¼ ë¡œë“œ"""
        if os.path.exists(self.config_file):
            with open(self.config_file, 'r') as f:
                return json.load(f)
        else:
            # ê¸°ë³¸ ì„¤ì • ìƒì„±
            default_config = {
                "dataset_path": "dataset/test.csv",
                "models_to_test": ["gpt-4o-mini", "claude-3-haiku", "llama-3-8b", "qwen-2.5-7b"],
                "use_rag": True,
                "top_k_values": [1, 5, 10, 20, 50],
                "max_questions": 100,
                "chromadb_path": "/workspace/jke/chromadb_qwen",
                "collection_name": "network_devices",
                "max_iterations": 3,
                "output_dir": "benchmark_results"
            }
            
            with open(self.config_file, 'w') as f:
                json.dump(default_config, f, indent=2)
            
            return default_config
    
    def load_dataset(self) -> pd.DataFrame:
        """ë²¤ì¹˜ë§ˆí¬ ë°ì´í„°ì…‹ ë¡œë“œ"""
        dataset_path = self.config['dataset_path']
        
        if not os.path.exists(dataset_path):
            raise FileNotFoundError(f"Dataset not found: {dataset_path}")
        
        df = pd.read_csv(dataset_path)
        
        # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
        required_columns = ['question', 'ground_truth']
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"Missing required column: {col}")
        
        # ë°ì´í„° ì œí•œ
        max_questions = self.config.get('max_questions', len(df))
        df = df.head(max_questions)
        
        return df
    
    def run_baseline_experiment(self, experiment_id: str, model_name: str, questions: List[str], ground_truths: List[str]):
        """Baseline ì‹¤í—˜ (RAG ì—†ìŒ)"""
        print(f"\nğŸš€ Running Baseline Experiment: {model_name}")
        
        model = self.llm_manager.get_model(model_name)
        if not model or not model.is_available():
            print(f"âŒ Model {model_name} not available")
            return
        
        results = []
        
        for i, (question, gt) in enumerate(zip(questions, ground_truths)):
            print(f"Processing {i+1}/{len(questions)}: {question[:50]}...")
            
            try:
                start_time = time.time()
                prediction = model.generate(question)
                end_time = time.time()
                
                # í‰ê°€
                metrics = self.evaluator.evaluate_prediction(prediction, gt)
                
                result = ExperimentResult(
                    experiment_id=experiment_id,
                    timestamp=datetime.now().isoformat(),
                    model_name=model_name,
                    question=question,
                    predicted_answer=prediction,
                    ground_truth=gt,
                    exact_match=metrics['exact_match'],
                    f1_score=metrics['f1_score'],
                    bert_score=metrics['bert_score'],
                    rouge_l=metrics['rouge_l'],
                    use_rag=False,
                    top_k=None,
                    temperature=model.config.temperature,
                    max_tokens=model.config.max_tokens,
                    response_time=end_time - start_time,
                    tokens_used=None,
                    cost_estimate=None,
                    retrieval_docs=None,
                    error_message=None,
                    success=True
                )
                
                self.logger.log_result(result)
                results.append(result)
                
            except Exception as e:
                print(f"âŒ Error processing question {i+1}: {e}")
                
                error_result = ExperimentResult(
                    experiment_id=experiment_id,
                    timestamp=datetime.now().isoformat(),
                    model_name=model_name,
                    question=question,
                    predicted_answer="",
                    ground_truth=gt,
                    exact_match=0.0,
                    f1_score=0.0,
                    bert_score=0.0,
                    rouge_l=0.0,
                    use_rag=False,
                    top_k=None,
                    temperature=model.config.temperature,
                    max_tokens=model.config.max_tokens,
                    response_time=0.0,
                    tokens_used=None,
                    cost_estimate=None,
                    retrieval_docs=None,
                    error_message=str(e),
                    success=False
                )
                
                self.logger.log_result(error_result)
        
        print(f"âœ… Baseline experiment completed: {len(results)} successful predictions")
        return results
    
    def run_rag_experiment(self, experiment_id: str, questions: List[str], ground_truths: List[str]):
        """RAG ì‹¤í—˜"""
        if not self.rag_pipeline:
            print("âŒ RAG pipeline not initialized")
            return
        
        print(f"\nğŸ” Running RAG Experiment")
        
        for top_k in self.config['top_k_values']:
            print(f"\nğŸ“Š Testing Top-K = {top_k}")
            
            for i, (question, gt) in enumerate(zip(questions, ground_truths)):
                print(f"Processing {i+1}/{len(questions)}: {question[:50]}...")
                
                try:
                    start_time = time.time()
                    rag_result = self.rag_pipeline.process_query(question, top_k_chroma=top_k)
                    end_time = time.time()
                    
                    prediction = rag_result['final_answer']
                    
                    # í‰ê°€
                    metrics = self.evaluator.evaluate_prediction(prediction, gt)
                    
                    result = ExperimentResult(
                        experiment_id=experiment_id,
                        timestamp=datetime.now().isoformat(),
                        model_name="rag_pipeline",
                        question=question,
                        predicted_answer=prediction,
                        ground_truth=gt,
                        exact_match=metrics['exact_match'],
                        f1_score=metrics['f1_score'],
                        bert_score=metrics['bert_score'],
                        rouge_l=metrics['rouge_l'],
                        use_rag=True,
                        top_k=top_k,
                        temperature=0.1,
                        max_tokens=2000,
                        response_time=end_time - start_time,
                        tokens_used=None,
                        cost_estimate=None,
                        retrieval_docs=None,
                        error_message=None,
                        success=True
                    )
                    
                    self.logger.log_result(result)
                    
                except Exception as e:
                    print(f"âŒ Error processing question {i+1}: {e}")
                    
                    error_result = ExperimentResult(
                        experiment_id=experiment_id,
                        timestamp=datetime.now().isoformat(),
                        model_name="rag_pipeline",
                        question=question,
                        predicted_answer="",
                        ground_truth=gt,
                        exact_match=0.0,
                        f1_score=0.0,
                        bert_score=0.0,
                        rouge_l=0.0,
                        use_rag=True,
                        top_k=top_k,
                        temperature=0.1,
                        max_tokens=2000,
                        response_time=0.0,
                        tokens_used=None,
                        cost_estimate=None,
                        retrieval_docs=None,
                        error_message=str(e),
                        success=False
                    )
                    
                    self.logger.log_result(error_result)
    
    def run_full_benchmark(self):
        """ì „ì²´ ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰"""
        experiment_id = f"benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        print(f"ğŸ¯ Starting Full Benchmark: {experiment_id}")
        
        # ë°ì´í„°ì…‹ ë¡œë“œ
        df = self.load_dataset()
        questions = df['question'].tolist()
        ground_truths = df['ground_truth'].tolist()
        
        print(f"ğŸ“Š Dataset loaded: {len(questions)} questions")
        
        # ì‹¤í—˜ ë©”íƒ€ë°ì´í„° ë¡œê¹…
        self.logger.log_experiment_metadata(
            experiment_id=experiment_id,
            description="Full benchmark comparison of LLM models",
            config_file=self.config_file,
            dataset_path=self.config['dataset_path'],
            total_questions=len(questions)
        )
        
        # Baseline ì‹¤í—˜ë“¤
        if self.config.get('run_baseline', True):
            for model_name in self.config['models_to_test']:
                if model_name in self.llm_manager.list_available_models():
                    self.run_baseline_experiment(experiment_id, model_name, questions, ground_truths)
                else:
                    print(f"âš ï¸ Model {model_name} not available for baseline")
        
        # RAG ì‹¤í—˜
        if self.config.get('use_rag', False):
            self.run_rag_experiment(experiment_id, questions, ground_truths)
        
        # ì‹¤í—˜ ì™„ë£Œ
        self.logger.update_experiment_status(experiment_id, "completed")
        
        # ê²°ê³¼ ë¶„ì„ ë° ì‹œê°í™”
        print("\nğŸ“ˆ Generating analysis and visualizations...")
        self.visualizer.plot_model_comparison(experiment_id)
        self.visualizer.plot_rag_impact()
        self.visualizer.plot_response_time_analysis()
        if self.config.get('use_rag', False):
            self.visualizer.plot_top_k_performance()
        
        # ë¦¬í¬íŠ¸ ìƒì„±
        report_path = self.visualizer.generate_experiment_report(experiment_id)
        
        print(f"\nğŸ‰ Benchmark completed!")
        print(f"ğŸ“‹ Report: {report_path}")
        print(f"ğŸ“Š Results: {self.logger.db_path}")
        
        return experiment_id

def main():
    parser = argparse.ArgumentParser(description='LLM Network Benchmark Runner')
    parser.add_argument('--config', default='benchmark_config.json', help='Config file path')
    parser.add_argument('--baseline-only', action='store_true', help='Run only baseline experiments')
    parser.add_argument('--rag-only', action='store_true', help='Run only RAG experiments')
    parser.add_argument('--models', nargs='+', help='Specific models to test')
    parser.add_argument('--max-questions', type=int, help='Maximum number of questions to test')
    
    args = parser.parse_args()
    
    # ë²¤ì¹˜ë§ˆí¬ ëŸ¬ë„ˆ ì´ˆê¸°í™”
    runner = BenchmarkRunner(args.config)
    
    # ì„¤ì • ì˜¤ë²„ë¼ì´ë“œ
    if args.baseline_only:
        runner.config['use_rag'] = False
    elif args.rag_only:
        runner.config['run_baseline'] = False
    
    if args.models:
        runner.config['models_to_test'] = args.models
    
    if args.max_questions:
        runner.config['max_questions'] = args.max_questions
    
    # ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
    experiment_id = runner.run_full_benchmark()
    
    # ê²°ê³¼ ìš”ì•½
    summary = runner.logger.get_experiment_summary()
    print("\nğŸ“Š Final Results Summary:")
    print(summary.to_string(index=False))

if __name__ == "__main__":
    main()
