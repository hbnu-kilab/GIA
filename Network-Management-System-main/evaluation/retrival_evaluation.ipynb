{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6410646",
   "metadata": {},
   "source": [
    "# Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f62d4857",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62196cb28dca4d1a80afb0d1be0c8ea0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Loaded existing collection: network_devices\n",
      "[INFO] Total documents in collection: 227\n",
      "=== Heuristic Query ===\n",
      "Recall: {1: 0.21945701357466063, 5: 0.36425339366515835, 10: 0.43891402714932126, 20: 0.5, 50: 0.5361990950226244}\n",
      "MRR: 0.2813999906984041\n",
      "=== LLM Query ===\n",
      "Recall: {1: 0.2171945701357466, 5: 0.3393665158371041, 10: 0.4343891402714932, 20: 0.5090497737556561, 50: 0.5294117647058824}\n",
      "MRR: 0.27187639795697005\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "\n",
    "chatgpt_system_prompt = \"\"\"You are an expert network engineering assistant with deep knowledge of \n",
    "network configurations, troubleshooting, and security best practices. You have access to various \n",
    "network device configurations, XML schemas, and technical documentation.\"\"\"\n",
    "\n",
    "openai_client = OpenAI()  # 환경변수 OPENAI_API_KEY 필요\n",
    "\n",
    "\n",
    "class HuggingFaceEmbedder:\n",
    "    def __init__(self, model_name, device, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedder = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": device},\n",
    "            encode_kwargs={\"batch_size\": batch_size}\n",
    "        )\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def embed(self, texts: list[str] | str) -> list[list[float]]:\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        return self.embedder.embed_documents(texts)\n",
    "\n",
    "class ChromaDB:\n",
    "    \"\"\"사전 임베딩된 XML 파일들을 위한 ChromaDB 인터페이스\"\"\"\n",
    "    def __init__(self, db_path: str, collection_name: str, embedder: object):\n",
    "        self.db_path = db_path\n",
    "        self.client = chromadb.PersistentClient(path=db_path)\n",
    "        self.embedder = embedder\n",
    "        try:\n",
    "            # 기존 컬렉션 로드 (사전 임베딩된 XML 데이터)\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "            print(f\"[INFO] Loaded existing collection: {collection_name}\")\n",
    "            print(f\"[INFO] Total documents in collection: {self.collection.count()}\")\n",
    "        except:\n",
    "            # 컬렉션이 없으면 새로 생성\n",
    "            self.collection = self.client.create_collection(name=collection_name)\n",
    "            print(f\"[INFO] Created new collection: {collection_name}\")\n",
    "\n",
    "    def add_docs(self, ids: list[str], docs: list[str], metadatas: list[dict] | None = None):\n",
    "        \"\"\"새 문서 추가 (필요시)\"\"\"\n",
    "        embeddings = self.embedder.embed(docs)\n",
    "        self.collection.add(ids=ids, documents=docs, embeddings=embeddings, metadatas=metadatas)\n",
    "\n",
    "    def query(self, text: str, n_results: int = 5) -> Dict:\n",
    "        \"\"\"벡터 유사도 검색\"\"\"\n",
    "        q_emb = self.embedder.embed(text)\n",
    "        return self.collection.query(query_embeddings=q_emb, n_results=n_results)\n",
    "\n",
    "\n",
    "def get_xml_query(question: str, answer: str) -> str:\n",
    "    \"\"\"ChromaDB의 XML 파일 검색을 위한 쿼리 생성\"\"\"\n",
    "    query_prompt = '''\n",
    "        You are responsible for generating search queries in ChromaDB to find relevant XML network configuration files and related documents.\n",
    "\n",
    "        CONTEXT:\n",
    "            User Question: {question}\n",
    "            Search History (information found or concluded so far): {answer}\n",
    "\n",
    "        OBJECTIVE:\n",
    "            Generate more effective search queries that expand beyond what has already been attempted or concluded in the history.\n",
    "\n",
    "        OUTPUT INSTRUCTIONS:\n",
    "            - Output ONLY the search query string.\n",
    "            - Do not include explanations, reasoning, or extra text.\n",
    "        '''\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": chatgpt_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nContent: {answer}\\n\\nInstruction: {query_prompt}\"}\n",
    "        ],\n",
    "        temperature=0.05\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def recall_at_k(results: list[str], ground_truth: str, k: int = 5) -> float:\n",
    "    \"\"\"정답 텍스트가 top-k 문서 안에 포함되면 1, 아니면 0\"\"\"\n",
    "    return 1.0 if any(ground_truth in doc for doc in results[:k]) else 0.0\n",
    "\n",
    "def reciprocal_rank(results: list[str], ground_truth: str) -> float:\n",
    "    \"\"\"정답이 몇 번째 문서에서 처음 등장하는지 Reciprocal Rank 계산\"\"\"\n",
    "    for idx, doc in enumerate(results, start=1):\n",
    "        if ground_truth in doc:\n",
    "            return 1.0 / idx\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_retrieval(db, dataset: pd.DataFrame, query_fn, k_values=[1,5,10,20,50]):\n",
    "    \"\"\"approximate retrieval 평가 (GT 텍스트 기반)\"\"\"\n",
    "    recall_scores = {k: [] for k in k_values}\n",
    "    rr_scores = []\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        q, gt = row[\"question\"], row[\"ground_truth\"]\n",
    "\n",
    "        # 쿼리 생성\n",
    "        query = query_fn(q, row.get(\"answer\",\"\"))\n",
    "        \n",
    "        # 검색 실행\n",
    "        results = db.query(query, n_results=max(k_values))\n",
    "        docs = results[\"documents\"][0] if results else []\n",
    "        \n",
    "        # Recall@k 계산\n",
    "        for k in k_values:\n",
    "            recall_scores[k].append(recall_at_k(docs, gt, k))\n",
    "        \n",
    "        # MRR 계산\n",
    "        rr_scores.append(reciprocal_rank(docs, gt))\n",
    "    \n",
    "    # 평균 결과 리턴\n",
    "    recall_avg = {k: np.mean(v) for k, v in recall_scores.items()}\n",
    "    mrr = np.mean(rr_scores)\n",
    "    return recall_avg, mrr\n",
    "\n",
    "embedder = HuggingFaceEmbedder(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-8B\",\n",
    "    device=\"cuda:1\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "CHROMADB_PATH = \"/workspace/jke/chromadb_qwen\"  # 사전 임베딩된 XML 파일들이 저장된 경로\n",
    "COLLECTION_NAME = \"network_devices\"  # XML 설정 파일 컬렉션\n",
    "\n",
    "# ChromaDB 초기화 (db_path는 chroma persist 디렉토리 경로, collection_name은 기존에 생성한 이름)\n",
    "chroma_db = ChromaDB(\n",
    "    db_path=CHROMADB_PATH,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedder=embedder\n",
    ")\n",
    "\n",
    "dataset = pd.read_csv(\"/workspace/jke/evaluation/test.csv\")\n",
    "\n",
    "# === 사용 예시 ===\n",
    "# 휴리스틱 방식\n",
    "recall_h, mrr_h = evaluate_retrieval(\n",
    "    chroma_db, dataset,\n",
    "    query_fn=lambda q, a: f\"단순 조회, {q}\"\n",
    ")\n",
    "\n",
    "# LLM 방식\n",
    "recall_llm, mrr_llm = evaluate_retrieval(\n",
    "    chroma_db, dataset,\n",
    "    query_fn=lambda q, a: get_xml_query(q, \"\")\n",
    ")\n",
    "\n",
    "print(\"=== Heuristic Query ===\")\n",
    "print(\"Recall:\", recall_h)\n",
    "print(\"MRR:\", mrr_h)\n",
    "\n",
    "print(\"=== LLM Query ===\")\n",
    "print(\"Recall:\", recall_llm)\n",
    "print(\"MRR:\", mrr_llm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63129fda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "738d972a29a64dea97c39cf2e2b526e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15b64289bcdc42df863a8675205287f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9b2a7dace4b48fa95524bf705fefeb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6b5e6acc1864e3ba9f9aceaafad7211",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2907eb7699e8421886e958c498679656",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748c82ab81e242d6970e26c9424fa061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8696c1e51ccd40608fe66d4d81b539da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202be7bc632c47e9a63af03b9816663e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] ReRanker initialized: cross-encoder/ms-marco-MiniLM-L-6-v2 on cuda:1\n",
      "[INFO] Loaded existing collection: network_devices\n",
      "[INFO] Total documents in collection: 227\n",
      "[INFO] Loaded existing collection: network_devices\n",
      "[INFO] Total documents in collection: 227\n",
      "=== 평가 시작 ===\n",
      "\n",
      "1. Heuristic Query (No ReRanking)\n",
      "Recall: {1: 0.21945701357466063, 5: 0.36425339366515835, 10: 0.43891402714932126, 20: 0.5, 50: 0.5361990950226244}\n",
      "MRR: 0.2813999906984041\n",
      "\n",
      "2. LLM Query (No ReRanking)\n",
      "Recall: {1: 0.2239819004524887, 5: 0.3438914027149321, 10: 0.4411764705882353, 20: 0.5067873303167421, 50: 0.5294117647058824}\n",
      "MRR: 0.2771820717142009\n",
      "\n",
      "3. Heuristic Query + BERT ReRanking\n",
      "Recall: {1: 0.21040723981900453, 5: 0.2647058823529412, 10: 0.3393665158371041, 20: 0.4230769230769231, 50: 0.48868778280542985}\n",
      "MRR: 0.24756165383196818\n",
      "\n",
      "4. LLM Query + BERT ReRanking\n",
      "Recall: {1: 0.2149321266968326, 5: 0.2647058823529412, 10: 0.416289592760181, 20: 0.4638009049773756, 50: 0.5452488687782805}\n",
      "MRR: 0.2568898323936854\n",
      "\n",
      "============================================================\n",
      "성능 비교 요약\n",
      "============================================================\n",
      "\n",
      "Heuristic (No ReRank):\n",
      "  Recall@1: 0.2195\n",
      "  Recall@5: 0.3643\n",
      "  Recall@10: 0.4389\n",
      "  MRR: 0.2814\n",
      "\n",
      "LLM (No ReRank):\n",
      "  Recall@1: 0.2240\n",
      "  Recall@5: 0.3439\n",
      "  Recall@10: 0.4412\n",
      "  MRR: 0.2772\n",
      "\n",
      "Heuristic + BERT ReRank:\n",
      "  Recall@1: 0.2104\n",
      "  Recall@5: 0.2647\n",
      "  Recall@10: 0.3394\n",
      "  MRR: 0.2476\n",
      "\n",
      "LLM + BERT ReRank:\n",
      "  Recall@1: 0.2149\n",
      "  Recall@5: 0.2647\n",
      "  Recall@10: 0.4163\n",
      "  MRR: 0.2569\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openai import OpenAI\n",
    "import chromadb\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from sentence_transformers import CrossEncoder\n",
    "import torch\n",
    "\n",
    "\n",
    "chatgpt_system_prompt = \"\"\"You are an expert network engineering assistant with deep knowledge of \n",
    "network configurations, troubleshooting, and security best practices. You have access to various \n",
    "network device configurations, XML schemas, and technical documentation.\"\"\"\n",
    "\n",
    "openai_client = OpenAI()  # 환경변수 OPENAI_API_KEY 필요\n",
    "\n",
    "\n",
    "class HuggingFaceEmbedder:\n",
    "    def __init__(self, model_name, device, batch_size):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.embedder = HuggingFaceEmbeddings(\n",
    "            model_name=model_name,\n",
    "            model_kwargs={\"device\": device},\n",
    "            encode_kwargs={\"batch_size\": batch_size}\n",
    "        )\n",
    "\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def embed(self, texts: list[str] | str) -> list[list[float]]:\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        return self.embedder.embed_documents(texts)\n",
    "\n",
    "\n",
    "class BERTReRanker:\n",
    "    \"\"\"BERT 기반 Cross-Encoder ReRanker\"\"\"\n",
    "    def __init__(self, model_name: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\", device: str = \"cuda\"):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            model_name: Cross-encoder 모델명 (기본값: ms-marco-MiniLM-L-6-v2)\n",
    "            device: 디바이스 설정\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.model = CrossEncoder(model_name, device=device)\n",
    "        print(f\"[INFO] ReRanker initialized: {model_name} on {device}\")\n",
    "    \n",
    "    def rerank(self, query: str, documents: List[str], top_k: int = None) -> List[Tuple[str, float]]:\n",
    "        \"\"\"\n",
    "        문서들을 쿼리와의 관련성 점수로 재순위화\n",
    "        \n",
    "        Args:\n",
    "            query: 검색 쿼리\n",
    "            documents: 재순위화할 문서 리스트\n",
    "            top_k: 상위 k개 문서만 반환 (None이면 모든 문서 반환)\n",
    "            \n",
    "        Returns:\n",
    "            (문서, 점수) 튜플의 리스트, 점수 내림차순 정렬\n",
    "        \"\"\"\n",
    "        if not documents:\n",
    "            return []\n",
    "            \n",
    "        # 쿼리-문서 쌍 생성\n",
    "        pairs = [[query, doc] for doc in documents]\n",
    "        \n",
    "        # Cross-encoder로 관련성 점수 계산\n",
    "        scores = self.model.predict(pairs)\n",
    "        \n",
    "        # 문서와 점수를 함께 묶고 점수 내림차순 정렬\n",
    "        doc_scores = list(zip(documents, scores))\n",
    "        doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # top_k 개수만큼만 반환\n",
    "        if top_k:\n",
    "            doc_scores = doc_scores[:top_k]\n",
    "            \n",
    "        return doc_scores\n",
    "\n",
    "\n",
    "class ChromaDB:\n",
    "    \"\"\"사전 임베딩된 XML 파일들을 위한 ChromaDB 인터페이스 (ReRanking 포함)\"\"\"\n",
    "    def __init__(self, db_path: str, collection_name: str, embedder: object, use_reranking: bool = True, reranker_model: str = \"cross-encoder/ms-marco-MiniLM-L-6-v2\"):\n",
    "        self.db_path = db_path\n",
    "        self.client = chromadb.PersistentClient(path=db_path)\n",
    "        self.embedder = embedder\n",
    "        self.use_reranking = use_reranking\n",
    "        \n",
    "        # ReRanker 초기화\n",
    "        if self.use_reranking:\n",
    "            device = getattr(embedder, 'device', 'cuda')\n",
    "            self.reranker = BERTReRanker(model_name=reranker_model, device=device)\n",
    "        \n",
    "        try:\n",
    "            # 기존 컬렉션 로드 (사전 임베딩된 XML 데이터)\n",
    "            self.collection = self.client.get_collection(name=collection_name)\n",
    "            print(f\"[INFO] Loaded existing collection: {collection_name}\")\n",
    "            print(f\"[INFO] Total documents in collection: {self.collection.count()}\")\n",
    "        except:\n",
    "            # 컬렉션이 없으면 새로 생성\n",
    "            self.collection = self.client.create_collection(name=collection_name)\n",
    "            print(f\"[INFO] Created new collection: {collection_name}\")\n",
    "\n",
    "    def add_docs(self, ids: list[str], docs: list[str], metadatas: list[dict] | None = None):\n",
    "        \"\"\"새 문서 추가 (필요시)\"\"\"\n",
    "        embeddings = self.embedder.embed(docs)\n",
    "        self.collection.add(ids=ids, documents=docs, embeddings=embeddings, metadatas=metadatas)\n",
    "\n",
    "    def query(self, text: str, n_results: int = 5, initial_retrieval_multiplier: int = 3) -> Dict:\n",
    "        \"\"\"벡터 유사도 검색 + BERT ReRanking\"\"\"\n",
    "        \n",
    "        if self.use_reranking:\n",
    "            # ReRanking을 사용하는 경우: 더 많은 문서를 초기 검색\n",
    "            initial_n_results = min(n_results * initial_retrieval_multiplier, self.collection.count())\n",
    "        else:\n",
    "            initial_n_results = n_results\n",
    "        \n",
    "        # 1단계: 벡터 유사도 검색\n",
    "        q_emb = self.embedder.embed(text)\n",
    "        initial_results = self.collection.query(query_embeddings=q_emb, n_results=initial_n_results)\n",
    "        \n",
    "        if not self.use_reranking:\n",
    "            return initial_results\n",
    "            \n",
    "        # 2단계: BERT ReRanking\n",
    "        if not initial_results['documents'] or not initial_results['documents'][0]:\n",
    "            return initial_results\n",
    "            \n",
    "        documents = initial_results['documents'][0]\n",
    "        ids = initial_results['ids'][0]\n",
    "        metadatas = initial_results.get('metadatas', [[]])[0] if initial_results.get('metadatas') else [{}] * len(documents)\n",
    "        distances = initial_results.get('distances', [[]])[0] if initial_results.get('distances') else [0.0] * len(documents)\n",
    "        \n",
    "        # ReRanking 수행\n",
    "        reranked_results = self.reranker.rerank(text, documents, top_k=n_results)\n",
    "        \n",
    "        # 결과 재구성\n",
    "        reranked_documents = []\n",
    "        reranked_ids = []\n",
    "        reranked_metadatas = []\n",
    "        reranked_distances = []  # 이제 ReRanker 점수를 거리로 변환 (1 - score)\n",
    "        \n",
    "        for doc, score in reranked_results:\n",
    "            doc_idx = documents.index(doc)\n",
    "            reranked_documents.append(doc)\n",
    "            reranked_ids.append(ids[doc_idx])\n",
    "            reranked_metadatas.append(metadatas[doc_idx])\n",
    "            reranked_distances.append(1.0 - score)  # 점수를 거리로 변환 (높은 점수 = 낮은 거리)\n",
    "        \n",
    "        return {\n",
    "            'documents': [reranked_documents],\n",
    "            'ids': [reranked_ids], \n",
    "            'metadatas': [reranked_metadatas] if any(reranked_metadatas) else None,\n",
    "            'distances': [reranked_distances]\n",
    "        }\n",
    "\n",
    "\n",
    "def get_xml_query(question: str, answer: str) -> str:\n",
    "    \"\"\"ChromaDB의 XML 파일 검색을 위한 쿼리 생성\"\"\"\n",
    "    query_prompt = '''\n",
    "        You are responsible for generating search queries in ChromaDB to find relevant XML network configuration files and related documents.\n",
    "\n",
    "        CONTEXT:\n",
    "            User Question: {question}\n",
    "            Search History (information found or concluded so far): {answer}\n",
    "\n",
    "        OBJECTIVE:\n",
    "            Generate more effective search queries that expand beyond what has already been attempted or concluded in the history.\n",
    "\n",
    "        OUTPUT INSTRUCTIONS:\n",
    "            - Output ONLY the search query string.\n",
    "            - Do not include explanations, reasoning, or extra text.\n",
    "        '''\n",
    "    \n",
    "    response = openai_client.chat.completions.create(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": chatgpt_system_prompt},\n",
    "            {\"role\": \"user\", \"content\": f\"Question: {question}\\n\\nContent: {answer}\\n\\nInstruction: {query_prompt}\"}\n",
    "        ],\n",
    "        temperature=0.05\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def recall_at_k(results: list[str], ground_truth: str, k: int = 5) -> float:\n",
    "    \"\"\"정답 텍스트가 top-k 문서 안에 포함되면 1, 아니면 0\"\"\"\n",
    "    return 1.0 if any(ground_truth in doc for doc in results[:k]) else 0.0\n",
    "\n",
    "def reciprocal_rank(results: list[str], ground_truth: str) -> float:\n",
    "    \"\"\"정답이 몇 번째 문서에서 처음 등장하는지 Reciprocal Rank 계산\"\"\"\n",
    "    for idx, doc in enumerate(results, start=1):\n",
    "        if ground_truth in doc:\n",
    "            return 1.0 / idx\n",
    "    return 0.0\n",
    "\n",
    "def evaluate_retrieval(db, dataset: pd.DataFrame, query_fn, k_values=[1,5,10,20,50]):\n",
    "    \"\"\"approximate retrieval 평가 (GT 텍스트 기반)\"\"\"\n",
    "    recall_scores = {k: [] for k in k_values}\n",
    "    rr_scores = []\n",
    "\n",
    "    for _, row in dataset.iterrows():\n",
    "        q, gt = row[\"question\"], row[\"ground_truth\"]\n",
    "\n",
    "        # 쿼리 생성\n",
    "        query = query_fn(q, row.get(\"answer\",\"\"))\n",
    "        \n",
    "        # 검색 실행\n",
    "        results = db.query(query, n_results=max(k_values))\n",
    "        docs = results[\"documents\"][0] if results else []\n",
    "        \n",
    "        # Recall@k 계산\n",
    "        for k in k_values:\n",
    "            recall_scores[k].append(recall_at_k(docs, gt, k))\n",
    "        \n",
    "        # MRR 계산\n",
    "        rr_scores.append(reciprocal_rank(docs, gt))\n",
    "    \n",
    "    # 평균 결과 리턴\n",
    "    recall_avg = {k: np.mean(v) for k, v in recall_scores.items()}\n",
    "    mrr = np.mean(rr_scores)\n",
    "    return recall_avg, mrr\n",
    "\n",
    "# 임베딩 모델 초기화\n",
    "embedder = HuggingFaceEmbedder(\n",
    "    model_name=\"Qwen/Qwen3-Embedding-8B\",\n",
    "    device=\"cuda:1\",\n",
    "    batch_size=32\n",
    ")\n",
    "\n",
    "CHROMADB_PATH = \"/workspace/jke/chromadb_qwen\"  # 사전 임베딩된 XML 파일들이 저장된 경로\n",
    "COLLECTION_NAME = \"network_devices\"  # XML 설정 파일 컬렉션\n",
    "\n",
    "# ChromaDB 초기화 (ReRanking 사용)\n",
    "chroma_db_with_rerank = ChromaDB(\n",
    "    db_path=CHROMADB_PATH,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedder=embedder,\n",
    "    use_reranking=True,\n",
    "    reranker_model=\"cross-encoder/ms-marco-MiniLM-L-6-v2\"  # BERT 기반 cross-encoder\n",
    ")\n",
    "\n",
    "# ChromaDB 초기화 (ReRanking 미사용 - 비교용)\n",
    "chroma_db_no_rerank = ChromaDB(\n",
    "    db_path=CHROMADB_PATH,\n",
    "    collection_name=COLLECTION_NAME,\n",
    "    embedder=embedder,\n",
    "    use_reranking=False\n",
    ")\n",
    "\n",
    "dataset = pd.read_csv(\"/workspace/jke/evaluation/test.csv\")\n",
    "\n",
    "# === 평가 실행 ===\n",
    "print(\"=== 평가 시작 ===\")\n",
    "\n",
    "# 1. 기본 휴리스틱 방식 (ReRanking 없음)\n",
    "print(\"\\n1. Heuristic Query (No ReRanking)\")\n",
    "recall_h, mrr_h = evaluate_retrieval(\n",
    "    chroma_db_no_rerank, dataset,\n",
    "    query_fn=lambda q, a: f\"단순 조회, {q}\"\n",
    ")\n",
    "print(\"Recall:\", recall_h)\n",
    "print(\"MRR:\", mrr_h)\n",
    "\n",
    "# 2. LLM 방식 (ReRanking 없음)\n",
    "print(\"\\n2. LLM Query (No ReRanking)\")\n",
    "recall_llm, mrr_llm = evaluate_retrieval(\n",
    "    chroma_db_no_rerank, dataset,\n",
    "    query_fn=lambda q, a: get_xml_query(q, \"\")\n",
    ")\n",
    "print(\"Recall:\", recall_llm)\n",
    "print(\"MRR:\", mrr_llm)\n",
    "\n",
    "# 3. 휴리스틱 방식 + BERT ReRanking\n",
    "print(\"\\n3. Heuristic Query + BERT ReRanking\")\n",
    "recall_h_rerank, mrr_h_rerank = evaluate_retrieval(\n",
    "    chroma_db_with_rerank, dataset,\n",
    "    query_fn=lambda q, a: f\"단순 조회, {q}\"\n",
    ")\n",
    "print(\"Recall:\", recall_h_rerank)\n",
    "print(\"MRR:\", mrr_h_rerank)\n",
    "\n",
    "# 4. LLM 방식 + BERT ReRanking\n",
    "print(\"\\n4. LLM Query + BERT ReRanking\")\n",
    "recall_llm_rerank, mrr_llm_rerank = evaluate_retrieval(\n",
    "    chroma_db_with_rerank, dataset,\n",
    "    query_fn=lambda q, a: get_xml_query(q, \"\")\n",
    ")\n",
    "print(\"Recall:\", recall_llm_rerank)\n",
    "print(\"MRR:\", mrr_llm_rerank)\n",
    "\n",
    "# === 성능 비교 출력 ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"성능 비교 요약\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "methods = [\n",
    "    (\"Heuristic (No ReRank)\", recall_h, mrr_h),\n",
    "    (\"LLM (No ReRank)\", recall_llm, mrr_llm), \n",
    "    (\"Heuristic + BERT ReRank\", recall_h_rerank, mrr_h_rerank),\n",
    "    (\"LLM + BERT ReRank\", recall_llm_rerank, mrr_llm_rerank)\n",
    "]\n",
    "\n",
    "for method_name, recall_scores, mrr_score in methods:\n",
    "    print(f\"\\n{method_name}:\")\n",
    "    print(f\"  Recall@1: {recall_scores[1]:.4f}\")\n",
    "    print(f\"  Recall@5: {recall_scores[5]:.4f}\")\n",
    "    print(f\"  Recall@10: {recall_scores[10]:.4f}\")\n",
    "    print(f\"  MRR: {mrr_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0e9133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jke",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
