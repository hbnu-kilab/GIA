# ğŸ§  LLM ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ RAG ê°œì„  ë°©ì•ˆ

## ğŸ’¡ í˜„ì¬ RAG ë°©ì‹ì˜ í•œê³„ì 

### ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ ë°©ì‹
```python
# í˜„ì¬ pipeline_3_advanced.pyì˜ ë°©ì‹
def retrieve_documents(question: str, top_k: int = 5):
    # 1. ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ì„ë² ë”©
    query_embedding = embedding_model.embed(question)
    
    # 2. ChromaDBì—ì„œ ìœ ì‚¬ë„ ê²€ìƒ‰
    results = chromadb.query(
        query_embeddings=[query_embedding],
        n_results=top_k
    )
    
    # 3. ìƒìœ„ Kê°œ ë¬¸ì„œ ë°˜í™˜
    return results['documents']
```

### í•œê³„ì  ë¶„ì„
1. **ì˜ë¯¸ì  ìœ ì‚¬ë„ì˜ í•œê³„**: ë²¡í„° ìœ ì‚¬ë„ â‰  ì‹¤ì œ ë‹µë³€ ìœ ìš©ì„±
2. **ì»¨í…ìŠ¤íŠ¸ ë¬´ì‹œ**: ì§ˆë¬¸ì˜ ë§¥ë½ì„ ê³ ë ¤í•˜ì§€ ì•ŠìŒ
3. **ê³ ì •ì  ì„ íƒ**: í•­ìƒ ê°™ì€ ê¸°ì¤€ìœ¼ë¡œ ë¬¸ì„œ ì„ íƒ

## ğŸš€ LLM ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ ë°©ì‹

### í•µì‹¬ ì•„ì´ë””ì–´
"LLMì´ ì§ì ‘ ë¬¸ì„œ ëª©ë¡ì„ ë³´ê³  ì§ˆë¬¸ì— ê°€ì¥ ìœ ìš©í•œ ë¬¸ì„œë“¤ì„ ì„ íƒí•˜ê²Œ í•˜ì"

### êµ¬í˜„ ë°©ë²• 1: ë¬¸ì„œ ìš”ì•½ ê¸°ë°˜ ì„ íƒ
```python
def llm_based_document_selection(question: str, top_k: int = 10):
    # 1. ëª¨ë“  ë¬¸ì„œì˜ ìš”ì•½ ì •ë³´ ì¤€ë¹„
    doc_summaries = get_all_document_summaries()
    
    # 2. LLMì—ê²Œ ë¬¸ì„œ ì„ íƒ ìš”ì²­
    selection_prompt = f"""
    ë„¤íŠ¸ì›Œí¬ ì—”ì§€ë‹ˆì–´ë§ ì§ˆë¬¸ì— ë‹µí•˜ê¸° ìœ„í•´ ê°€ì¥ ìœ ìš©í•œ ë¬¸ì„œë“¤ì„ ì„ íƒí•´ì£¼ì„¸ìš”.

    ì§ˆë¬¸: {question}

    ì‚¬ìš© ê°€ëŠ¥í•œ ë¬¸ì„œë“¤:
    {format_document_list(doc_summaries)}

    ìœ„ ë¬¸ì„œ ì¤‘ì—ì„œ ì´ ì§ˆë¬¸ì— ë‹µí•˜ëŠ” ë° ê°€ì¥ ìœ ìš©í•œ ìƒìœ„ {top_k}ê°œ ë¬¸ì„œì˜ 
    ë²ˆí˜¸ë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”. ë²ˆí˜¸ë§Œ ì‰¼í‘œë¡œ êµ¬ë¶„í•´ì„œ ë‹µí•˜ì„¸ìš”.

    ì˜ˆ: 1,5,12,23,45,67,89,101,134,156
    """
    
    # 3. LLM ì‘ë‹µìœ¼ë¡œ ë¬¸ì„œ ì„ íƒ
    response = llm.generate(selection_prompt)
    selected_ids = parse_selected_ids(response)
    
    # 4. ì„ íƒëœ ë¬¸ì„œë“¤ì˜ ì „ì²´ ë‚´ìš© ë¡œë“œ
    selected_documents = load_documents(selected_ids)
    
    return selected_documents

def get_all_document_summaries():
    """ëª¨ë“  ë¬¸ì„œì˜ ìš”ì•½ ì •ë³´ ìƒì„±"""
    summaries = []
    for i, doc_path in enumerate(all_document_paths):
        summary = {
            'id': i+1,
            'filename': os.path.basename(doc_path),
            'device': extract_device_name(doc_path),
            'section': extract_section_type(doc_path),
            'preview': get_first_lines(doc_path, 3)
        }
        summaries.append(summary)
    return summaries

def format_document_list(summaries):
    """ë¬¸ì„œ ëª©ë¡ì„ LLMì´ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ í¬ë§·"""
    formatted = []
    for summary in summaries:
        line = f"[{summary['id']}] {summary['filename']} - " \
               f"ì¥ë¹„:{summary['device']}, ì„¹ì…˜:{summary['section']}"
        formatted.append(line)
    return "\n".join(formatted)
```

### êµ¬í˜„ ë°©ë²• 2: í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼
```python
def hybrid_document_selection(question: str, top_k: int = 10):
    # 1ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ í›„ë³´êµ° ì¶•ì†Œ (20ê°œ)
    vector_candidates = vector_search(question, top_k=20)
    
    # 2ë‹¨ê³„: LLMì´ í›„ë³´êµ°ì—ì„œ ìµœì¢… ì„ íƒ (10ê°œ)
    final_selection = llm_select_from_candidates(
        question, vector_candidates, final_k=top_k
    )
    
    return final_selection

def llm_select_from_candidates(question, candidates, final_k):
    """LLMì´ í›„ë³´ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ìµœì¢… ì„ íƒ"""
    candidate_info = []
    for i, doc in enumerate(candidates):
        candidate_info.append(f"[{i+1}] {doc['metadata']['filename']}")
    
    prompt = f"""
    ë‹¤ìŒ í›„ë³´ ë¬¸ì„œë“¤ ì¤‘ì—ì„œ ì§ˆë¬¸ '{question}'ì— ë‹µí•˜ëŠ” ë° 
    ê°€ì¥ ìœ ìš©í•œ {final_k}ê°œë¥¼ ì„ íƒí•´ì£¼ì„¸ìš”:
    
    {chr(10).join(candidate_info)}
    
    ì„ íƒëœ ë¬¸ì„œ ë²ˆí˜¸: 
    """
    
    response = llm.generate(prompt)
    selected_indices = parse_indices(response)
    
    return [candidates[i-1] for i in selected_indices if i <= len(candidates)]
```

## ğŸ“Š êµ¬ì²´ì  êµ¬í˜„ ê³„íš

### Step 1: ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¤€ë¹„
```python
# docs7_export í´ë”ì˜ ëª¨ë“  ë¬¸ì„œ ë¶„ì„
def analyze_document_structure():
    """
    ê¸°ì¡´ ë¬¸ì„œë“¤ì˜ êµ¬ì¡° ë¶„ì„:
    - 01_sample10_device.txt â†’ ì¥ë¹„: sample10, ì„¹ì…˜: device
    - 38_sample8_bgp_neighbor.txt â†’ ì¥ë¹„: sample8, ì„¹ì…˜: bgp_neighbor
    """
    doc_metadata = {}
    for filename in os.listdir("docs7_export"):
        parts = filename.replace('.txt', '').split('_')
        doc_id = parts[0]
        device = parts[1] 
        section = '_'.join(parts[2:])
        
        doc_metadata[filename] = {
            'doc_id': doc_id,
            'device': device,
            'section': section,
            'full_path': f"docs7_export/{filename}"
        }
    
    return doc_metadata
```

### Step 2: ìƒˆë¡œìš´ RAG íŒŒì´í”„ë¼ì¸ í´ë˜ìŠ¤
```python
class LLMBasedRAG:
    """LLMì´ ë¬¸ì„œë¥¼ ì§ì ‘ ì„ íƒí•˜ëŠ” RAG ì‹œìŠ¤í…œ"""
    
    def __init__(self, docs_path="docs7_export"):
        self.docs_path = docs_path
        self.doc_metadata = self.analyze_documents()
        self.llm = get_llm_client()
    
    def process_query(self, question: str, selection_method="llm_direct"):
        """
        selection_method options:
        - "llm_direct": LLMì´ ì§ì ‘ ë¬¸ì„œ ì„ íƒ
        - "hybrid": ë²¡í„° ê²€ìƒ‰ + LLM ì„ íƒ
        - "vector_baseline": ê¸°ì¡´ ë²¡í„° ê²€ìƒ‰ (ë¹„êµìš©)
        """
        
        if selection_method == "llm_direct":
            selected_docs = self.llm_select_documents(question)
        elif selection_method == "hybrid":
            selected_docs = self.hybrid_select_documents(question)
        else:
            selected_docs = self.vector_select_documents(question)
        
        # ì„ íƒëœ ë¬¸ì„œë“¤ë¡œ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = self.build_context(selected_docs)
        
        # ìµœì¢… ë‹µë³€ ìƒì„±
        answer = self.generate_answer(question, context)
        
        return {
            'answer': answer,
            'selected_documents': selected_docs,
            'selection_method': selection_method
        }
```

### Step 3: ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜ ì„¤ê³„
```python
def compare_selection_methods():
    """3ê°€ì§€ ë¬¸ì„œ ì„ íƒ ë°©ë²• ì„±ëŠ¥ ë¹„êµ"""
    
    methods = [
        "vector_baseline",  # ê¸°ì¡´ ë°©ì‹
        "llm_direct",      # LLM ì§ì ‘ ì„ íƒ
        "hybrid"           # í•˜ì´ë¸Œë¦¬ë“œ
    ]
    
    results = {}
    
    for method in methods:
        print(f"Testing {method}...")
        
        method_results = []
        for question, ground_truth in test_dataset:
            # ê° ë°©ë²•ìœ¼ë¡œ ë‹µë³€ ìƒì„±
            result = rag_system.process_query(question, method)
            
            # ì„±ëŠ¥ í‰ê°€
            metrics = evaluate_answer(result['answer'], ground_truth)
            metrics['method'] = method
            metrics['num_docs_used'] = len(result['selected_documents'])
            
            method_results.append(metrics)
        
        results[method] = method_results
    
    return results
```

## ğŸ¯ ì˜ˆìƒ ì„±ëŠ¥ ê°œì„  íš¨ê³¼

### ê°€ì„¤
1. **ì •í™•ë„ í–¥ìƒ**: 5-15% ê°œì„  (BERT Score ê¸°ì¤€)
2. **ì»¨í…ìŠ¤íŠ¸ í’ˆì§ˆ**: ë” ê´€ë ¨ì„± ë†’ì€ ë¬¸ì„œ ì„ íƒ
3. **ì„¤ëª… ê°€ëŠ¥ì„±**: LLMì´ ì„ íƒí•œ ì´ìœ ë¥¼ ì„¤ëª… ê°€ëŠ¥

### ì¸¡ì • ì§€í‘œ
- **Document Relevance Score**: ì„ íƒëœ ë¬¸ì„œì˜ ì‹¤ì œ ìœ ìš©ì„±
- **Context Utilization Rate**: ì»¨í…ìŠ¤íŠ¸ ì •ë³´ í™œìš©ë„  
- **Selection Consistency**: ê°™ì€ ì§ˆë¬¸ì— ëŒ€í•œ ì„ íƒ ì¼ê´€ì„±

## ğŸ”§ ì‹¤ì œ êµ¬í˜„ì„ ìœ„í•œ ì½”ë“œ ìˆ˜ì •

### 1. enhanced_benchmark_runner.py ìˆ˜ì •
```python
# ìƒˆë¡œìš´ RAG ëª¨ë“œ ì¶”ê°€
"experiments": {
    "rag_llm_select": {
        "description": "LLM ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ RAG",
        "use_rag": true,
        "selection_method": "llm_direct",
        "max_iterations": 1,
        "top_k_contexts": 10
    }
}
```

### 2. ìƒˆë¡œìš´ íŒŒì´í”„ë¼ì¸ íŒŒì¼ ìƒì„±
```bash
# pipeline_4_llm_selection.py ìƒì„±
# LLM ê¸°ë°˜ ë¬¸ì„œ ì„ íƒ êµ¬í˜„
```

### 3. ë¹„êµ ì‹¤í—˜ ì„¤ì •
```python
def run_selection_method_comparison():
    """ë¬¸ì„œ ì„ íƒ ë°©ë²•ë³„ ì„±ëŠ¥ ë¹„êµ"""
    methods = ["vector", "llm_direct", "hybrid"]
    
    for method in methods:
        experiment_id = f"selection_{method}_{timestamp}"
        run_experiment_with_method(experiment_id, method)
    
    # ê²°ê³¼ ë¹„êµ ë¶„ì„
    generate_selection_comparison_report()
```

## ğŸ“‹ êµ¬í˜„ ìš°ì„ ìˆœìœ„

### Phase 1: ê¸°ë³¸ êµ¬í˜„ (1-2ì¼)
- [ ] ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¶„ì„ ì½”ë“œ
- [ ] ê¸°ë³¸ LLM ì„ íƒ ë¡œì§
- [ ] ì†Œê·œëª¨ í…ŒìŠ¤íŠ¸ (10ê°œ ì§ˆë¬¸)

### Phase 2: ìµœì í™” (2-3ì¼)  
- [ ] í•˜ì´ë¸Œë¦¬ë“œ ë°©ì‹ êµ¬í˜„
- [ ] ì„±ëŠ¥ ë¹„êµ ì‹¤í—˜
- [ ] í”„ë¡¬í”„íŠ¸ ìµœì í™”

### Phase 3: í†µí•© (1ì¼)
- [ ] ê¸°ì¡´ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œì— í†µí•©
- [ ] ì „ì²´ ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸
- [ ] ê²°ê³¼ ë¶„ì„ ë° ë¦¬í¬íŠ¸

## ğŸ’° ë¹„ìš© ë° ì„±ëŠ¥ ê³ ë ¤ì‚¬í•­

### ë¹„ìš© ì¦ê°€ ìš”ì¸
- **ì¶”ê°€ LLM í˜¸ì¶œ**: ë¬¸ì„œ ì„ íƒì„ ìœ„í•œ ë³„ë„ API í˜¸ì¶œ
- **ê¸´ í”„ë¡¬í”„íŠ¸**: ë¬¸ì„œ ëª©ë¡ì„ í¬í•¨í•œ ë” ê¸´ ì…ë ¥

### ìµœì í™” ë°©ì•ˆ
- **ë¬¸ì„œ ìš”ì•½ ìºì‹±**: í•œ ë²ˆ ìƒì„±í•œ ìš”ì•½ ì¬ì‚¬ìš©
- **ë°°ì¹˜ ì„ íƒ**: ì—¬ëŸ¬ ì§ˆë¬¸ì— ëŒ€í•´ í•œ ë²ˆì— ë¬¸ì„œ ì„ íƒ
- **í›„ë³´êµ° ì¶•ì†Œ**: ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ 1ì°¨ í•„í„°ë§

ì´ ë°©ì‹ì„ êµ¬í˜„í•˜ë©´ "RAG ì„±ëŠ¥ì´ ì•ˆ ì¢‹ë‹¤"ëŠ” ë¬¸ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤!
