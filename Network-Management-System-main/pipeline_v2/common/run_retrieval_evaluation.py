from __future__ import annotations

import os
from datetime import datetime
from pathlib import Path
import json
import pandas as pd
import sys
import argparse

# Ensure parent (pipeline_v2) is importable when running this module directly
CURRENT_DIR = Path(__file__).parent
PARENT_DIR = CURRENT_DIR.parent
if str(PARENT_DIR) not in sys.path:
    sys.path.insert(0, str(PARENT_DIR))

from rag_pipeline import NetworkEngineeringPipeline
from common.llm_utils import get_xml_query, ExperimentLogger, TrackedOpenAIClient  # LLM/ë¡œê·¸
from config import CHROMADB_PATH, COLLECTION_NAME, XML_DIRECTORY


def recall_at_k(metadatas: list[dict], ground_truth_filenames_str: str, k: int) -> float:
    """ì •ë‹µ ë¬¸ì„œ ëª©ë¡ ì¤‘ í•˜ë‚˜ë¼ë„ top-k ê²€ìƒ‰ ê²°ê³¼ ì•ˆì— í¬í•¨ë˜ë©´ 1, ì•„ë‹ˆë©´ 0"""
    if not metadatas:
        return 0.0
    # ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ì •ë‹µ íŒŒì¼ ëª©ë¡ì„ setìœ¼ë¡œ ë³€í™˜ (ê³µë°± ì œê±°)
    ground_truth_set = set(x.strip() for x in str(ground_truth_filenames_str).split(',') if x.strip())
    if not ground_truth_set:
        return 0.0
    filenames_in_results = {str(meta.get('filename', '')).strip() for meta in metadatas[:k]}
    # ë‘ setì˜ êµì§‘í•©ì´ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ ì„±ê³µ
    return 1.0 if not ground_truth_set.isdisjoint(filenames_in_results) else 0.0


def reciprocal_rank(metadatas: list[dict], ground_truth_filenames_str: str) -> float:
    """ì •ë‹µ ë¬¸ì„œ ëª©ë¡ ì¤‘ ê°€ì¥ ë¨¼ì € ë“±ì¥í•˜ëŠ” ê²ƒì˜ Reciprocal Rank ê³„ì‚°"""
    if not metadatas:
        return 0.0
    ground_truth_set = set(x.strip() for x in str(ground_truth_filenames_str).split(',') if x.strip())
    if not ground_truth_set:
        return 0.0
    for idx, meta in enumerate(metadatas, start=1):
        if str(meta.get('filename', '')).strip() in ground_truth_set:
            return 1.0 / idx  # ê°€ì¥ ë¨¼ì € ì°¾ì€ ê²ƒì˜ ì ìˆ˜ ë°˜í™˜
    return 0.0


def evaluate_retrieval(pipeline, dataset: pd.DataFrame, query_fn, k_values: list[int]):
    """RAG ê²€ìƒ‰ ì„±ëŠ¥ í‰ê°€ (Heuristic/LLM Query + GPT ReRank)"""
    import numpy as np

    recall_scores = {k: [] for k in k_values}
    rr_scores = []
    max_k = max(k_values)
    for _, row in dataset.iterrows():
        question = row["question"]
        gt_filenames = row["ground_truth_filename"]  # ì‰¼í‘œ êµ¬ë¶„ ê°€ëŠ¥

        # 1) ì¿¼ë¦¬ ìƒì„±
        query = query_fn(question, "")

        # 2) ê²€ìƒ‰ + GPT ReRank
        documents, metadatas = pipeline.get_chromadb_content_for_eval(query, n_results=max_k)

        # 3) Recall@k
        for k in k_values:
            recall_scores[k].append(recall_at_k(metadatas, gt_filenames, k))

        # 4) MRR
        rr_scores.append(reciprocal_rank(metadatas, gt_filenames))

    recall_avg = {k: float(np.mean(v)) if v else 0.0 for k, v in recall_scores.items()}
    mrr = float(np.mean(rr_scores)) if rr_scores else 0.0
    return recall_avg, mrr

def main():
    print("ğŸš€ Retrieval ì„±ëŠ¥ í‰ê°€ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")

    parser = argparse.ArgumentParser(description="Run retrieval evaluation (Heuristic/LLM + GPT ReRank)")
    parser.add_argument(
        "--dataset",
        default=str(PARENT_DIR.parent / "dataset" / "retrieval_test.csv"),
        help="Retrieval í‰ê°€ CSV ê²½ë¡œ (columns: question, ground_truth_filename)",
    )
    parser.add_argument(
        "--k-values",
        default="1,5,10",
        help="Comma-separated K values for Recall@K (e.g., 1,5,10)",
    )
    args = parser.parse_args()

    # ê³µìš© ì¶œë ¥ ë””ë ‰í† ë¦¬/ë¡œê±° ì¤€ë¹„ (LLM ì¬ì •ë ¬ ë¡œê¹…ìš©)
    ts = datetime.now().strftime("%Y%m%d_%H%M%S")
    out_dir = PARENT_DIR / "experiment_results" / f"retrieval_{ts}"
    out_dir.mkdir(parents=True, exist_ok=True)
    logger = ExperimentLogger("retrieval_evaluation", str(out_dir))
    # rag_pipelineì˜ ì „ì—­ LLM í•¸ë“¤ ì£¼ì… (GPT ReRank ì‚¬ìš©)
    import rag_pipeline as rp
    rp.tracked_openai_client = TrackedOpenAIClient(logger)

    # 1. í‰ê°€í•  íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    # NetworkEngineeringPipelineì€ ë‚´ë¶€ì— Embedderì™€ DBë¥¼ ëª¨ë‘ í¬í•¨í•˜ê³  ìˆìŒ
    pipeline = NetworkEngineeringPipeline(
        chromadb_path=CHROMADB_PATH,
        collection_name=COLLECTION_NAME,
        max_iterations=0, # í‰ê°€ ì‹œì—ëŠ” ë°˜ë³µì´ í•„ìš” ì—†ìŒ
        xml_directory=XML_DIRECTORY,
    )

    # 2. í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ (í•„ìˆ˜ ì»¬ëŸ¼: question, ground_truth_filename)
    ds_path = Path(args.dataset)
    if not ds_path.exists():
        raise FileNotFoundError(
            f"âŒ í‰ê°€ ë°ì´í„°ì…‹ì´ ì—†ìŠµë‹ˆë‹¤: {ds_path}\n"
            f"í•„ìˆ˜ ì»¬ëŸ¼: question, ground_truth_filename (ì˜ˆ: ce1.xml)"
        )
    dataset = pd.read_csv(ds_path)
    cols = set(dataset.columns)
    if "ground_truth_filename" not in cols and "source_files" in cols:
        # allow legacy column name
        dataset = dataset.rename(columns={"source_files": "ground_truth_filename"})
        cols = set(dataset.columns)
    if not {"question", "ground_truth_filename"}.issubset(cols):
        raise ValueError("âŒ retrieval_test.csvì—ëŠ” question, ground_truth_filename ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤ (ë˜ëŠ” source_filesë¥¼ ì‚¬ìš©í•˜ë©´ ìë™ ë§¤í•‘)")
    print(f"ğŸ“Š í‰ê°€ ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(dataset)}ê°œ ì§ˆë¬¸ â†’ {ds_path}")

    # 3. í‰ê°€ ì‹¤í–‰
    k_values = [int(x.strip()) for x in str(args.k_values).split(',') if x.strip()]

    # --- Heuristic Query + GPT ReRanking ---
    print("\nEvaluating: Heuristic Query + GPT ReRanking...")
    recall_h, mrr_h = evaluate_retrieval(
        pipeline,
        dataset,
        query_fn=lambda q, a: f"ë‹¨ìˆœ ì¡°íšŒ, {q}",
        k_values=k_values
    )
    print(f"  - Recall Scores: {recall_h}")
    print(f"  - MRR Score: {mrr_h:.4f}")

    # --- LLM Query + GPT ReRanking ---
    print("\nEvaluating: LLM Query + GPT ReRanking...")
    recall_llm, mrr_llm = evaluate_retrieval(
        pipeline,
        dataset,
        query_fn=lambda q, a: get_xml_query(q, ""), # get_xml_queryëŠ” LLM í˜¸ì¶œ í•¨ìˆ˜
        k_values=k_values
    )
    print(f"  - Recall Scores: {recall_llm}")
    print(f"  - MRR Score: {mrr_llm:.4f}")

    # 4. ê²°ê³¼ ì €ì¥ (Markdown/JSON)

    md_lines = []
    md_lines.append("## í‘œ 4: Retrieval Performance (RAGë§Œ)")
    md_lines.append("| Method | Recall@1 | Recall@5 | Recall@10| Recall@20 | MRR |")
    md_lines.append("|--------|---------:|---------:|----------:|----------:|----:|")
    md_lines.append(
        f"| Heuristic + GPT ReRank | {recall_h.get(1,0):.4f} | {recall_h.get(5,0):.4f} | {recall_h.get(10,0):.4f} | {recall_h.get(20,0):.4f} | {mrr_h:.4f} |"
    )
    md_lines.append(
        f"| LLM Query + GPT ReRank | {recall_llm.get(1,0):.4f} | {recall_llm.get(5,0):.4f} | {recall_llm.get(10,0):.4f} | {recall_llm.get(20,0):.4f} | {mrr_llm:.4f} |"
    )
    md = "\n".join(md_lines)
    (out_dir / "retrieval_performance.md").write_text(md, encoding="utf-8")

    payload = {
        "k_values": k_values,
        "dataset_path": str(ds_path),
        "methods": {
            "heuristic+gpt_rerank": {
                "recall@1": float(f"{recall_h.get(1, 0):.3f}"),
                "recall@5": float(f"{recall_h.get(5, 0):.3f}"),
                "recall@10": float(f"{recall_h.get(10, 0):.3f}"),
                "recall@20": float(f"{recall_h.get(20, 0):.3f}"),
                "mrr": float(f"{mrr_h:.3f}"),
            },
            "llm+gpt_rerank": {
                "recall@1": float(f"{recall_llm.get(1, 0):.3f}"),
                "recall@5": float(f"{recall_llm.get(5, 0):.3f}"),
                "recall@10": float(f"{recall_llm.get(10, 0):.3f}"),
                "recall@20": float(f"{recall_llm.get(20, 0):.3f}"),
                "mrr": float(f"{mrr_llm:.3f}"),
            },
        },
        "timestamp": ts,
    }
    (out_dir / "retrieval_performance.json").write_text(json.dumps(payload, ensure_ascii=False, indent=2), encoding="utf-8")

    # 5. ì½˜ì†” ì¶œë ¥ ìš”ì•½
    print("\n" + "=" * 70)
    print("âœ… Retrieval ì„±ëŠ¥ ìš”ì•½ (ì €ì¥ ì™„ë£Œ)")
    print(md)
    print(f"ğŸ“„ ì €ì¥ ìœ„ì¹˜: {out_dir}")
    print("=" * 70)

    # ë§ˆë¬´ë¦¬
    logger.finalize_experiment()


if __name__ == "__main__":
    main()
