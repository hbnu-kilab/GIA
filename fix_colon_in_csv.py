#!/usr/bin/env python3
"""
CSV íŒŒì¼ì—ì„œ pre_GT ì»¬ëŸ¼ì˜ ì½œë¡ ì„ ì§ì ‘ ì œê±°í•˜ê³  ë‹¤ì‹œ í‰ê°€
"""
import pandas as pd
import json
from pathlib import Path
from typing import Dict
from Evaluation.pipeline_v2.common.data_utils import clean_ground_truth_text
from Evaluation.pipeline_v2.common.evaluation import calculate_exact_match, calculate_f1_score, calculate_relaxed_exact_match, calculate_relaxed_f1_score

def fix_colon_in_csv(csv_path: str) -> str:
    """CSV íŒŒì¼ì—ì„œ pre_GT ì»¬ëŸ¼ì˜ ì½œë¡ ì„ ì œê±°"""
    print(f"ì½œë¡  ì œê±° ì¤‘: {csv_path}")
    
    # CSV ì½ê¸°
    df = pd.read_csv(csv_path)
    
    # pre_GT ì»¬ëŸ¼ì—ì„œ ì½œë¡  ì œê±°
    original_count = 0
    fixed_count = 0
    
    for i, row in df.iterrows():
        if pd.notna(row['pre_GT']):
            original_text = str(row['pre_GT'])
            if original_text.startswith(': '):
                # ë§¨ ì•ì˜ ': ' ì œê±°
                fixed_text = original_text[2:]  # ': ' (2ê¸€ì) ì œê±°
                df.at[i, 'pre_GT'] = fixed_text
                fixed_count += 1
            original_count += 1
    
    print(f"ì „ì²´ pre_GT: {original_count}ê°œ")
    print(f"ì½œë¡  ì œê±°: {fixed_count}ê°œ")
    
    # ìˆ˜ì •ëœ íŒŒì¼ ì €ì¥
    output_path = csv_path.replace('.csv', '_fixed.csv')
    df.to_csv(output_path, index=False)
    
    return output_path

def evaluate_fixed_csv(csv_path: str) -> Dict:
    """ì½œë¡ ì´ ì œê±°ëœ CSV ì¬í‰ê°€"""
    print(f"ì¬í‰ê°€ ì¤‘: {csv_path}")
    
    df = pd.read_csv(csv_path)
    
    # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ì •ë¦¬ (ì´ì œ ì½œë¡ ì´ ì´ë¯¸ ì œê±°ë¨)
    predictions = []
    ground_truths = []
    origins = df['origin'].tolist()
    
    for _, row in df.iterrows():
        # ì˜ˆì¸¡ ê²°ê³¼ (ì´ë¯¸ ì½œë¡  ì œê±°ë¨, ì¶”ê°€ ì •ë¦¬ë§Œ)
        pred_raw = str(row['pre_GT']) if pd.notna(row['pre_GT']) else ""
        pred_clean = clean_ground_truth_text(pred_raw)
        predictions.append(pred_clean)
        
        # ì •ë‹µ ì •ë¦¬
        gt_raw = str(row['ground_truth']) if pd.notna(row['ground_truth']) else ""
        gt_clean = clean_ground_truth_text(gt_raw)
        ground_truths.append(gt_clean)
    
    # ì¸ë±ìŠ¤ ê·¸ë£¹ ë¶„ë¦¬
    rule_idx = [i for i, o in enumerate(origins) if o == "rule_based"]
    enhanced_idx = [i for i, o in enumerate(origins) if o == "enhanced_llm_with_agent"]
    
    print(f"Rule-based ì§ˆë¬¸: {len(rule_idx)}ê°œ")
    print(f"Enhanced LLM ì§ˆë¬¸: {len(enhanced_idx)}ê°œ")
    
    # ì „ì²´ í‰ê°€
    overall_em = calculate_exact_match(predictions, ground_truths)
    overall_f1 = calculate_f1_score(predictions, ground_truths)
    overall_em_relaxed = calculate_relaxed_exact_match(predictions, ground_truths)
    overall_f1_relaxed = calculate_relaxed_f1_score(predictions, ground_truths)
    
    # Rule-based í‰ê°€ (GTë§Œ)
    if rule_idx:
        rule_pred_gt = [predictions[i] for i in rule_idx]
        rule_true_gt = [ground_truths[i] for i in rule_idx]
        rule_em = calculate_exact_match(rule_pred_gt, rule_true_gt)
        rule_f1 = calculate_f1_score(rule_pred_gt, rule_true_gt)
        rule_em_relaxed = calculate_relaxed_exact_match(rule_pred_gt, rule_true_gt)
        rule_f1_relaxed = calculate_relaxed_f1_score(rule_pred_gt, rule_true_gt)
    else:
        rule_em = rule_f1 = rule_em_relaxed = rule_f1_relaxed = 0.0
    
    # Enhanced LLM í‰ê°€ (GT + ì„¤ëª…)
    if enhanced_idx:
        enhanced_pred_gt = [predictions[i] for i in enhanced_idx]
        enhanced_true_gt = [ground_truths[i] for i in enhanced_idx]
        enhanced_em = calculate_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_f1 = calculate_f1_score(enhanced_pred_gt, enhanced_true_gt)
        enhanced_em_relaxed = calculate_relaxed_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_f1_relaxed = calculate_relaxed_f1_score(enhanced_pred_gt, enhanced_true_gt)
    else:
        enhanced_em = enhanced_f1 = enhanced_em_relaxed = enhanced_f1_relaxed = 0.0
    
    # ìƒ˜í”Œ ë¹„êµ ì¶œë ¥
    print(f"\nìƒ˜í”Œ ë¹„êµ (ì²˜ìŒ 5ê°œ):")
    for i in range(min(5, len(predictions))):
        pred = predictions[i]
        gt = ground_truths[i]
        match = pred.strip().lower() == gt.strip().lower()
        print(f"  [{i}] EM: {match}")
        print(f"      GT: '{gt}'")
        print(f"      Pred: '{pred}'")
        print()
    
    results = {
        "total_questions": len(predictions),
        "overall": {
            "exact_match": overall_em,
            "f1_score": overall_f1,
            "exact_match_relaxed": overall_em_relaxed,
            "f1_score_relaxed": overall_f1_relaxed
        },
        "rule_based": {
            "question_count": len(rule_idx),
            "exact_match": rule_em,
            "f1_score": rule_f1,
            "exact_match_relaxed": rule_em_relaxed,
            "f1_score_relaxed": rule_f1_relaxed
        },
        "enhanced_llm_with_agent": {
            "question_count": len(enhanced_idx),
            "exact_match": enhanced_em,
            "f1_score": enhanced_f1,
            "exact_match_relaxed": enhanced_em_relaxed,
            "f1_score_relaxed": enhanced_f1_relaxed
        }
    }
    
    return results

def update_json_with_fixed_evaluation(json_path: str, evaluation_results: Dict):
    """JSON íŒŒì¼ì„ ìˆ˜ì •ëœ í‰ê°€ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸"""
    print(f"JSON íŒŒì¼ ìµœì¢… ì—…ë°ì´íŠ¸: {json_path}")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ìµœì¢… ë°±ì—… ìƒì„±
    backup_path = json_path.replace('.json', '_before_colon_fix.json')
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # evaluation ì„¹ì…˜ êµì²´
    if 'evaluation' in data:
        data['evaluation'] = {
            "overall": {
                "exact_match": evaluation_results["overall"]["exact_match"],
                "f1_score": evaluation_results["overall"]["f1_score"],
                "total_questions": evaluation_results["total_questions"]
            },
            "overall_relaxed": {
                "exact_match": evaluation_results["overall"]["exact_match_relaxed"],
                "f1_score": evaluation_results["overall"]["f1_score_relaxed"],
                "total_questions": evaluation_results["total_questions"]
            },
            "rule_based": {
                "exact_match": evaluation_results["rule_based"]["exact_match"],
                "f1_score": evaluation_results["rule_based"]["f1_score"],
                "question_count": evaluation_results["rule_based"]["question_count"]
            },
            "rule_based_relaxed": {
                "exact_match": evaluation_results["rule_based"]["exact_match_relaxed"],
                "f1_score": evaluation_results["rule_based"]["f1_score_relaxed"],
                "question_count": evaluation_results["rule_based"]["question_count"]
            },
            "enhanced_llm": {
                "ground_truth": {
                    "exact_match": evaluation_results["enhanced_llm_with_agent"]["exact_match"],
                    "f1_score": evaluation_results["enhanced_llm_with_agent"]["f1_score"]
                },
                "explanation": data['evaluation'].get('enhanced_llm', {}).get('explanation', {}),
                "question_count": evaluation_results["enhanced_llm_with_agent"]["question_count"]
            }
        }
    
    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ì €ì¥
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def main():
    base_dir = Path("runs/exp_ab_k10/results")
    
    # ì²˜ë¦¬í•  íŒŒì¼ë“¤
    files_to_process = [
        ("ragA_after_k10_with_origin.csv", "ragA_k10.json"),
        ("ragB_after_k10_with_origin.csv", "ragB_k10.json"),
    ]
    
    for csv_file, json_file in files_to_process:
        csv_path = base_dir / csv_file
        json_path = base_dir / json_file
        
        if csv_path.exists() and json_path.exists():
            print(f"\n{'='*70}")
            print(f"ìµœì¢… ì½œë¡  ì œê±° ì²˜ë¦¬: {csv_file} -> {json_file}")
            print(f"{'='*70}")
            
            # 1. CSVì—ì„œ ì½œë¡  ì§ì ‘ ì œê±°
            fixed_csv = fix_colon_in_csv(str(csv_path))
            print(f"âœ… ì½œë¡ ì´ ì œê±°ëœ íŒŒì¼: {fixed_csv}")
            
            # 2. ì¬í‰ê°€
            evaluation_results = evaluate_fixed_csv(fixed_csv)
            
            # 3. ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸ¯ ìµœì¢… ê²°ê³¼:")
            print(f"  ì „ì²´ ì§ˆë¬¸: {evaluation_results['total_questions']}ê°œ")
            print(f"  Overall EM: {evaluation_results['overall']['exact_match']:.4f}")
            print(f"  Overall F1: {evaluation_results['overall']['f1_score']:.4f}")
            print(f"  Rule-based ({evaluation_results['rule_based']['question_count']}ê°œ):")
            print(f"    EM: {evaluation_results['rule_based']['exact_match']:.4f}")
            print(f"    F1: {evaluation_results['rule_based']['f1_score']:.4f}")
            print(f"  Enhanced LLM ({evaluation_results['enhanced_llm_with_agent']['question_count']}ê°œ):")
            print(f"    EM: {evaluation_results['enhanced_llm_with_agent']['exact_match']:.4f}")
            print(f"    F1: {evaluation_results['enhanced_llm_with_agent']['f1_score']:.4f}")
            
            # 4. JSON íŒŒì¼ ìµœì¢… ì—…ë°ì´íŠ¸
            update_json_with_fixed_evaluation(str(json_path), evaluation_results)
            print(f"âœ… {json_file} ìµœì¢… ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        else:
            print(f"âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file} ë˜ëŠ” {json_file}")
    
    print(f"\n{'='*70}")
    print("ğŸ‰ ëª¨ë“  ì½œë¡  ì œê±° ë° ìµœì¢… í‰ê°€ ì™„ë£Œ!")
    print("ì´ì œ ì§„ì§œ ì •í™•í•œ ì„±ëŠ¥ ê²°ê³¼ê°€ ë‚˜ì™”ìŠµë‹ˆë‹¤!")
    print(f"{'='*70}")

if __name__ == "__main__":
    main() 