#!/usr/bin/env python3
"""
ê³ ê¸‰ ì „ì²˜ë¦¬: TRUE/False í†µì¼, ì½”ë“œë¸”ë¡ ì œê±°, ë§ˆí¬ë‹¤ìš´ ìš”ì†Œ ì œê±°
"""
import pandas as pd
import json
import re
from pathlib import Path
from typing import Dict
from Evaluation.pipeline_v2.common.data_utils import clean_ground_truth_text
from Evaluation.pipeline_v2.common.evaluation import calculate_exact_match, calculate_f1_score, calculate_relaxed_exact_match, calculate_relaxed_f1_score

def advanced_clean_text(text: str, question: str = None) -> str:
    """ê³ ê¸‰ í…ìŠ¤íŠ¸ ì •ë¦¬"""
    if not text or pd.isna(text):
        return ""
    
    text = str(text).strip()
    
    # 1. ì½”ë“œ ë¸”ë¡ ì œê±° (```bash, ```, etc.)
    text = re.sub(r'```(?:bash|python|sh|shell)?\s*([^`]*?)```', r'\1', text, flags=re.DOTALL)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    
    # 2. ë§ˆí¬ë‹¤ìš´ ìš”ì†Œ ì œê±°
    text = re.sub(r'\*\*(.*?)\*\*', r'\1', text)  # **bold**
    text = re.sub(r'\*(.*?)\*', r'\1', text)      # *italic*
    text = re.sub(r'_(.*?)_', r'\1', text)        # _underline_
    
    # 3. ëŒ€ì‹œ(-) ì‹œì‘ ì œê±°
    text = re.sub(r'^-\s*', '', text.strip())
    
    # 4. TRUE/FALSE í‘œì¤€í™”
    text = text.strip()
    if text.upper() in ['TRUE', 'True', 'true', 'T']:
        return 'TRUE'
    elif text.upper() in ['FALSE', 'False', 'false', 'F']:
        return 'FALSE'
    
    # 5. user@ vs nso@ í†µì¼ (nso@ -> user@)
    text = re.sub(r'\bnso@', 'user@', text)
    
    # 6. í™”ì‚´í‘œ vs ì‰¼í‘œ ìˆœì„œ ì •ê·œí™” (ì§ˆë¬¸ì—ì„œ í™”ì‚´í‘œë¥¼ ìš”êµ¬í•˜ëŠ” ê²½ìš°ì—ë§Œ)
    # ì§ˆë¬¸ì— "â†’" ë˜ëŠ” "ì¥ë¹„ëª…â†’ì¥ë¹„ëª…" íŒ¨í„´ì´ ìˆëŠ” ê²½ìš°ì—ë§Œ ì²˜ë¦¬
    if question and ('â†’' in question or 'ì¥ë¹„ëª…â†’' in question or 'ì •ë‹µ í˜•ì‹:' in question and 'â†’' in question):
        if ',' in text and ('sample' in text.lower() or 'ce' in text.lower()):
            # ì¥ë¹„ëª…ìœ¼ë¡œ ë³´ì´ëŠ” ê²ƒë“¤ì„ í™”ì‚´í‘œë¡œ ì—°ê²° ì‹œë„
            parts = [p.strip() for p in text.split(',')]
            equipment_parts = []
            other_parts = []
            
            for part in parts:
                # ì¥ë¹„ëª… íŒ¨í„´ í™•ì¸ (sample7, CE1, IP ì£¼ì†Œ ë“±)
                if (re.match(r'^(sample\d+|CE\d+|\d+\.\d+\.\d+\.\d+)$', part.strip()) or 
                    part.strip() in ['sample7', 'sample8', 'sample9', 'sample10', 'CE1', 'CE2']):
                    equipment_parts.append(part.strip())
                else:
                    other_parts.append(part.strip())
            
            # ì¥ë¹„ê°€ 2ê°œ ì´ìƒì´ë©´ í™”ì‚´í‘œë¡œ ì—°ê²°
            if len(equipment_parts) >= 2:
                equipment_str = 'â†’'.join(equipment_parts)
                if other_parts:
                    text = equipment_str + ',' + ','.join(other_parts)
                else:
                    text = equipment_str
    
    # 7. ê¸°ë³¸ ì •ë¦¬ (ê³µë°±, íŠ¹ìˆ˜ë¬¸ì)
    text = re.sub(r'\s+', ' ', text)
    
    return text.strip()

def apply_advanced_preprocessing(csv_path: str) -> str:
    """CSV íŒŒì¼ì— ê³ ê¸‰ ì „ì²˜ë¦¬ ì ìš©"""
    print(f"ê³ ê¸‰ ì „ì²˜ë¦¬ ì ìš© ì¤‘: {csv_path}")
    
    # CSV íŒŒì‹± ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë” ê°•ë ¥í•œ ì˜µì…˜ ì ìš©
    df = pd.read_csv(csv_path, sep=',', quotechar='"', skipinitialspace=True, on_bad_lines='skip')
    
    # pre_GTì™€ ground_truth ëª¨ë‘ ì „ì²˜ë¦¬ ì ìš©
    pre_gt_changes = 0
    gt_changes = 0
    
    print("\në³€ê²½ ì‚¬ë¡€ë“¤:")
    for i, row in df.iterrows():
        # pre_GT ì „ì²˜ë¦¬ (ì§ˆë¬¸ ì»¨í…ìŠ¤íŠ¸ í¬í•¨)
        if pd.notna(row['pre_GT']):
            original_pred = str(row['pre_GT'])
            question = str(row.get('question', '')) if 'question' in df.columns else None
            cleaned_pred = advanced_clean_text(original_pred, question)
            if original_pred != cleaned_pred:
                df.at[i, 'pre_GT'] = cleaned_pred
                pre_gt_changes += 1
                if pre_gt_changes <= 5:  # ì²˜ìŒ 5ê°œ ì˜ˆì‹œ ì¶œë ¥
                    print(f"  pre_GT[{i}]: '{original_pred}' -> '{cleaned_pred}'")
        
        # ground_truth ì „ì²˜ë¦¬ (ë¹„êµë¥¼ ìœ„í•´)
        if pd.notna(row['ground_truth']):
            original_gt = str(row['ground_truth'])
            question = str(row.get('question', '')) if 'question' in df.columns else None
            cleaned_gt = advanced_clean_text(original_gt, question)
            if original_gt != cleaned_gt:
                df.at[i, 'ground_truth'] = cleaned_gt
                gt_changes += 1
                if gt_changes <= 5:  # ì²˜ìŒ 5ê°œ ì˜ˆì‹œ ì¶œë ¥
                    print(f"  ground_truth[{i}]: '{original_gt}' -> '{cleaned_gt}'")
    
    print(f"\nì „ì²˜ë¦¬ ê²°ê³¼:")
    print(f"  pre_GT ë³€ê²½: {pre_gt_changes}ê°œ")
    print(f"  ground_truth ë³€ê²½: {gt_changes}ê°œ")
    
    # ì €ì¥
    output_path = csv_path.replace('.csv', '_advanced.csv')
    df.to_csv(output_path, index=False)
    
    return output_path

def evaluate_advanced_csv(csv_path: str) -> Dict:
    """ê³ ê¸‰ ì „ì²˜ë¦¬ëœ CSV ì¬í‰ê°€"""
    print(f"\nê³ ê¸‰ ì „ì²˜ë¦¬ ê²°ê³¼ ì¬í‰ê°€: {csv_path}")
    
    df = pd.read_csv(csv_path, sep=',', quotechar='"', skipinitialspace=True, on_bad_lines='skip')
    
    # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ì •ë¦¬ (ì¶”ê°€ ì •ë¦¬)
    predictions = []
    ground_truths = []
    origins = df['origin'].tolist()
    
    for _, row in df.iterrows():
        # ì˜ˆì¸¡ ê²°ê³¼ (ì´ë¯¸ ê³ ê¸‰ ì „ì²˜ë¦¬ë¨, ê¸°ë³¸ ì •ë¦¬ë§Œ ì¶”ê°€)
        pred_raw = str(row['pre_GT']) if pd.notna(row['pre_GT']) else ""
        pred_clean = clean_ground_truth_text(pred_raw)
        predictions.append(pred_clean)
        
        # ì •ë‹µ ì •ë¦¬
        gt_raw = str(row['ground_truth']) if pd.notna(row['ground_truth']) else ""
        gt_clean = clean_ground_truth_text(gt_raw)
        ground_truths.append(gt_clean)
    
    # ì¸ë±ìŠ¤ ê·¸ë£¹ ë¶„ë¦¬
    rule_idx = [i for i, o in enumerate(origins) if o == "rule_based"]
    enhanced_idx = [i for i, o in enumerate(origins) if o == "enhanced_llm_with_agent"]
    
    print(f"Rule-based ì§ˆë¬¸: {len(rule_idx)}ê°œ")
    print(f"Enhanced LLM ì§ˆë¬¸: {len(enhanced_idx)}ê°œ")
    
    # TRUE/FALSE ë§¤ì¹­ ì²´í¬
    true_false_matches = 0
    true_false_total = 0
    
    for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):
        if gt.upper() in ['TRUE', 'FALSE'] or pred.upper() in ['TRUE', 'FALSE']:
            true_false_total += 1
            if pred.upper() == gt.upper():
                true_false_matches += 1
    
    if true_false_total > 0:
        print(f"TRUE/FALSE ì§ˆë¬¸: {true_false_matches}/{true_false_total} ë§¤ì¹­ ({true_false_matches/true_false_total*100:.1f}%)")
    
    # ì „ì²´ í‰ê°€
    overall_em = calculate_exact_match(predictions, ground_truths)
    overall_f1 = calculate_f1_score(predictions, ground_truths)
    overall_em_relaxed = calculate_relaxed_exact_match(predictions, ground_truths)
    overall_f1_relaxed = calculate_relaxed_f1_score(predictions, ground_truths)
    
    # Rule-based í‰ê°€
    if rule_idx:
        rule_pred_gt = [predictions[i] for i in rule_idx]
        rule_true_gt = [ground_truths[i] for i in rule_idx]
        rule_em = calculate_exact_match(rule_pred_gt, rule_true_gt)
        rule_f1 = calculate_f1_score(rule_pred_gt, rule_true_gt)
        rule_em_relaxed = calculate_relaxed_exact_match(rule_pred_gt, rule_true_gt)
        rule_f1_relaxed = calculate_relaxed_f1_score(rule_pred_gt, rule_true_gt)
    else:
        rule_em = rule_f1 = rule_em_relaxed = rule_f1_relaxed = 0.0
    
    # Enhanced LLM í‰ê°€
    if enhanced_idx:
        enhanced_pred_gt = [predictions[i] for i in enhanced_idx]
        enhanced_true_gt = [ground_truths[i] for i in enhanced_idx]
        enhanced_em = calculate_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_f1 = calculate_f1_score(enhanced_pred_gt, enhanced_true_gt)
        enhanced_em_relaxed = calculate_relaxed_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_f1_relaxed = calculate_relaxed_f1_score(enhanced_pred_gt, enhanced_true_gt)
    else:
        enhanced_em = enhanced_f1 = enhanced_em_relaxed = enhanced_f1_relaxed = 0.0
    
    # ìƒ˜í”Œ ë¹„êµ ì¶œë ¥ (TRUE/FALSE ìš°ì„ )
    print(f"\nìƒ˜í”Œ ë¹„êµ (TRUE/FALSE ìš°ì„ ):")
    shown_samples = 0
    for i, (pred, gt) in enumerate(zip(predictions, ground_truths)):
        if shown_samples >= 10:
            break
        if gt.upper() in ['TRUE', 'FALSE'] or pred.upper() in ['TRUE', 'FALSE']:
            match = pred.upper() == gt.upper()
            print(f"  [{i}] EM: {match}")
            print(f"      GT: '{gt}'")
            print(f"      Pred: '{pred}'")
            print()
            shown_samples += 1
    
    # ì¼ë°˜ ìƒ˜í”Œë“¤ë„ ëª‡ê°œ ë³´ì—¬ì£¼ê¸°
    if shown_samples < 5:
        print(f"ì¼ë°˜ ìƒ˜í”Œë“¤:")
        for i in range(min(5-shown_samples, len(predictions))):
            pred = predictions[i]
            gt = ground_truths[i]
            match = pred.strip().lower() == gt.strip().lower()
            print(f"  [{i}] EM: {match}")
            print(f"      GT: '{gt}'")
            print(f"      Pred: '{pred}'")
            print()
    
    results = {
        "total_questions": len(predictions),
        "overall": {
            "exact_match": overall_em,
            "f1_score": overall_f1,
            "exact_match_relaxed": overall_em_relaxed,
            "f1_score_relaxed": overall_f1_relaxed
        },
        "rule_based": {
            "question_count": len(rule_idx),
            "exact_match": rule_em,
            "f1_score": rule_f1,
            "exact_match_relaxed": rule_em_relaxed,
            "f1_score_relaxed": rule_f1_relaxed
        },
        "enhanced_llm_with_agent": {
            "question_count": len(enhanced_idx),
            "exact_match": enhanced_em,
            "f1_score": enhanced_f1,
            "exact_match_relaxed": enhanced_em_relaxed,
            "f1_score_relaxed": enhanced_f1_relaxed
        }
    }
    
    return results

def update_json_with_advanced_evaluation(json_path: str, evaluation_results: Dict):
    """JSON íŒŒì¼ì„ ê³ ê¸‰ ì „ì²˜ë¦¬ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸"""
    print(f"JSON íŒŒì¼ ê³ ê¸‰ ì—…ë°ì´íŠ¸: {json_path}")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ê³ ê¸‰ ë°±ì—… ìƒì„±
    backup_path = json_path.replace('.json', '_before_advanced.json')
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # evaluation ì„¹ì…˜ êµì²´
    if 'evaluation' in data:
        data['evaluation'] = {
            "overall": {
                "exact_match": evaluation_results["overall"]["exact_match"],
                "f1_score": evaluation_results["overall"]["f1_score"],
                "total_questions": evaluation_results["total_questions"]
            },
            "overall_relaxed": {
                "exact_match": evaluation_results["overall"]["exact_match_relaxed"],
                "f1_score": evaluation_results["overall"]["f1_score_relaxed"],
                "total_questions": evaluation_results["total_questions"]
            },
            "rule_based": {
                "exact_match": evaluation_results["rule_based"]["exact_match"],
                "f1_score": evaluation_results["rule_based"]["f1_score"],
                "question_count": evaluation_results["rule_based"]["question_count"]
            },
            "rule_based_relaxed": {
                "exact_match": evaluation_results["rule_based"]["exact_match_relaxed"],
                "f1_score": evaluation_results["rule_based"]["f1_score_relaxed"],
                "question_count": evaluation_results["rule_based"]["question_count"]
            },
            "enhanced_llm": {
                "ground_truth": {
                    "exact_match": evaluation_results["enhanced_llm_with_agent"]["exact_match"],
                    "f1_score": evaluation_results["enhanced_llm_with_agent"]["f1_score"]
                },
                "explanation": data['evaluation'].get('enhanced_llm', {}).get('explanation', {}),
                "question_count": evaluation_results["enhanced_llm_with_agent"]["question_count"]
            }
        }
    
    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ì €ì¥
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def main():
    base_dir = Path("runs/exp_ab_k10/results")
    
    # ì²˜ë¦¬í•  íŒŒì¼ë“¤
    files_to_process = [
        ("ragA_after_k10_with_origin_fixed.csv", "ragA_k10.json"),
        ("ragB_after_k10_with_origin_fixed.csv", "ragB_k10.json"),
    ]
    
    for csv_file, json_file in files_to_process:
        csv_path = base_dir / csv_file
        json_path = base_dir / json_file
        
        if csv_path.exists() and json_path.exists():
            print(f"\n{'='*80}")
            print(f"ê³ ê¸‰ ì „ì²˜ë¦¬: {csv_file} -> {json_file}")
            print(f"{'='*80}")
            
            # 1. ê³ ê¸‰ ì „ì²˜ë¦¬ ì ìš©
            advanced_csv = apply_advanced_preprocessing(str(csv_path))
            print(f"âœ… ê³ ê¸‰ ì „ì²˜ë¦¬ ì™„ë£Œ: {advanced_csv}")
            
            # 2. ì¬í‰ê°€
            evaluation_results = evaluate_advanced_csv(advanced_csv)
            
            # 3. ê²°ê³¼ ì¶œë ¥
            print(f"\nğŸš€ ìµœì¢… ê³ ê¸‰ ì „ì²˜ë¦¬ ê²°ê³¼:")
            print(f"  ì „ì²´ ì§ˆë¬¸: {evaluation_results['total_questions']}ê°œ")
            print(f"  Overall EM: {evaluation_results['overall']['exact_match']:.4f}")
            print(f"  Overall F1: {evaluation_results['overall']['f1_score']:.4f}")
            print(f"  Rule-based ({evaluation_results['rule_based']['question_count']}ê°œ):")
            print(f"    EM: {evaluation_results['rule_based']['exact_match']:.4f}")
            print(f"    F1: {evaluation_results['rule_based']['f1_score']:.4f}")
            print(f"  Enhanced LLM ({evaluation_results['enhanced_llm_with_agent']['question_count']}ê°œ):")
            print(f"    EM: {evaluation_results['enhanced_llm_with_agent']['exact_match']:.4f}")
            print(f"    F1: {evaluation_results['enhanced_llm_with_agent']['f1_score']:.4f}")
            
            # 4. JSON íŒŒì¼ ìµœì¢… ì—…ë°ì´íŠ¸
            update_json_with_advanced_evaluation(str(json_path), evaluation_results)
            print(f"âœ… {json_file} ê³ ê¸‰ ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        else:
            print(f"âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file} ë˜ëŠ” {json_file}")
    
    print(f"\n{'='*80}")
    print("ğŸ‰ ëª¨ë“  ê³ ê¸‰ ì „ì²˜ë¦¬ ë° ìµœì¢… í‰ê°€ ì™„ë£Œ!")
    print("TRUE/FALSE í†µì¼, ì½”ë“œë¸”ë¡ ì œê±°, ë§ˆí¬ë‹¤ìš´ ì •ë¦¬ ëª¨ë‘ ì ìš©ë¨!")
    print(f"{'='*80}")

if __name__ == "__main__":
    main() 