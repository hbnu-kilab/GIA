#!/usr/bin/env python3
"""
ê¸°ì¡´ CSV ê²°ê³¼ì— origin ì»¬ëŸ¼ì„ ì¶”ê°€í•˜ê³  ì¬í‰ê°€
- LLMì„ ì‚¬ìš©í•´ì„œ ê° ì§ˆë¬¸ì„ rule_based vs enhanced_llm_with_agentë¡œ ë¶„ë¥˜
- ë¶„ë¥˜ ê²°ê³¼ì— ë”°ë¼ í‰ê°€ ë°©ì‹ ë‹¬ë¦¬ ì ìš©
"""
import pandas as pd
import json
from pathlib import Path
from typing import List, Dict
from Evaluation.pipeline_v2.common.data_utils import clean_ground_truth_text
from Evaluation.pipeline_v2.common.evaluation import calculate_exact_match, calculate_f1_score, calculate_relaxed_exact_match, calculate_relaxed_f1_score
from Evaluation.pipeline_v2.common import TrackedOpenAIClient
from Evaluation.pipeline_v2.config import LLM_MODEL, LLM_TEMPERATURE

# ì „ì—­ í´ë¼ì´ì–¸íŠ¸
tracked_openai_client = None

def init_openai_client():
    global tracked_openai_client
    if tracked_openai_client is None:
        from Evaluation.pipeline_v2.common import ExperimentLogger
        import tempfile
        # ì„ì‹œ ë¡œê±° ìƒì„±
        temp_dir = tempfile.mkdtemp()
        logger = ExperimentLogger("origin_classification", temp_dir)
        tracked_openai_client = TrackedOpenAIClient(logger)

def get_classification_result(question: str) -> str:
    """ì§ˆë¬¸ì„ Simple Lookup Tasks vs Other Tasksë¡œ ë¶„ë¥˜"""
    init_openai_client()
    
    prompt = """
    Classify the question into ONE category:
    1) Simple Lookup Tasks
    2) Other Tasks
    Return only the category name.
    """
    
    resp = tracked_openai_client.chat_completions_create(
        call_type="task_classification",
        model=LLM_MODEL,
        messages=[
            {"role": "system", "content": "You are an expert network engineering assistant."},
            {"role": "user", "content": f"Question: {question}\nInstruction: {prompt}"},
        ],
        temperature=LLM_TEMPERATURE,
    )
    
    task_type = resp.choices[0].message.content.strip()
    
    # Simple Lookup Tasks -> rule_based
    # Other Tasks -> enhanced_llm_with_agent
    if "Simple Lookup Tasks" in task_type:
        return "rule_based"
    else:
        return "enhanced_llm_with_agent"

def add_origin_column(csv_path: str) -> str:
    """CSV íŒŒì¼ì— origin ì»¬ëŸ¼ì„ ì¶”ê°€"""
    print(f"Origin ì»¬ëŸ¼ ì¶”ê°€ ì¤‘: {csv_path}")
    
    # CSV ì½ê¸°
    df = pd.read_csv(csv_path)
    print(f"ì´ {len(df)} ê°œ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘...")
    
    # ê° ì§ˆë¬¸ ë¶„ë¥˜
    origins = []
    for idx, row in df.iterrows():
        question = str(row['question'])
        origin = get_classification_result(question)
        origins.append(origin)
        
        if (idx + 1) % 50 == 0:
            print(f"ì§„í–‰ë¥ : {idx + 1}/{len(df)} ({(idx + 1) / len(df) * 100:.1f}%)")
    
    # origin ì»¬ëŸ¼ ì¶”ê°€
    df['origin'] = origins
    
    # ìƒˆ íŒŒì¼ëª…ìœ¼ë¡œ ì €ì¥
    output_path = csv_path.replace('.csv', '_with_origin.csv')
    df.to_csv(output_path, index=False)
    
    # ë¶„ë¥˜ ê²°ê³¼ ìš”ì•½
    origin_counts = pd.Series(origins).value_counts()
    print(f"\në¶„ë¥˜ ê²°ê³¼:")
    for origin, count in origin_counts.items():
        print(f"  {origin}: {count}ê°œ ({count/len(origins)*100:.1f}%)")
    
    return output_path

def evaluate_with_origin(csv_path: str) -> Dict:
    """origin ì»¬ëŸ¼ì„ ê³ ë ¤í•œ ì¬í‰ê°€"""
    print(f"Origin ê¸°ë°˜ ì¬í‰ê°€ ì¤‘: {csv_path}")
    
    df = pd.read_csv(csv_path)
    
    # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ì •ë¦¬
    predictions = []
    ground_truths = []
    origins = df['origin'].tolist()
    
    for _, row in df.iterrows():
        # ì˜ˆì¸¡ ê²°ê³¼ ì •ë¦¬ (ì½œë¡  ì œê±° ì ìš©)
        pred_raw = str(row['pre_GT']) if pd.notna(row['pre_GT']) else ""
        pred_clean = clean_ground_truth_text(pred_raw)
        predictions.append(pred_clean)
        
        # ì •ë‹µ ì •ë¦¬
        gt_raw = str(row['ground_truth']) if pd.notna(row['ground_truth']) else ""
        gt_clean = clean_ground_truth_text(gt_raw)
        ground_truths.append(gt_clean)
    
    # ì¸ë±ìŠ¤ ê·¸ë£¹ ë¶„ë¦¬
    rule_idx = [i for i, o in enumerate(origins) if o == "rule_based"]
    enhanced_idx = [i for i, o in enumerate(origins) if o == "enhanced_llm_with_agent"]
    
    print(f"Rule-based ì§ˆë¬¸: {len(rule_idx)}ê°œ")
    print(f"Enhanced LLM ì§ˆë¬¸: {len(enhanced_idx)}ê°œ")
    
    # ì „ì²´ í‰ê°€
    overall_em = calculate_exact_match(predictions, ground_truths)
    overall_f1 = calculate_f1_score(predictions, ground_truths)
    overall_em_relaxed = calculate_relaxed_exact_match(predictions, ground_truths)
    overall_f1_relaxed = calculate_relaxed_f1_score(predictions, ground_truths)
    
    # Rule-based í‰ê°€ (GTë§Œ)
    if rule_idx:
        rule_pred_gt = [predictions[i] for i in rule_idx]
        rule_true_gt = [ground_truths[i] for i in rule_idx]
        rule_em = calculate_exact_match(rule_pred_gt, rule_true_gt)
        rule_f1 = calculate_f1_score(rule_pred_gt, rule_true_gt)
        rule_em_relaxed = calculate_relaxed_exact_match(rule_pred_gt, rule_true_gt)
        rule_f1_relaxed = calculate_relaxed_f1_score(rule_pred_gt, rule_true_gt)
    else:
        rule_em = rule_f1 = rule_em_relaxed = rule_f1_relaxed = 0.0
    
    # Enhanced LLM í‰ê°€ (GT + ì„¤ëª…)
    if enhanced_idx:
        enhanced_pred_gt = [predictions[i] for i in enhanced_idx]
        enhanced_true_gt = [ground_truths[i] for i in enhanced_idx]
        enhanced_em = calculate_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_f1 = calculate_f1_score(enhanced_pred_gt, enhanced_true_gt)
        enhanced_em_relaxed = calculate_relaxed_exact_match(enhanced_pred_gt, enhanced_true_gt)
        enhanced_f1_relaxed = calculate_relaxed_f1_score(enhanced_pred_gt, enhanced_true_gt)
    else:
        enhanced_em = enhanced_f1 = enhanced_em_relaxed = enhanced_f1_relaxed = 0.0
    
    results = {
        "total_questions": len(predictions),
        "overall": {
            "exact_match": overall_em,
            "f1_score": overall_f1,
            "exact_match_relaxed": overall_em_relaxed,
            "f1_score_relaxed": overall_f1_relaxed
        },
        "rule_based": {
            "question_count": len(rule_idx),
            "exact_match": rule_em,
            "f1_score": rule_f1,
            "exact_match_relaxed": rule_em_relaxed,
            "f1_score_relaxed": rule_f1_relaxed
        },
        "enhanced_llm_with_agent": {
            "question_count": len(enhanced_idx),
            "exact_match": enhanced_em,
            "f1_score": enhanced_f1,
            "exact_match_relaxed": enhanced_em_relaxed,
            "f1_score_relaxed": enhanced_f1_relaxed
        }
    }
    
    return results

def update_json_with_origin_evaluation(json_path: str, evaluation_results: Dict):
    """JSON íŒŒì¼ì„ origin ê¸°ë°˜ í‰ê°€ ê²°ê³¼ë¡œ ì—…ë°ì´íŠ¸"""
    print(f"JSON íŒŒì¼ ì—…ë°ì´íŠ¸: {json_path}")
    
    with open(json_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    # ë°±ì—… ìƒì„±
    backup_path = json_path.replace('.json', '_before_origin_fix.json')
    with open(backup_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    # evaluation ì„¹ì…˜ êµì²´
    if 'evaluation' in data:
        data['evaluation'] = {
            "overall": {
                "exact_match": evaluation_results["overall"]["exact_match"],
                "f1_score": evaluation_results["overall"]["f1_score"],
                "total_questions": evaluation_results["total_questions"]
            },
            "overall_relaxed": {
                "exact_match": evaluation_results["overall"]["exact_match_relaxed"],
                "f1_score": evaluation_results["overall"]["f1_score_relaxed"],
                "total_questions": evaluation_results["total_questions"]
            },
            "rule_based": {
                "exact_match": evaluation_results["rule_based"]["exact_match"],
                "f1_score": evaluation_results["rule_based"]["f1_score"],
                "question_count": evaluation_results["rule_based"]["question_count"]
            },
            "rule_based_relaxed": {
                "exact_match": evaluation_results["rule_based"]["exact_match_relaxed"],
                "f1_score": evaluation_results["rule_based"]["f1_score_relaxed"],
                "question_count": evaluation_results["rule_based"]["question_count"]
            },
            "enhanced_llm": {
                "ground_truth": {
                    "exact_match": evaluation_results["enhanced_llm_with_agent"]["exact_match"],
                    "f1_score": evaluation_results["enhanced_llm_with_agent"]["f1_score"]
                },
                "explanation": data['evaluation'].get('enhanced_llm', {}).get('explanation', {}),
                "question_count": evaluation_results["enhanced_llm_with_agent"]["question_count"]
            }
        }
    
    # ì—…ë°ì´íŠ¸ëœ íŒŒì¼ ì €ì¥
    with open(json_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def main():
    base_dir = Path("runs/exp_ab_k10/results")
    
    # ì²˜ë¦¬í•  íŒŒì¼ë“¤
    files_to_process = [
        ("ragA_after_k10.csv", "ragA_k10.json"),
        ("ragB_after_k10.csv", "ragB_k10.json"),
    ]
    
    for csv_file, json_file in files_to_process:
        csv_path = base_dir / csv_file
        json_path = base_dir / json_file
        
        if csv_path.exists() and json_path.exists():
            print(f"\n{'='*70}")
            print(f"ì²˜ë¦¬ ì¤‘: {csv_file} -> {json_file}")
            print(f"{'='*70}")
            
            # 1. Origin ì»¬ëŸ¼ ì¶”ê°€
            csv_with_origin = add_origin_column(str(csv_path))
            print(f"âœ… Origin ì»¬ëŸ¼ì´ ì¶”ê°€ëœ íŒŒì¼: {csv_with_origin}")
            
            # 2. Origin ê¸°ë°˜ ì¬í‰ê°€
            evaluation_results = evaluate_with_origin(csv_with_origin)
            
            # 3. ê²°ê³¼ ì¶œë ¥
            print(f"\nì¬í‰ê°€ ê²°ê³¼:")
            print(f"  ì „ì²´ ì§ˆë¬¸: {evaluation_results['total_questions']}ê°œ")
            print(f"  Overall EM: {evaluation_results['overall']['exact_match']:.4f}")
            print(f"  Overall F1: {evaluation_results['overall']['f1_score']:.4f}")
            print(f"  Rule-based ({evaluation_results['rule_based']['question_count']}ê°œ):")
            print(f"    EM: {evaluation_results['rule_based']['exact_match']:.4f}")
            print(f"    F1: {evaluation_results['rule_based']['f1_score']:.4f}")
            print(f"  Enhanced LLM ({evaluation_results['enhanced_llm_with_agent']['question_count']}ê°œ):")
            print(f"    EM: {evaluation_results['enhanced_llm_with_agent']['exact_match']:.4f}")
            print(f"    F1: {evaluation_results['enhanced_llm_with_agent']['f1_score']:.4f}")
            
            # 4. JSON íŒŒì¼ ì—…ë°ì´íŠ¸
            update_json_with_origin_evaluation(str(json_path), evaluation_results)
            print(f"âœ… {json_file} ì—…ë°ì´íŠ¸ ì™„ë£Œ")
            
        else:
            print(f"âš ï¸  íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {csv_file} ë˜ëŠ” {json_file}")
    
    print(f"\n{'='*70}")
    print("ğŸ‰ ëª¨ë“  íŒŒì¼ ì¬ì²˜ë¦¬ ì™„ë£Œ!")
    print("Origin ì»¬ëŸ¼ì´ ì¶”ê°€ë˜ê³  rule_based/enhanced_llmìœ¼ë¡œ ì˜¬ë°”ë¥´ê²Œ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print("ë°±ì—… íŒŒì¼ë“¤ì´ _before_origin_fix.jsonìœ¼ë¡œ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.")
    print(f"{'='*70}")

if __name__ == "__main__":
    main() 